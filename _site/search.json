[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSo, You Think You Can Model a Heartbeat?\n\n\n\n\n\n\ncardiovascular\n\n\nnon-linear\n\n\nsimulation\n\n\neducational\n\n\n\nYour heart isn’t a metronome, it’s a chaotic jazz drummer. Dive with me into the math madness behind why your heartbeat skyrockets during burpees and how a non-linear model (featuring logistic functions and elderly Zumba warriors) decodes this cardiac drama. Spoiler: Your Fitbit is shook. Code, curves, and caffeine included. \n\n\n\n\n\nMar 24, 2025\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\nGoing NUTS: A Step-by-Step Guide to Adaptive Hamiltonian Sampling in R\n\n\n\n\n\n\nHMC\n\n\nMCMC\n\n\nalgorithms\n\n\neducational\n\n\n\nYou may have wonder what’s the engine behind many Bayesian analysis in modern research? Well, in this post we’ll not only cover the fundamentals, but we’ll also build from the ground-up a fully functional Hamiltonian Monte Carlo with No-U-Turn-Sampler (HMC-NUTS), the bad boy behind the hood of the Bayesian inference monster truck. \n\n\n\n\n\nJan 21, 2025\n\n\n41 min\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Neuron’s Spark: Building Intuition for Action Potential Dynamics\n\n\n\n\n\n\nnon-linear\n\n\nneuroscience\n\n\neducational\n\n\n\nAction potentials are the language of neurons. In this post we build a simplified model to explain how these electrical signals are generated. We’ll explore the role of ion channels and build an intuitive understanding of neuronal firing. \n\n\n\n\n\nJan 6, 2025\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations\n\n\n\n\n\n\nnon-linear\n\n\nsimulation\n\n\nbiology\n\n\neducational\n\n\n\nIn this post, we’ll take you step by step through building and playing with a model of lymphocyte dynamics. So, grab your keyboard, and let’s whip up a recipe for understanding how T cells become the immune system’s MVPs. \n\n\n\n\n\nDec 23, 2024\n\n\n40 min\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: A Case for Synaptic Plasticity\n\n\n\n\n\n\nnon-linear\n\n\neducational\n\n\nneuroscience\n\n\n\nIn this second edition we’ll be discussing the role of long term potentiation and depression, a form of neuroplasticity, and their dynamics using non-linear models. Here, we’ll dig deep into the statistical intricacies of the overlapping dynamics of synaptic neuroplasticity. \n\n\n\n\n\nDec 14, 2024\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: Pharmacokinetics and Indomethacin\n\n\n\n\n\n\nnon-linear\n\n\neducational\n\n\ninference\n\n\n\nHere we dive in the process of making a non-linear model to predict the decay of plasma levels of an anti-inflammatory drug, and compare frequentist and bayesian methods. \n\n\n\n\n\nAug 20, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Good, The Bad, and Hamiltonian Monte Carlo\n\n\n\n\n\n\nmcmc\n\n\nalgorithms\n\n\ninference\n\n\n\nFollowing the footsteps of the previous post, here we dive deep ourselves into the mud of hamiltonian mechanics and how its dynamics can help us to explore parameter space more efficiently. \n\n\n\n\n\nMay 15, 2024\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte What?\n\n\n\n\n\n\nmcmc\n\n\nalgorithms\n\n\neducational\n\n\n\nIn this post we will submerge into the main idea behind Markov Chain Monte Carlo (MCMC for short) and why it is useful within the bayesian inference framework. \n\n\n\n\n\nApr 25, 2024\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Bayesically Speaking\n\n\n\n\n\n\nnews\n\n\n\nHi everyone! This is the first post of Bayesically Speaking, so get your seatbelt on and get ready to join me on this ride! \n\n\n\n\n\nJun 10, 2023\n\n\n13 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "about.html#what-is-all-the-fuzz-about",
    "href": "about.html#what-is-all-the-fuzz-about",
    "title": "About",
    "section": "What is all the fuzz about?",
    "text": "What is all the fuzz about?\nHello and welcome to Bayesically Speaking, the blog where I, Matías, share my passion for statistics, bayesian methods and coffee. If you are curious about how to use data and probability to understand and solve real world problems, you have come to the right place. Here you will find practical examples, tutorials, tips and tricks on how to apply statistical thinking and inference to various domains and scenarios."
  },
  {
    "objectID": "about.html#who-is-this-guy",
    "href": "about.html#who-is-this-guy",
    "title": "About",
    "section": "Who is this guy?",
    "text": "Who is this guy?\nWho am I and why should you care? Well, I am a researcher who loves numbers and coffee (not necessarily in that order). I have a special interest in bayesian methods, which are a powerful and flexible way of doing statistics that allows you to incorporate prior knowledge and uncertainty into your analysis. Bayesian methods are not magic, though. They have their limitations and challenges, just like any other approach. That’s why I don’t shy away from using other tools when they are appropriate, such as frequentist methods or p-values. My goal is not to start a war between different schools of thought, but to show you how to use the best tool for the job."
  },
  {
    "objectID": "about.html#a-secret-weapon",
    "href": "about.html#a-secret-weapon",
    "title": "About",
    "section": "A secret weapon",
    "text": "A secret weapon\nIn this blog, you will also learn how to create beautiful and informative graphics and plots using R, which is a free and open source software for data analysis and visualization. R is my favorite tool for doing statistics, because it has a huge community of users and developers who create amazing packages and resources for all kinds of purposes. R can also do much more than just statistics, such as web scraping, text mining, machine learning and more. Even this website was build using R!"
  },
  {
    "objectID": "posts/2024-12-10 a-case-for-synaptic-plasticity/index.html",
    "href": "posts/2024-12-10 a-case-for-synaptic-plasticity/index.html",
    "title": "Non-linear models: A Case for Synaptic Plasticity",
    "section": "",
    "text": "Introduction\nIf you missed the first edition of this series of non-linear models posts, go see it now!\nThe human brain is a show-off, really. It’s constantly rewiring itself, adapting to new challenges, and making you forget where you put your keys (all at the same time). This remarkable ability, called neuronal plasticity, is the brain’s way of saying, “Don’t worry, I can learn, unlearn, and even relearn, if necessary.” Imagine a guitarist practicing a tricky solo. With each attempt (and many failed ones), the neurons involved in mastering that melody become like overzealous gym buddies: stronger, faster, and annoyingly efficient. That’s plasticity in action.\nBut capturing this in a model? Oh, that’s another story. Modeling neuronal plasticity is like trying to explain a teenager’s mood swings (messy, non-linear, and full of unexpected spikes). The process involves sharp changes in synaptic strength, calcium signaling thresholds that act like overly jealous gatekeepers, and dynamics so complex they make rocket science look like child’s play. Traditional models just shrug and wave the white flag.\nLet’s welcome the non-linear models, the superheroes of mathematical modeling. In this post, we’ll take a closer look at how these models, specifically logistic growth equations, can tame the chaos of neuronal plasticity. We’ll demystify the biology, turn it into manageable math, and then buckle up to simulate it all in R. Yes, R… the land of syntax errors and endless parentheses, but don’t worry, I’ll guide you.\nBy the end of this journey, you’ll not only appreciate the elegance of neuronal plasticity but also see how non-linear modeling can turn brain mysteries into manageable equations. So whether you’re a neuroscience nerd, or someone who just likes the idea of bossing around neurons via code, this post has something for you. Let’s dive in, and don’t forget to pack your sense of humor. You’re going to need it!\n\n\n\nUnderstanding Neuronal Plasticity\nNeuronal plasticity is the brain’s way of being a multitasking genius or (depending on your perspective), a workaholic control freak. At its heart, it’s the neurons’ ability to adapt their structure and function, responding to experiences, activity, or the occasional existential crisis. This adaptability powers the brain’s greatest hits: learning, memory, and even bouncing back from injuries. This concept, stating that neuronal connections can be remodeled by experience, is also known as Hebbian Theory, and its behind many studied mechanisms by which plastic changes occurs at the synapse-level (Scott and Frank 2023). Think of it as the neural equivalent of turning your spare bedroom into a home gym, if neurons are adaptable, why shouldn’t you be?\n\nThe Role of Synaptic Strength\nPlasticity does its magic at the synapse, that microscopic handshake where one neuron whispers (or yells) to another. The strength of this connection, aptly named synaptic strength, determines how effectively neurons gossip. Strengthening these connections, a process grandiosely called Long-Term Potentiation (LTP), makes neural communication as smooth as butter on warm toast (Malenka 2003). On the flip side, Long-Term Depression (LTD) weakens connections, pruning the neural network like a gardener trimming overzealous hedges. Goodbye, redundant pathways; hello, optimization.\n Long-Term Potentiation (LTP) and Long-Term Depression (LTD) occurs as a function of synapse communication. This is what makes some brain pathways stronger or weaker over time, contributing to both the acquiring and loosing cognitive functions and motor skills. Source: Biology 2e. OpenStax.\n\n\nCalcium: The Master Regulator\nIf synaptic strength is the party, calcium ions (\\(Ca^{2+}\\)) are the overzealous DJ deciding whether to pump up the volume or call it a night. Calcium levels dictate whether the synapse becomes stronger or weaker, essentially flipping the plasticity switches:\n\nHigh calcium levels? Cue the LTP rave, connections strengthen, neurons fire happily ever after.\nLow calcium levels? Time for the LTD chill-out session, synapses quiet down and pathways are pruned.\n\nThe drama lies in the thresholds. Calcium must hit the sweet spot: too high, and it’s all systems go for strengthening; too low, and the neuron gets out its metaphorical scissors. Anything in between is neural purgatory (synaptic strength stays stable), which is another way of saying, “Meh, let’s just keep things as they are.”\n\n\nDynamic and Non-Linear Nature\nHere’s where things get tricky: the relationship between calcium levels and synaptic strength isn’t linear, because why would the brain ever take the easy route? A tiny nudge in calcium concentration can tip the balance dramatically, depending on whether those pesky thresholds are crossed. Feedback loops ensure that synaptic adjustments don’t spiral out of control, think of them as neural quality control, keeping things proportional and adaptive (Lisman, Yasuda, and Raghavachari 2012; Yasuda, Hayashi, and Hell 2022).\n\n\nCode\n# Define calcium levels and thresholds\ncalcium_levels &lt;- seq(0, 2, by = 0.01) # Simulated calcium levels (arbitrary units)\nltp_threshold &lt;- 1.2  # Threshold for LTP activation\nltd_threshold &lt;- 0.8  # Threshold for LTD activation\n\n# Define synaptic strength change based on calcium\nsynaptic_strength_change &lt;- function(calcium, ltp_thresh, ltd_thresh) {\n  # Non-linear responses for LTP and LTD\n  ltp_response &lt;- ifelse(calcium &gt; ltp_thresh, (calcium - ltp_thresh)^2, 0)\n  ltd_response &lt;- ifelse(calcium &lt; ltd_thresh, -(ltd_thresh - calcium)^2, 0)\n  ltp_response + ltd_response\n}\n\n# Compute synaptic strength changes\nstrength_changes &lt;- synaptic_strength_change(calcium_levels, ltp_threshold, ltd_threshold)\n\n# Create a data frame for plotting\ndata &lt;- data.frame(\n  Calcium = calcium_levels,\n  StrengthChange = strength_changes\n)\n\n# Plot\nggplot(data, aes(x = Calcium, y = StrengthChange)) +\n  geom_line(color = \"orange\", size = 1) +\n  geom_vline(xintercept = ltp_threshold, linetype = \"dashed\", color = \"darkgreen\", size = 1/2) +\n  geom_vline(xintercept = ltd_threshold, linetype = \"dashed\", color = \"darkred\", size = 1/2) +\n  annotate(\"text\", x = ltp_threshold + 0.01, y = 0.5, label = \"LTP Threshold\", color = \"darkgreen\", hjust = 0, size = 6) +\n  annotate(\"text\", x = ltd_threshold - 0.01, y = -0.5, label = \"LTD Threshold\", color = \"darkred\", hjust = 1, size = 6) +\n  labs(\n    title = \"Non-Linear Relationship Between Calcium and Synaptic Strength\",\n    x = \"Calcium Levels (arbitrary units)\",\n    y = \"Change in Synaptic Strength\"\n  )\n\n\n\n\n\nHere, the relationship between calcium levels and the change in synaptic strength is simplified to illustrate the relationship between calcium dynamics and synaptic plasticity.\n\n\n\n\nThis delicate dance between calcium signaling, thresholds, and synaptic strength is the essence of neuronal plasticity’s beauty. It’s also the reason why modeling this process is as tricky as explaining quantum physics to a toddler. But don’t worry; the non-linear models we’ll tackle next will help demystify this wild ride.\n\n\n\nThe Challenge of Modeling Neuronal Plasticity\nModeling neuronal plasticity is like trying to choreograph a dance for cats: dynamic, unpredictable, and prone to sudden chaos. The brain doesn’t play by the simple rules of linear systems. Instead, it thrives on thresholds, feedback loops, and behaviors that emerge as if neurons decided to collectively say, “Let’s make this interesting.”\n\nThe Complexity of Biological Systems\n Depiction of the step-by-step communication between neurons at the synapse-level through calcium signaling at the presynaptic neuron, responsible for neurotransmitter releasing and neuron depolarization. Source: Biology 2e. OpenStax.\nNeuronal plasticity isn’t just complex, it’s a web of biochemical drama. Take calcium concentration dynamics, these levels don’t just sit still like a well-behaved variable. They oscillate wildly, influenced by synaptic activity and timing, like calcium’s trying out for a rhythm section. These oscillations trigger intracellular signaling cascades that either crank up the volume or dial it down, depending on the situation. And then there’s synaptic change over time, a gradual process, often following a sigmoid or exponential curve. In other words, synaptic strength doesn’t just flick a switch; it warms up, stretches, and then decides if it’s going to do some serious lifting.\nLinear models? They try their best but are basically out of their depth here. Sure, they can hint at early synaptic changes, but when feedback mechanisms, thresholds, and saturation effects enter the chat, linear models might as well pack up and go home.\n\n\nThreshold-Dependent Behavior\nCalcium thresholds are where the real drama unfolds. Picture this: when \\(Ca^{2+}\\) crosses the high threshold (\\(C_{\\text{LTP}}\\)), synaptic strength goes into overdrive, climbing exponentially until it maxes out like a hiker who’s finally reached the summit. But if \\(Ca^{2+}\\) drops below the low threshold (\\(C_{\\text{LTD}}\\)), it’s pruning time, and synaptic strength takes a sharp nosedive to the baseline. These thresholds create non-linearity, where tiny calcium fluctuations can flip the synaptic switch from party mode to shutdown in a heartbeat.\n\n\nFeedback and Regulation\nIf thresholds are the stage, feedback mechanisms are the stagehands, constantly adjusting the scene. Positive feedback amplifies responses, with LTP inviting more calcium to the party and reinforcing synaptic strengthening. Negative feedback, on the other hand, keeps LTD from going full demolition crew by dialing down activity and avoiding unnecessary pruning. Together, these loops keep the system both chaotic and oddly balanced, a testament to biological ingenuity (or masochism).\n\n\nVariability and Noise\nAnd let’s not forget the noise. Neurons are the original rebels, no two behave identically. Calcium concentrations fluctuate, receptors show wildly different sensitivities, and neurotransmitter release has a habit of being unpredictable. Stochastic effects at the molecular level add another layer of randomness, making modeling this system akin to predicting the stock market in a thunderstorm.\n\n\nWhy Non-Linear Models?\nNon-linear models, like logistic growth equations, come to the rescue with their ability to handle the brain’s antics. They’re perfect for capturing threshold-dependent dynamics, saturation effects, and time-dependent changes, all while leaving room for noise and feedback. It’s as if these models were built specifically for taming the wild world of neuronal plasticity.\nIn the next section, we’ll get ready the magic of non-linear modeling, focusing on logistic growth equations. Spoiler: these equations are like cheat codes for understanding synaptic strength changes. Let’s see them in action!\n\n\n\nIntroducing the Non-Linear Model\nTo simulate the dynamics of neuronal plasticity, we need a model that doesn’t crumble under the weight of complexity. Let me introduce you the logistic growth model, our mathematical hero. This model elegantly captures the key ingredients of synaptic changes:\n\nThreshold behavior: Synaptic strength only changes when stimulation crosses a “Do Not Disturb” threshold.\n\nSaturation: Synaptic strength doesn’t keep growing forever (because even neurons know their limits).\n\nTime-dependence: Synaptic changes are gradual, like making sourdough bread. Patience is key.\n\nLogistic equations are pretty simple at its core, they all follow the same structure.\n\\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nThis kind of equations will (in general) yield the following behavior.\n\n\nCode\nlogistic &lt;- function(x) { 1 / (1 + exp(-x)) }\n\nggplot() +\n  geom_hline(yintercept = c(0, 1), linetype = 2, size = 1/2, col = \"gray50\") +\n  stat_function(fun = logistic, xlim = c(-10, 10), linewidth = 1, col = \"orange\") +\n  annotate(geom = \"text\", label = \"f(x) == frac(1,1 + e^{-x})\", x = -3, y = 0.5, parse = TRUE, size = 6) +\n  labs(title = \"Asymptotic Behavior of Logistic Functions\",\n       y = expression(italic(f)(x)),\n       x = expression(italic(x)))\n\n\n\n\n\n\n\n\n\nThis logistic growth model is widely used in biology to describe systems where growth starts slow, accelerates, and then hits a plateau (also called, asymptotes). In our case, it’s the perfect candidate for modeling synaptic strength during LTP and LTD, all while looking deceptively simple.\n\nLooking into the model\nIf we tweak the forementioned logistic equation, we get something like this:\n\\[\nS(t) = S_{\\text{min}} + \\frac{S_{\\text{max}} - S_{\\text{min}}}{1 + e^{-\\lambda (t - t_0)}}\n\\]\nLet’s break it down without making your eyes glaze over:\n\n\\(S(t)\\): Synaptic strength at time \\(t\\). This is what we’re modeling.\n\n\\(S_{\\text{min}}\\): The baseline synaptic strength, think of it as the starting line.\n\n\\(S_{\\text{max}}\\): The upper limit of synaptic strength after all the excitement of LTP.\n\n\\(\\lambda\\): The rate of change, or how quickly the synapse is building muscle.\n\n\\(t_0\\): The starting point of stimulation, shifting the curve along the time axis.\n\n\n\n\n\n\n\nPedagogic convenience\n\n\n\n\n\nUsing synaptic strength is simpler and more abstract, making it easier to fit and interpret in many contexts, especially when data on calcium dynamics is unavailable.\nExplicitly modeling calcium dynamics and thresholds can provide deeper biological insights but at the cost of added complexity.\n\n\n\nHere’s why this logistic structure is very convenient:\n\nSynaptic strength starts close to \\(S_{\\text{min}}\\) when time is still warming up (\\(t \\ll t_0\\)).\n\nIt ramps up as \\(t\\) approaches \\(t_0\\), because neurons love to make a dramatic entrance.\n\nGrowth slows as \\(S(t)\\) nears \\(S_{\\text{max}}\\), reflecting the biological saturation point (like most physiological processes).\n\n\n\nModeling LTP: The Big Boost\nIn Long-Term Potentiation (LTP), the synapse gets a power-up, causing a rapid and lasting increase in strength. Using the logistic growth model:\n\n\\(S_{\\text{min}}\\) is the baseline before stimulation.\n\n\\(S_{\\text{max}}\\) is the exciting new level of strength after LTP has worked its magic.\n\nTweaking \\(\\lambda\\) and \\(t_0\\) lets us control how quickly and when the synapse gets its boost, perfect for simulating the effects of different stimuli.\n\n\nModeling LTD: The Great Decline\nIn Long-Term Depression (LTD), the synapse doesn’t throw a party, it hits the brakes instead. For this, we flip the logistic equation on its head, changing the sign on the logistic part of the function:\n\\[\nS(t) = S_{\\text{max}} - \\frac{S_{\\text{max}} - S_{\\text{min}}}{1 + e^{-\\lambda (t - t_0)}}\n\\]\nHere:\n\n\\(S_{\\text{max}}\\) is where the synaptic strength starts before LTD brings it down a notch.\n\n\\(S_{\\text{min}}\\) is the new baseline after the synapse gets reorganized.\n\nIt’s like a reverse LTP (same logistics), but with a focus on pruning rather than boosting.\n\n\nCombining LTP and LTD: The Balancing Act\nNeurons aren’t simple creatures. They often experience LTP and LTD simultaneously, like trying to watch a movie while your neighbor is mowing the lawn. To capture this, we combine two logistic equations into one glorious model:\n\\[\nS(t) = S_{\\text{baseline}} + \\frac{\\Delta S_{\\text{LTP}}}{1 + e^{-\\lambda_{\\text{LTP}} (t - t_{\\text{LTP}})}} - \\frac{\\Delta S_{\\text{LTD}}}{1 + e^{-\\lambda_{\\text{LTD}} (t - t_{\\text{LTD}})}}\n\\]\nI know what you’re thinking… it seems like its too much. However, for sanity sakes lets break it down:\n\n\\(S_{\\text{baseline}}\\): The synaptic strength before anything exciting happens.\n\\(\\Delta S_{\\text{LTP}} = S_{\\text{max}} - S_{\\text{baseline}}\\): The increase due to LTP.\n\\(\\Delta S_{\\text{LTD}} = S_{\\text{baseline}} - S_{\\text{min}}\\): The decrease due to LTD.\n\\(\\lambda_{\\text{LTP}}\\) and \\(\\lambda_{\\text{LTD}}\\): Rates of change for LTP and LTD (i.e., the “how quickly” part).\n\\(t_{\\text{LTP}}\\) and \\(t_{\\text{LTD}}\\): Onset times for LTP and LTD (i.e., the “when” part).\n\nThis combined model lets us handle real-world complexity, where strengthening and weakening signals overlap like a neural tug-of-war.\n\n\nWhy Two Logistic Functions?\nYou might wonder why we bother with two logistic functions instead of just one that switches direction halfway. The answer: biology loves chaos.\n\nLTP and LTD aren’t mutually exclusive. They can overlap, fight, or ignore each other altogether.\n\nTheir onset times (\\(t_{\\text{LTP}}\\) and \\(t_{\\text{LTD}}\\)) and rates (\\(\\lambda_{\\text{LTP}}\\), \\(\\lambda_{\\text{LTD}}\\)) often vary, making a single equation woefully inadequate.\n\nOf course, we could have just used a single logistic function. However, in most real-world scenarios, beyond the pedagogic convenience, simple models are just the initial phase of any model building process.\nDespite any complexity we could add to the model, we must always remember that any model is always “an approximation to reality” at best. Or, as the famous George Box would say:\n\n“Essentially all models are wrong, but some are useful”\n\n\n\n\n\n\n\nLTP and LTD at the Same Time?\n\n\n\n\n\nIn some fascinating scenarios, neurons manage to pull off the ultimate balancing act: inducing both LTP and LTD simultaneously. This can happen when synaptic inputs are spatially separated, allowing different synapses on the same neuron to independently engage in potentiation or depression based on local calcium dynamics (Scott and Frank 2023). It’s like a multi-tasking maniac performing two distinct solos at once. Similarly, in spike-timing-dependent plasticity (discussed up ahead), complex spike-timing patterns can produce overlaps in the conditions for LTP and LTD, with some synapses “winning” in one direction and others in the opposite.\nSometimes, the brain’s calcium dynamics get a little wild, oscillating between the ranges required for LTP and LTD due to the convergence of excitatory and inhibitory inputs. Throw in a dash of neuromodulation (like dopamine or acetylcholine tweaking thresholds) and some synapses might lean toward strengthening while others favor weakening. This interplay can also occur when synapses start in different states: stronger synapses may undergo LTD to maintain homeostasis, while weaker ones are busy potentiating (Scott and Frank 2023). Interestingly, experimental protocols designed to study synaptic plasticity in mood disorders often force these conditions, with pharmacological interventions inducing concurrent LTP and LTD in some brain areas (Krystal, Kavalali, and Monteggia 2024).\n\n\n\nTwo logistic functions give us the flexibility to capture these dynamics without assuming biology plays nice.\nIn the next section, we’ll fire up R and use these equations to simulate synaptic strength changes. Spoiler: it’s going to be visually satisfying, so stick around!\n\n\n\nSimulating Synaptic Plasticity in R\nNow that we’ve got our fancy non-linear model ready to roll, it’s time to turn theory into practice (or, more specifically, into code). Let’s use R to simulate synaptic changes over time. Think of this as our chance to play neuroscientific alchemist, blending math, biology, and programming into a beautiful plot. Spoiler alert: things will go up, things will go down, and everything will make sense… eventually.\n\nStep 1: Defining the Model\nFirst, we need to teach R how to think like a synapse. We’ll define our logistic functions for LTP and LTD, then combine them to model the overall synaptic strength. It’s like giving R its own brain, except this one follows your rules and doesn’t forget where it put the car keys.\nThe logistic function describes how synaptic strength changes with time. For LTP, strength rises from a baseline to a maximum. For LTD, the opposite happens: it’s the digital equivalent of neurons ghosting each other.\n\n\nCode\n# Define the logistic function\nlogistic &lt;- function(t, t_onset, lambda, delta_S) {\n  delta_S / (1 + exp(-lambda * (t - t_onset)))\n}\n\n# Define the combined model\nsynaptic_strength &lt;- function(t, S_baseline, t_LTP, lambda_LTP, delta_LTP,\n                               t_LTD, lambda_LTD, delta_LTD) {\n  \n  ltp &lt;- logistic(t, t_LTP, lambda_LTP, delta_LTP)\n  ltd &lt;- -logistic(t, t_LTD, lambda_LTD, delta_LTD)\n  strength &lt;- S_baseline + ltp + ltd\n  \n  data.table::data.table(\n    Time = t, \n    LTP = ltp,\n    LTD = ltd,\n    `LTP + LTD` = strength\n  )\n}\n\n\nNotice how LTP and LTD are modeled as separate entities but combine their forces (or opposing forces) in the overall strength equation. It’s teamwork, or in this case, team tug-of-war.\n\n\nStep 2: Setting the Stage\nNext, we’ll define some parameters. Think of this as setting up your neural experiment in a lab, but with fewer pipettes and a lot more debugging.\nWe’ll start with baseline synaptic strength set to 1 (for our convenience), simulate LTP kicking in at time 5, and introduce LTD at time 10. Each process has its own speed (\\(\\lambda\\)) and magnitude (\\(\\Delta S\\)).\n\n\nCode\n# Parameters for the simulation\nS_baseline &lt;- 1\nt_LTP &lt;- 5\nlambda_LTP &lt;- 0.5\ndelta_LTP &lt;- 0.5\nt_LTD &lt;- 10\nlambda_LTD &lt;- 0.3\ndelta_LTD &lt;- 0.3\n\n# Time range for simulation\ntime &lt;- seq(0, 20, by = 0.1)\n\n# Compute synaptic strength over time\nS &lt;- synaptic_strength(time, S_baseline, t_LTP, lambda_LTP, delta_LTP,\n                       t_LTD, lambda_LTD, delta_LTD)\n\n## Format the resulting simulation into a convenient plotting format for later\nS &lt;- data.table::melt(S, id.vars = \"Time\")\n\n\nBy now, we’ve turned R into a synaptic storyteller, weaving tales of LTP, LTD, and the epic battle for dominance over synaptic strength.\n\n\nStep 3: Plotting the Results\nIt’s time for the pièce de résistance: visualization! Using ggplot2, we’ll craft a plot that shows how synaptic strength evolves. Imagine this as the biologist’s equivalent of putting your art project on the fridge.\n\n\nCode\n# Plot the synaptic strength\nggplot(S, aes(x = Time, y = value)) +\n  facet_wrap(~ variable, scales = \"free_y\", nrow = 1) +\n  geom_line(aes(color = variable), size = 1, show.legend = FALSE) +\n  scale_x_continuous(expand = c(0,0,0,0.01)) +\n  scale_y_continuous(n.breaks = 6) +\n  labs(title = \"Synaptic Strength Over Time\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\") \n\n\n\n\n\n\n\n\n\nThe resulting plot reveals that the resulting synaptic strength is the result of the simultaneously gradual rise in synaptic strength thanks to LTP, followed by a dip as LTD gets its turn (plus the constant that determines the baseline strength). It’s like a see-saw, but the laws of biochemistry decide who gets to go up or down.\n\n\n\n\n\n\nWhat do the units of synaptic strength means?\n\n\n\n\n\nThe synaptic strength (\\(S\\)) in the model is dimensionless. It represents a normalized value that captures the relative magnitude of potentiation or depression of a synapse, typically scaled between \\(S_{\\text{min}}\\) and \\(S_{\\text{max}}\\) (e.g., 0.5 to 1.5 in the upcoming extended model). This normalization simplifies the model by abstracting away specific biophysical units, such as conductance (\\(\\mu S\\)) or post-synaptic current (\\(pA\\)), to focus on the dynamics of synaptic changes.\nIf needed, the model could be adapted to use specific units, like synaptic conductance, by rescaling \\(S\\) and its parameters to reflect actual physiological measurements. For instance, calcium oscillation amplitudes and the associated logistic functions would need to be parameterized according to empirical data from experiments measuring synaptic responses.\n\n\n\n\n\nStep 4: Playing with Parameters\nThis is where things get really fun. By tweaking parameters, you can explore different scenarios, like what happens if neurons get hyped with LTP or slack off on LTD. For example, cranking up the LTP magnitude to 1.5 (from 0.5) while leaving LTD at 0.3 results in a net strengthening of the synapse.\n\n\nCode\n# Example: Modify LTP magnitude\ndelta_LTP &lt;- seq(0.5, 1.5, by = 0.1)\n\n# Recompute synaptic strength\ndata_modified &lt;- lapply(delta_LTP, function(x) {\n  strength &lt;- synaptic_strength(time, S_baseline, t_LTP, lambda_LTP, x,\n                                t_LTD, lambda_LTD, delta_LTD)\n  strength$delta_ltp &lt;- x\n  strength\n}) |&gt; \n  rbindlist()\n\n# Plot the modified synaptic strength\nggplot(data_modified, aes(x = Time, y = `LTP + LTD`, col = ordered(delta_ltp))) +\n  geom_line(size = 1) +\n  labs(title = \"Modified Synaptic Strength (Stronger LTP)\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\",\n       col = expression(Delta*\"S\"[LTP]))\n\n\n\n\n\n\n\n\n\nThis flexibility lets you conduct virtual experiments, saving you from lab-induced caffeine dependency while still yielding valuable insights.\nIn the next section, we’ll explore the implications of these simulations and explore how tweaking parameters can reveal deeper truths about neuronal plasticity. Stay tuned because in science, even the tiniest tweak can lead to a plot twist.\n\n\n\nExtending the Model to Include LTP and LTD Dynamics\nOur model so far is like a sturdy bicycle: simple, reliable, and great for short trips. But neuroscience is a highway with twists, turns, and occasional potholes, so it’s time to upgrade to a more dynamic model, think of this as strapping a rocket engine to that bike. Synaptic plasticity isn’t just about changes in strength; timing, saturation, and other nuances come into play. Let’s extend our model to capture some of these complexities.\n\nIncorporating Temporal Dependencies\nTiming is everything in neuroscience. In the world of synapses, when presynaptic and postsynaptic neurons fire relative to each other determines whether you get LTP or LTD. It’s a bit like a dance: if one partner steps too late, the magic is lost, and the audience (synaptic strength) is unimpressed.\nThis timing phenomenon, called spike-timing-dependent plasticity (STDP), hinges on \\(\\Delta t\\) (the interval between pre- and postsynaptic spikes). LTP loves when presynaptic spikes lead the way, while LTD thrives when postsynaptic spikes take the lead (Scott and Frank 2023).\nTo capture this, we introduce exponential decay functions for the magnitudes of LTP and LTD:\n\\[\n\\begin{aligned}\n\\Delta S_{\\text{LTP}} &= A_{\\text{LTP}} \\cdot e^{-\\frac{\\Delta t}{\\tau_{\\text{LTP}}}} \\\\\n\\Delta S_{\\text{LTD}} &= A_{\\text{LTD}} \\cdot e^{-\\frac{\\Delta t}{\\tau_{\\text{LTD}}}}\n\\end{aligned}\n\\]\nHere, \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\) are the maximum amplitudes of change in synaptic strength, equivalent to \\(\\Delta S\\) (the size of the “dance move”), and \\(\\tau_{\\text{LTP}}\\), \\(\\tau_{\\text{LTD}}\\) are time constants that govern how quickly the effect of time between spikes (\\(\\Delta t\\)) fades. If you’ve ever tried to tell a joke but botched the timing, you already understand how crucial these constants are.\nNow, let’s see how each of these parameters affects the synaptic choreography. If \\(A_{\\text{LTP}}\\) (the amplitude of the LTP move) is cranked up, the presynaptic dancer delivers an eye-catching performance, resulting in a stronger synapse. A higher amplitude here means even a moderately timed spike can still significantly strengthen the synapse. However, if \\(A_{\\text{LTD}}\\) takes center stage, the postsynaptic dancer has the spotlight, weakening the synapse more dramatically. Reducing either amplitude dulls their impact; it’s like both dancers are phoning it in, leaving the synapse barely moved, regardless of the timing.\nNow, consider \\(\\tau_{\\text{LTP}}\\) and \\(\\tau_{\\text{LTD}}\\), the time constants that control how forgiving the synapse is about timing. A large \\(\\tau_{\\text{LTP}}\\) means the presynaptic dancer gets more leeway; even if the moves are slightly late, the synapse is still impressed, leading to a broader window for LTP. In contrast, a smaller \\(\\tau_{\\text{LTP}}\\) signals a pickier synapse, rewarding only perfectly timed presynaptic spikes. Similarly, a long \\(\\tau_{\\text{LTD}}\\) gives the postsynaptic neuron plenty of room to weaken the synapse, tolerating more variability in timing. With a shorter \\(\\tau_{\\text{LTD}}\\), the postsynaptic dancer must hit the perfect beat to have any meaningful impact.\nThe plot below illustrates the STDP model, showing how the magnitude of synaptic changes depends on the timing (\\(\\Delta t\\)). Positive \\(\\Delta t\\) corresponds to presynaptic spikes leading, favoring LTP, while negative \\(\\Delta t\\) corresponds to postsynaptic spikes leading, favoring LTD.\n\n\nCode\n# Parameters for STDP\nA_LTP &lt;- 0.5  # Maximum potentiation amplitude\ntau_LTP &lt;- 20  # LTP time constant\nA_LTD &lt;- -0.5  # Maximum depression amplitude (negative)\ntau_LTD &lt;- 20  # LTD time constant\n\n# Function to compute LTP and LTD effects\nSTDP_effect &lt;- function(delta_t) {\n  ifelse(delta_t &gt;= 0,\n         A_LTP * exp(-delta_t / tau_LTP),  # LTP for positive delta_t\n         A_LTD * exp(delta_t / tau_LTD))  # LTD for negative delta_t\n}\n\n# Simulate data\ndelta_t &lt;- seq(-50, 50, by = 1)\nsynaptic_changes &lt;- sapply(delta_t, STDP_effect)\n\n# Create a data frame for plotting\ndata &lt;- data.frame(delta_t, synaptic_changes)\n\n# Plot the STDP curve\nggplot(data, aes(x = delta_t, y = synaptic_changes)) +\n  geom_line(color = \"orange\", size = 1) +\n  labs(title = \"Spike-Timing-Dependent Plasticity (STDP) Curve\",\n       x = \"Time Difference (Δt, ms)\",\n       y = \"Synaptic Change (ΔS)\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  annotate(\"text\", x = 30, y = 0.5, label = \"LTP Zone\", size = 6) +\n  annotate(\"text\", x = -30, y = -0.3, label = \"LTD Zone\", size = 6)\n\n\n\n\n\n\n\n\n\nThis plot captures the essence of STDP. It extends the model by introducing \\(\\Delta t\\) as a critical parameter, linking the timing of neural activity directly to the direction and magnitude of synaptic changes. It bridges abstract mathematical functions with tangible biological timing phenomena, making the model more representative of real neural processes.\n\n\n\n\n\n\nAddressing Saturation Effects\n\n\n\n\n\nSynaptic strength isn’t infinite, it has physiological constraints. However, in the current model, this limitation is already handled effectively by the logistic framework and STDP parameters. The logistic function naturally ensures that synaptic strength asymptotes at biologically meaningful upper and lower bounds, dictated by parameters like \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\). These parameters cap the magnitude of synaptic changes, avoiding unbounded potentiation or depression.\nIntroducing additional saturation mechanisms, such as hard boundaries on \\(S(t)\\), would be redundant in this context. The plateau behavior of the logistic function inherently aligns with physiological observations, maintaining synaptic changes within plausible limits without requiring explicit constraints. This reflects the elegance of using non-linear models: the boundaries are built into the system’s behavior, reducing the need for arbitrary external adjustments.\n\n\n\n\n\nSimulating the Extended Model\nLet’s see these concepts in action using R. By adding the timing-dependent amplitudes and enforcing strength limits, we upgrade our model to reflect these new dynamics.\n\n\nCode\n# Parameters for STDP-based dynamics\nA_LTP &lt;- 0.5\ntau_LTP &lt;- 20\nA_LTD &lt;- 0.3\ntau_LTD &lt;- 15\n\n# STDP-based amplitude functions\namplitude_LTP &lt;- function(delta_t, A_LTP, tau_LTP) {\n  A_LTP * exp(-abs(delta_t) / tau_LTP)\n}\n\namplitude_LTD &lt;- function(delta_t, A_LTD, tau_LTD) {\n  A_LTD * exp(-abs(delta_t) / tau_LTD)\n}\n\n# Extended model with saturation effects\nextended_synaptic_strength &lt;- function(t, S_baseline, \n                                       t_LTP, lambda_LTP, A_LTP, tau_LTP, \n                                       t_LTD, lambda_LTD, A_LTD, tau_LTD, \n                                       delta_t) {\n  S_baseline +\n       amplitude_LTP(delta_t, A_LTP, tau_LTP) / (1 + exp(-lambda_LTP * (t - t_LTP))) -\n       amplitude_LTD(delta_t, A_LTD, tau_LTD) / (1 + exp(-lambda_LTD * (t - t_LTD)))\n}\n\ndelta_t &lt;- seq(3, 30, length.out = 10)  # Example time difference between spikes\n\n# Simulate the extended model\nS_extended &lt;- lapply(delta_t, function(x) {\n  data.table(delta_t = x, \n             time = time,\n             strength = extended_synaptic_strength(time, S_baseline, \n                                                   t_LTP, lambda_LTP, A_LTP, tau_LTP, \n                                                   t_LTD, lambda_LTD, A_LTD, tau_LTD, \n                                                   delta_t = x) )\n}) |&gt; \n  rbindlist()\n\n\nWith this setup, our model now considers both the timing of spikes and the physiological constraints on synaptic strength.\n\n\nVisualizing the Extended Model\nLet’s plot the extended model to visualize these added dynamics.\n\n\nCode\n# Plot the extended synaptic strength\nggplot(S_extended, aes(x = time, y = strength, col = ordered(delta_t))) +\n  geom_line(aes(group = delta_t), size = 1) +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(title = \"Extended Synaptic Strength with Dynamics\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\",\n       col = expression(Delta*italic(t)~\"Between Spikes\"))\n\n\n\n\n\n\n\n\n\nThe plot reveals two key features:\n\nSTDP Influence: LTP and LTD magnitudes decay and flow based on spike timing (\\(\\Delta t\\)), showing how temporal precision can shape neural connections.\nSaturation Effects: Synaptic strength remains bounded thanks to parameters like \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\), showcasing the physiological checks and balances that prevent runaway excitation or extreme depression.\n\n\n\nFurther Extensions\nWhat’s next? We could incorporate frequency-dependent plasticity to explore how high-frequency stimulation turbocharges LTP or how low-frequency inputs foster LTD. Or we could unleash this model on a network of neurons to study how multiple synapses interact. The possibilities are endless!\nBy extending the model, we’re not just adding bells and whistles. We’re crafting a tool that captures the messy, dynamic beauty of real-world synaptic plasticity.\n\n\n\nExploring Parameter Effects\nOur extended model is like a well-equipped chemistry set for the brain, where each parameter represents a crucial ingredient in the recipe of synaptic plasticity. By varying these parameters, we can simulate how neurons behave under different conditions, from hyperactive learning states to subdued inhibition. This section focus on how key parameters influence the model’s behavior, explores their biological significance, and demonstrates how to experiment with them in R to uncover fascinating insights.\n\nSensitivity Analysis\nSensitivity analysis is a systematic way of tweaking the model’s parameters to observe their impact on synaptic strength. It’s akin to turning the dials on a stereo system to find the perfect sound balance, adjusting bass, treble, and volume to craft the desired output. In our case, the knobs control properties like calcium dynamics, synaptic decay rates, and baseline strength. These adjustments not only refine the model but also provide valuable biological insights into how real-life neurons might behave in various scenarios, from learning and memory formation to neurological disorders.\nHere’s an example of how you can experiment with the parameters in R to visualize their effects:\n\n\nCode\n# Define a range of parameter values\nlambda_LTP_vals &lt;- seq(0.5, 2, by = 0.5)  # Example: Steepness of LTP\nlambda_LTD_vals &lt;- seq(0.5, 2, by = 0.5)  # Example: Steepness of LTD\n\n# Simulate model for different parameters\nresults &lt;- expand.grid(\n  time = time, S_baseline = S_baseline, \n  t_LTP = t_LTP, lambda_LTP = lambda_LTP_vals, A_LTP = A_LTP, tau_LTP = tau_LTP,\n  t_LTD = t_LTD, lambda_LTD = lambda_LTD_vals, A_LTD = A_LTD, tau_LTD = tau_LTD,\n  delta_t = 0\n)\nresults$strength &lt;- mapply(extended_synaptic_strength, \n                           results$time, results$S_baseline,\n                           results$t_LTP, results$lambda_LTP, results$A_LTP, results$tau_LTP,\n                           results$t_LTD, results$lambda_LTD, results$A_LTD, results$tau_LTD,\n                           results$delta_t)\n\n# Plot sensitivity to lambda parameters\nggplot(results, aes(x = time, y = strength)) +\n  facet_wrap(~ ordered(lambda_LTD, levels = lambda_LTD_vals, \n                       labels = paste0(\"LTD Growth Rate = \", lambda_LTD_vals)), nrow = 2) +\n  geom_line(aes(color = ordered(lambda_LTP)), linewidth = 1) +\n  labs(title = \"Sensitivity Analysis: Impact of LTP and LTP Rate\",\n       x = \"Time (arbitrary units)\", y = \"Synaptic Strength\",\n       color = \"LTP Decay Rate\")\n\n\n\n\n\n\n\n\n\n\n\nFrom Model Parameters to Biological Applications\nThe calcium oscillation amplitude, \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\), dictates the maximum strength of synaptic potentiation and depression. Imagine these amplitudes as the size of a neuron’s emotional response, whether it leaps for joy (LTP) or sighs deeply in despair (LTD). Increasing \\(A_{\\text{LTP}}\\) simulates a brain on overdrive, as in heightened learning states, while lowering it mirrors synaptic inhibition or even neurodegeneration.\nThe decay constants, \\(\\tau_{\\text{LTP}}\\) and \\(\\tau_{\\text{LTD}}\\), add a temporal layer to this story. These parameters determine how quickly the effects of potentiation and depression fade over time, similar to how fast a good mood or a bad one dissipates. Short decay constants are like fleeting bursts of inspiration, ideal for circuits needing rapid adaptation, such as the visual system. In contrast, long decay constants suit processes like memory consolidation in the hippocampus, where stability over time is crucial.\nGrowth rates, denoted by \\(\\lambda_{\\text{LTP}}\\) and \\(\\lambda_{\\text{LTD}}\\), control how steeply synaptic strength transitions in response to stimuli. Think of these as the sensitivity of a neuron’s accelerator pedal. A steep curve means the synapse reacts instantly to changes, akin to a sprinter’s quick start. Shallow growth rates, on the other hand, are like a marathon runner pacing themselves, better reflecting gradual changes in long-term neural adaptation.\nFinally, the baseline synaptic strength, \\(S_{\\text{baseline}}\\), sets the initial tone of the model. This parameter represents the synapse’s “default mood” before any plasticity occurs. Adjusting \\(S_{\\text{baseline}}\\) allows us to model different starting conditions, from a highly potentiated synapse involved in an established skill to a depressed one, such as in the aging brain or during neurodegeneration.\n\n\nFurther Applications\nExploring these parameters is not just a theoretical exercise; it has practical applications in neuroscience research and beyond. Researchers can use sensitivity analysis to design experiments or predict neuronal responses to stimuli. Educators can use these simulations as teaching tools to demonstrate the complexity of synaptic plasticity. Clinicians might find such models useful for exploring the effects of treatments targeting synaptic mechanisms, providing insights into potential therapies for conditions like Alzheimer’s disease or epilepsy.\nUltimately, parameter exploration transforms our model into a versatile tool for understanding the dynamic interplay of synaptic mechanisms. It serves as a gateway to uncovering the mysteries of learning, memory, and neurological disorders while offering practical insights into experimental and therapeutic approaches.\n\n\nCode\n# Define a range of parameter values\nn &lt;- 30\nset.seed(1234)\nvals &lt;- data.table(\n  synapse_id = seq_len(n),\n  S_baseline = runif(n, 0.8, 1.2),\n  t_LTP = runif(n, 0, 20),\n  t_LTD = runif(n, 0, 20),\n  lambda_LTP = runif(n, 1.0, 1.5),\n  lambda_LTD = runif(n, 0.5, 1.0),\n  A_LTP = runif(n, 1.0, 1.5),\n  A_LTD = runif(n, 0.5, 1.0),\n  tau_LTP = runif(n, 15, 20),\n  tau_LTD = runif(n, 10, 25),\n  delta_t = runif(n, 0, 5)\n)\n\nvals &lt;- vals[vals[, list(t = time), synapse_id], on = \"synapse_id\"]\n\nvals[, synapse_strength := extended_synaptic_strength(\n  t, S_baseline, t_LTP, lambda_LTP, A_LTP, tau_LTP, \n  t_LTD, lambda_LTD, A_LTD, tau_LTD, delta_t\n) + rnorm(length(t), 0, 0.05), synapse_id]\n\n# Plot sensitivity to lambda parameters\nggplot(vals, aes(x = t, y = synapse_strength)) +\n  geom_line(aes(group = synapse_id), color = \"gray85\", linewidth = 1/3) +\n  geom_hline(yintercept = 1, col = \"gray20\", linewidth = 1/2, linetype = 2) +\n  stat_summary(geom = \"line\", fun = mean, linewidth = 1, color = \"orange\") +\n  scale_x_continuous(expand = c(0,0,0,0)) +\n  labs(title = \"Simulating Many Synapses: Stochastic Processes\",\n       x = \"Time (arbitrary units)\", y = \"Synaptic Strength\") +\n  annotate(\"text\", x = 17, y = 1.1, label = \"LTP\", size = 6, color = \"darkgreen\") +\n  annotate(\"text\", x = 17, y = 0.9, label = \"LTD\", size = 6, color = \"darkred\")\n\n\n\n\n\nOrange line denotes the mean synaptic strength over time. Horizontal dashed line denotes the zero-change value associated with no variations in synaptic strength. LTP, long-term potentiation; LTD, long-term depression.\n\n\n\n\n\n\n\nFinal Remarks\nEmbarking on this journey through non-linear modeling of neuronal plasticity, we’ve uncovered the intricate dynamics of how synaptic strength shifts over time under the competing forces of LTP and LTD. By leveraging coupled logistic functions, we’ve illuminated the non-linear and time-sensitive nature of synaptic changes, emphasizing the crucial role of calcium oscillations as the molecular metronomes orchestrating these processes.\nThroughout this exploration, several pivotal lessons can be drawn. Logistic functions proved to be elegant tools for capturing the threshold-dependent behavior of synaptic transitions, moving beyond gradual shifts to reveal the tipping points inherent in plasticity. Moreover, we’ve seen how tweaking parameters like calcium amplitude or decay rates can profoundly alter synaptic behavior, echoing the biological reality where minor molecular adjustments yield vastly different functional outcomes. Finally, the extended model demonstrated its robustness by integrating dynamic temporal dependencies and saturation effects, offering a nuanced depiction of synaptic adaptation to stimuli.\nThis is but the tip of the iceberg. The framework we’ve developed invites exploration into countless other physiological processes where non-linear dynamics reign supreme. Consider extending these concepts to other forms of plasticity, such as structural changes in dendrites or the fine-tuning of synaptic homeostasis. Similar models can be applied to cardiac autonomic modulation1, where the balance of sympathetic and parasympathetic activity echoes the interplay of LTP and LTD. Or consider the feedback loops governing hormonal or metabolic regulation, fertile grounds for non-linear interactions.\n1 A big surprise in this regard is coming up for the next post!!For the intrepid explorer of neuronal plasticity, there’s much to be done. Experiment with the model’s parameters, simulate pathological scenarios, or challenge the boundaries of its assumptions. With each iteration, you’ll not only refine your understanding of synaptic plasticity but also contribute to the broader pursuit of understanding life’s complexities through mathematics and computation.\nNeuroscience (or science in general) thrives at the nexus of theory, experimentation, and computation. Non-linear models are indispensable in bridging these fields, revealing the brain’s staggering capacity for adaptation and resilience. Whether you’re a scientist, student, or curious mind, there’s always room to stay inquisitive, tinker with ideas, and push the boundaries of what we know.\nSo, grab your equations, fire up R, and let curiosity lead the way. Biology and mathematics are waiting to converge in ways that can transform how we understand the very essence of life. Stay curious, and never stop exploring!\n\n\n\n\n\n\nReferences\n\nKrystal, John H, Ege T Kavalali, and Lisa M Monteggia. 2024. “Ketamine and Rapid Antidepressant Action: New Treatments and Novel Synaptic Signaling Mechanisms.” Neuropsychopharmacology 49 (1): 41–50.\n\n\nLisman, John, Ryohei Yasuda, and Sridhar Raghavachari. 2012. “Mechanisms of CaMKII Action in Long-Term Potentiation.” Nature Reviews Neuroscience 13 (3): 169–82.\n\n\nMalenka, Robert C. 2003. “The Long-Term Potential of LTP.” Nature Reviews Neuroscience 4 (11): 923–26.\n\n\nScott, Daniel N, and Michael J Frank. 2023. “Adaptive Control of Synaptic Plasticity Integrates Micro-and Macroscopic Network Function.” Neuropsychopharmacology 48 (1): 121–44.\n\n\nYasuda, Ryohei, Yasunori Hayashi, and Johannes W Hell. 2022. “CaMKII: A Central Molecular Organizer of Synaptic Plasticity, Learning and Memory.” Nature Reviews Neuroscience 23 (11): 666–82.\n\nCitationBibTeX citation:@misc{castillo-aguilar2024,\n  author = {Castillo-Aguilar, Matías},\n  title = {Non-Linear Models: {A} {Case} for {Synaptic} {Plasticity}},\n  date = {2024-12-14},\n  url = {https://bayesically-speaking.com/posts/2024-12-10 a-case-for-synaptic-plasticity/},\n  doi = {10.59350/mgpv2-d5e33},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2024. “Non-Linear Models: A Case for\nSynaptic Plasticity.” December 14, 2024. https://doi.org/10.59350/mgpv2-d5e33."
  },
  {
    "objectID": "posts/2025-01-21 hmc-nuts-from-zero/index.html",
    "href": "posts/2025-01-21 hmc-nuts-from-zero/index.html",
    "title": "Going NUTS: A Step-by-Step Guide to Adaptive Hamiltonian Sampling in R",
    "section": "",
    "text": "Introduction\nRemember MCMC from our series on these algorithms? Good times. We wandered through probability space like tourists with outdated maps, occasionally stumbling upon the good parts of the posterior. Then came Hamiltonian Monte Carlo (HMC), which, let’s be honest, felt like finally getting a GPS. We could actually explore the posterior, using gradients to guide our journey. It was efficient! It was elegant! It was… still kind of a pain in the neck. Check our first post on MCMC principles and use cases before diving into this post, if you haven’t done it already.\n\nWhy Your HMC Was Probably Just Running in Circles\nSee, HMC, for all its brilliance, has this one little quirk: it needs to be told how far to travel in each step. It’s like having a GPS that only understands instructions in increments of “exactly 5 kilometers”. Want to go 4.8 km? Too bad. Want to go 5.2 km? Tough luck. This “step size” parameter is crucial. Too small, and you’re just shuffling around like a confused shopper in a supermarket, making painfully slow progress. Too large, and you’re overshooting your target, bouncing around like a pinball, and generally wasting everyone’s time (especially your computer’s). Check our second post on MCMC, more specifically on HMC, to get a better intuition of what this algorithm is doing.\n Typical HMC running in circles around the posterior. Photo from Redleg Nation.\nThis manual tuning of the step size is about as fun as watching paint dry. You fiddle with it, run your sampler, check the results, fiddle some more, run it again… It’s an iterative process that makes you question your life choices. You start to wonder if there’s a better way. A way to tell the sampler, “Hey, just go where you need to go, okay?”\nEnter the No-U-Turn Sampler (NUTS). Yes, it’s a ridiculous name. But it’s also a brilliant solution. NUTS is like giving your HMC GPS a brain. It dynamically figures out how far to travel in each step, adapting to the local geometry of the posterior. No more tedious manual tuning! No more endless tweaking! NUTS takes the wheel and drives you straight to the heart of the posterior distribution, efficiently exploring even the most complex landscapes.\nIn this post, we’re going to dive deep into the inner workings of NUTS. We’ll explore how it cleverly avoids those wasteful U-turns (hence the name), how it builds efficient trajectories, and how you can implement it yourself in R. By the end, you’ll be able to unleash the power of adaptive Hamiltonian sampling on your own Bayesian models, and finally say goodbye to the tyranny of fixed step sizes. You’ll learn how to transform your sampling from a tedious chore into a smooth, efficient, and (dare I say) even enjoyable experience. So buckle up, because we’re about to go NUTS!\n\n\n\nWhen Your Posterior Looks Like a Jackson Pollock Painting\nSo, we know HMC with fixed step sizes can be a bit… finicky. But why should we care? Are there actual problems where this matters? You bet your Bayesian priors there are! Imagine you’re trying to model something complex, like the spread of a disease, the performance of a stock portfolio, or the migratory patterns of extremely confused pigeons. These kinds of problems often lead to posterior distributions that are… well, let’s just say they’re not always nice, well-behaved Gaussians.\nSometimes, your posterior might have multiple peaks (multimodal), like a mountain range with several summits. Traditional MCMC methods, like Metropolis-Hastings or Gibbs sampling, can get stuck in one peak, completely missing the others. It’s like trying to find all the best restaurants in a city by only exploring one neighborhood. You might find a decent burger joint, but you’ll miss out on the amazing sushi place across town.\n\n\nCode\nmy_surface &lt;- function(m, p) {\n    alpha = 0.7\n    ext = pi\n    2 + alpha - 2 * cos(m) * cos(p) - alpha * cos(ext - 2 * p)\n}\n\nx &lt;- y &lt;- seq(from = -1.5*pi, to = 1.5*pi, length.out = 100)\nz &lt;- outer(x, y, my_surface)\n\npar(mai = c(0,0,0,0),\n    mar = c(0,1,0,0),\n    oma = c(0,0,0,0))\n\npersp(x = x, y = y, z = z,\n      theta = 40, phi = 20, col = \"lightblue\",\n      expand = 1/3)\n\n\n\n\n\nThe easiest posterior distribution that the average Bayesian analyst tackles in a typical day. Shortly, we’ll see how this banana-shaped distribution is explored with HMC-NUTS algorithm.\n\n\n\n\nFixed-step HMC fares a little better, thanks to its gradient-based exploration. But even it can struggle. Imagine trying to navigate that same mountain range with a vehicle that can only move in fixed increments. If the step size is too large, you might overshoot the peaks and bounce around erratically. If it’s too small, you’ll take forever to explore the terrain. And if the terrain is particularly treacherous, say, with narrow valleys and steep cliffs, you might get stuck altogether.\nA particularly illustrative example is a posterior with a “banana-shaped” density. This isn’t some exotic fruit we’re talking about; it’s a type of distribution that curves like a banana. These curved, elongated shapes are common in hierarchical models and other complex statistical models. They present a challenge for fixed-step HMC because the optimal step size varies drastically across the distribution. A small step size might be necessary in the curved part of the “banana”, while a much larger step size would be more efficient in the straighter parts.\nTo make this concrete, let’s consider a simple bivariate Gaussian distribution with a banana-shaped density. We can define this as follows:\n\\[\n\\begin{aligned}\nx &\\sim \\mathcal{N}(0, 1) \\\\\ny &\\sim \\mathcal{N}(x^2, 0.1)\n\\end{aligned}\n\\]\nThis creates a posterior where \\(y\\) is dependent on \\(x^2\\), resulting in the characteristic banana shape.\n\n\nCode\nxy_seq &lt;- seq(-4.5, 4.5, .01)\ndf &lt;- expand.grid(x = xy_seq, y = xy_seq)\ndf$z &lt;- with(df, {\n  x_square &lt;- (x^2)\n  yx_square &lt;- ((y - x_square)^2) / .1\n  exp(-0.5 * x_square - 0.5 * yx_square)\n})\n\nggplot(df, aes(x, -y, z = z)) +\n  geom_raster(aes(fill = z), show.legend = FALSE) +\n  geom_contour(bins = 8, show.legend = FALSE, alpha = .1, col = \"#206080\") +\n  scale_y_continuous(limits = c(-4.5,1), expand = c(0,0), \n                     oob = scales::squish_infinite) +\n  scale_x_continuous(limits = c(-2.5,2.5), expand = c(0,0), \n                     oob = scales::squish_infinite) +\n  scale_fill_gradient(low = \"white\", high = \"#B7CCDC\") +\n  theme_void()\n\n\n\n\n\nBanana distribution, highlighting that some distributions could be difficult to explore.\n\n\n\n\nAs you can see, this posterior is far from a simple, spherical Gaussian. Trying to efficiently sample from this distribution with a fixed step size is like trying to fit a square peg in a round hole (or, in this case, a banana-shaped hole).\n\n\nHow to Not Get Lost in Probability Space\nNow that we understand why fixed-step HMC can struggle, let’s dig into the magic behind NUTS. The core idea is to let the sampler decide how far to travel in each step, adapting to the local geometry of the posterior. This involves two key components: (1) adaptive path length and (2) the U-turn criterion.\n\nAdaptive Path Length\nIn fixed-step HMC, you predefine a fixed number of leapfrog steps. This determines the length of the trajectory. But what if the optimal trajectory length varies across the posterior? What if sometimes you need to take a short stroll and other times a long hike?\n Foot steps in representation of the concept of “stepsize”. Photo from magnezis magnestic in Unsplash.\nNUTS solves this by dynamically adjusting the trajectory length. It starts with a short trajectory (e.g., just one leapfrog step) and then doubles it repeatedly. This doubling process continues until a certain criterion is met (which we’ll get to in the next subsection).\nWhy doubling? Because it’s efficient! It allows NUTS to quickly explore a wide range of trajectory lengths without having to try every single possible length. It’s like searching for a light switch in a dark room: you start by checking nearby, then you check a wider area, and so on, doubling your search radius until you find it.\nNow, let’s consider a 2D example. Remember our banana-shaped posterior? With HMC-NUTS, we start with a single leapfrog step in a random direction. Then, we double the trajectory length by taking two steps, then four, and so on. The trajectory starts to resemble a branching path, exploring the posterior in ever-widening arcs.\nLet’s now see an interactive animation of the NUTS dynamics in action over our banana-shaped distribution.\n\n\nThe code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo.\nThis dynamic doubling of the trajectory length is a crucial part of NUTS’s efficiency. It allows the sampler to quickly adapt to the local geometry of the posterior, taking long steps when appropriate and short steps when necessary. But how does NUTS know when to stop doubling? That’s where the U-turn criterion comes in, which we’ll explore in the next section.\n\n\nThe U-Turn Criterion\nSo, we’re doubling the trajectory length, exploring the posterior like eager adventurers. But how do we know when to stop? We don’t want to keep doubling indefinitely, wasting computational resources and potentially revisiting already explored regions. This is where the U-turn criterion comes in.\nThe basic idea is simple: if our trajectory starts to double back on itself, making a “U-turn”, it’s a good sign that we’ve explored the relevant part of the posterior in that direction. Continuing further would just be redundant. It’s like hiking up a mountain: once you reach the summit and start heading down the other side, you know you’ve gone far enough in that direction. No need to keep walking just for the sake of it (unless you’re really into hiking, I guess).\n Photo from Ricardo Salinas’ site.\nMathematically, a U-turn is detected by looking at the dot product of the momentum vectors at the beginning and end of the trajectory. Remember, the momentum vector indicates the direction of travel. If the dot product is negative, it means the two momentum vectors are pointing in roughly opposite directions, a clear sign of a U-turn.\nLet’s break this down with a small example. Imagine we’re sampling from a 2D Gaussian. We start at a point \\(q_0\\) with momentum \\(p_0\\). We take a few leapfrog steps and end up at a point \\(q_T\\) with momentum \\(p_T\\).\n\nIf \\(p_0 \\cdot p_T &gt; 0\\), the momentum vectors are pointing in roughly the same direction. No U-turn yet. Keep exploring!\nIf \\(p_0 \\cdot p_T &lt; 0\\), the momentum vectors are pointing in roughly opposite directions. We’ve made a U-turn! Time to stop doubling the trajectory in this direction.\n\n\n\nCode\nq0 &lt;- c(1, 1)\np0 &lt;- c(-1, 1)\nepsilon &lt;- 0.1\nsteps &lt;- 60\nSigma &lt;- diag(1,2)\n\nxy_seq &lt;- seq(-3, 3, by = .1)\ndens_df &lt;- expand.grid(x = xy_seq, y = xy_seq)\ndens_df$z &lt;- mvtnorm::dmvnorm(dens_df, c(0,0), Sigma)\n\npositions &lt;- matrix(0, nrow = steps, ncol = length(q0))\npositions[1, ] &lt;- q0\n\n## Simplified leapfrog steps\nq &lt;- q0; p &lt;- p0\nfor (i in 2:steps) {\n  p_half &lt;- p + (epsilon / 2) * (-q)  # Half-step for momentum\n  q &lt;- q + epsilon * p_half           # Full-step for position\n  p &lt;- p_half + (epsilon / 2) * (-q)  # Second half-step for momentum\n  positions[i, ] &lt;- q\n}\n\n# Check U-Turn\nuturn_check &lt;- apply(positions, 1, function(qp) {\n  sum((qp - q0) * p0) &lt; 0\n})\n\nplot_data &lt;- data.table(x = positions[, 1], \n                        y = positions[, 2], \n                        is_uturn = uturn_check)\n\nggplot(plot_data, aes(x = x, y = y)) +\n  geom_contour(data = dens_df, aes(x, y, z = z), col = \"gray\", \n               linetype = 2) +\n  geom_point(aes(color = is_uturn), size = 3) +\n  geom_path(aes(color = is_uturn, group = 1)) +\n  annotate(geom = \"segment\", \n           x = q0[1], y = q0[2], \n           xend = q0[1] + p0[1], yend = q0[2] + p0[2], \n           arrow = arrow(length = unit(1/3, \"cm\"), type = \"closed\"), \n           lwd = 1, linejoin = \"mitre\") +\n  labs(title = \"U-Turn Check\", \n       x = expression(q[1]), \n       y = expression(q[2])) +\n  scale_color_manual(values = c(\"#008080\", \"#990050\"), \n                     labels = c(\"No U-Turn\", \"U-Turn\"),\n                     name = \"U-Turn State\")\n\n\n\n\n\nCharacterization of the trajectory of a particle in a 2D gaussian distribution. You can see that the particle rolls until a U turn is detected.\n\n\n\n\nThis U-turn criterion is a clever way to prevent NUTS from over-exploring. It ensures that the sampler focuses its efforts on the relevant regions of the posterior, leading to more efficient sampling. Now that we know how NUTS adapts the trajectory length and when to stop, let’s see how it organizes these trajectories into a neat and efficient structure: the binary tree.\n\n\nTree Building for Sampling\nWe now know how NUTS dynamically adjusts the trajectory length and how it uses the U-turn criterion to avoid redundant sampling. But how does it keep track of all these different trajectories? The answer is: with a binary tree!\nImagine you’re exploring a maze. You start at the entrance, and at each intersection, you have two choices: go left or go right. You keep making choices until you reach a dead end or decide to turn back. This is essentially how NUTS builds its binary tree.\n Picture of a Maze, representing that each choice (either left or right) is an example of binary decision, which represents the binary tree building process. Photo from Adobe Stock.\nNUTS starts with a single leapfrog step (our initial trajectory). This is the root of our tree. Then, it doubles the trajectory length by taking two steps: one to the “left” and one to the “right”. These two trajectories become the children of the root node. This doubling process continues, creating a balanced binary tree.\nWhy a tree? Because it’s an efficient way to organize the exploration. It allows NUTS to quickly explore different parts of the posterior without having to revisit already explored regions (thanks to the U-turn criterion).\nLet’s break down the tree-building process:\n\nStart with a single leapfrog step: This is the root node of the tree.\nDouble the trajectory length: Create two new trajectories by extending the current trajectory in both directions (forward and backward in time). These are the left and right children of the current node.\nCheck for U-turns: Use the U-turn criterion to determine if either of the new trajectories has made a U-turn. If so, stop extending that branch of the tree.\nRepeat steps 2 and 3: Continue doubling the trajectory length and checking for U-turns until a certain stopping criterion is met (e.g., a maximum tree depth).\n\n Binary tree building process. Photo from J. Mach. Learn. Res., 15(1), 1593-1623.\nNow, how does NUTS use this tree for sampling? It uses a process called recursive sampling. It starts at the root node and recursively traverses the tree, choosing either the left or right child at each node. The choice is made probabilistically, based on the Metropolis-Hastings acceptance criterion. This ensures that the samples are drawn from the target posterior distribution.\nThis tree-building and recursive sampling process is what makes NUTS so efficient. It allows it to explore the posterior in a structured and organized way, avoiding redundant sampling and quickly adapting to the local geometry.\n\n\n\nStep-by-Step Implementation of NUTS\nNow that we understand the theory behind NUTS, it’s time to put our knowledge into practice. In this section, we’ll build a NUTS sampler from scratch in R. Don’t worry, we’ll take it one step at a time (or, should I say, one leapfrog step at a time?).\n\nSetting Things Up\nBefore we start building our NUTS engine, we need to gather our tools. This involves defining (1) our target distribution, (2) its gradient, and (3) the leapfrog integration method. We’ll reuse the gradient and leapfrog functions from our previous HMC implementation (because why reinvent the wheel?).\nLet’s consider a simple 2D Gaussian distribution as our target posterior. This will make it easier to visualize and understand the behavior of NUTS. We can define the log-posterior as follows:\n\\[\n\\log p(q) = -\\frac{1}{2}q^\\top \\Sigma^{-1} q -\\frac{1}{2}\\log(\\det{\\Sigma}) + \\text{constant}\n\\]\nWhere \\(q = (q_1, q_2)\\) is our position vector and \\(\\Sigma\\) is the covariance matrix.\nNow, let’s define the gradient of the log-posterior:\n\\[\n\\nabla \\log p(q) = -\\Sigma^{-1} q\n\\]\nHere’s the R code for the log-posterior and its gradient:\n\n# Log-posterior function\nlog_posterior &lt;- function(q, Sigma, mu) {\n  diff &lt;- q - mu\n  -0.5 * t(diff) %*% solve(Sigma) %*% diff - 0.5 * log(det(Sigma))\n}\n\n## Gradient of Log-posterior\ngrad_log_posterior &lt;- function(q, Sigma, mu) {\n  -solve(Sigma, q - mu)\n}\n\nNext, we need the leapfrog integration function. Here’s a reminder (and the R code) from the previous HMC blog post:\n\n# Leapfrog integration\nleapfrog &lt;- function(q, p, grad_func, epsilon, Sigma, mu) {\n  grad_q &lt;- grad_func(q, Sigma, mu) # Gradient calculation\n  p &lt;- p + (epsilon / 2) * grad_q # Half-step for momentum\n  q &lt;- q + epsilon * p # Full-step for position\n  grad_q &lt;- grad_func(q, Sigma, mu)\n  p &lt;- p + (epsilon / 2) * grad_q # Half-step for momentum\n  list(q = q, p = p)\n}\n\nWith these functions in place, we’re ready to start implementing the core components of NUTS. We’ll begin with the initialization step in the next subsection.\nOkay, let’s dive into the core steps of NUTS implementation. This will be the most substantial part of the implementation section.\n\n\nCore Steps of NUTS\nNow that we have our tools ready, we can start building the core components of the NUTS sampler. This involves initialization, building the trajectory, checking for U-turns, building the tree, and handling acceptance and termination.\n\nInitialization\nFirst, we need to initialize the sampler. This involves setting the initial position \\(q\\), drawing a random momentum \\(p\\), and setting the hyperparameters, most importantly, the initial step size \\(\\epsilon\\).\n\n# Initialization\nnuts_init &lt;- function(q0, Sigma, mu, epsilon) {\n  list(\n    q = q0, # Initial position\n    mu = mu,\n    p = rnorm(length(q0), 0, 1), # Initial random momentum\n    epsilon = epsilon # Initial step size (this will be adapted later)\n  )\n}\n\n# Example initialization\nq0 &lt;- c(0, 0) # Starting position\nmu &lt;- c(-1,1) # Target location\nepsilon = 0.1 # Step size\n\nnuts_state &lt;- nuts_init(q0, Sigma, mu, epsilon)\n\n\n\nBuild the Trajectory\nThe next step is to build the trajectory using the leapfrog integrator. Here’s where the dynamic doubling comes in. We’ll also incorporate the U-turn check here.\n\n\nCode\n# U-turn check (dot product of momentum vectors)\nis_uturn &lt;- function(q_left, q_right, p_left, p_right) {\n  (sum((q_right - q_left) * p_left) &lt; 0) || \n    (sum((q_right - q_left) * p_right) &lt; 0)\n}\n\n# Build a trajectory (doubling until U-turn)\nbuild_trajectory &lt;- function(q, p, grad_func, epsilon, Sigma, mu) {\n  q_left &lt;- q\n  p_left &lt;- p\n  q_right &lt;- q\n  p_right &lt;- p\n  trajectory &lt;- list(q = q, p = p)\n  \n  j &lt;- 0\n  while (!is_uturn(q_left, q_right, p_left, p_right) && j &lt; 10) {\n    if (runif(1) &lt; 0.5) {\n      # Expand left\n      leapfrog_result &lt;- leapfrog(q_left, p_left, grad_func, -epsilon, Sigma, mu)\n      q_left &lt;- leapfrog_result$q\n      p_left &lt;- leapfrog_result$p\n    } else {\n      # Expand right\n      leapfrog_result &lt;- leapfrog(q_right, p_right, grad_func, epsilon, Sigma, mu)\n      q_right &lt;- leapfrog_result$q\n      p_right &lt;- leapfrog_result$p\n    }\n    trajectory$q &lt;- cbind(trajectory$q, leapfrog_result$q)\n    trajectory$p &lt;- cbind(trajectory$p, leapfrog_result$p)\n    j &lt;- j + 1\n  }\n  index &lt;- sample(1:ncol(trajectory$q), 1)\n  list(q = trajectory$q[, index], p = trajectory$p[, index])\n}\n\n\n\n\nTree Building\nWhile the previous function builds the trajectory by doubling it, the actual tree structure is implicit. This is because NUTS doesn’t explicitly store the entire tree in memory. It only needs to keep track of the leftmost and rightmost points of the trajectory and use them to sample.\n\n\nCode\nnsteps &lt;- 1000\npositions &lt;- matrix(NA, nrow = nsteps, ncol = 2)\n\nxy_seq &lt;- seq(-.5, 3, by = .1)\ndens_df &lt;- expand.grid(x = xy_seq, y = xy_seq)\ndens_df$z &lt;- mvtnorm::dmvnorm(dens_df, c(0,0), diag(1,2))\n\nset.seed(42)\nfor (i in seq_len(nsteps)) {\n  positions[i,] &lt;- build_trajectory(\n    q = c(1, 1),\n    p = c(-1, 1),\n    grad_func = grad_log_posterior,\n    epsilon = 0.1,\n    Sigma = diag(1,2),\n    mu = c(0,0)\n  )$q\n}\n\npositions &lt;- unique(positions) |&gt; \n  data.table::as.data.table()\n\npositions[V1 &gt; 1, side := \"Right\"]\npositions[V1 &lt; 1, side := \"Left\"]\npositions[V1 == 1, side := \"Initial Position\"]\n\nggplot(positions, aes(x = V1, y = V2)) +\n  geom_contour(data = dens_df, aes(x, y, z = z), col = \"gray\", \n               linetype = 2) +\n  geom_line(linewidth = 1) +\n  geom_line(aes(col = side), linewidth = 1) +\n  geom_point(aes(col = side), size = 3) +\n  labs(title = \"Binary Tree Trajectory Visualization\", \n       x = expression(q[1]), y = expression(q[2]),\n       col = \"Tree Side\") +\n  scale_color_manual(values = c(rgb(0,0.5,0.5,1),\n                                rgb(0.5,0,0.5,1),\n                                rgb(0.2,0.2,0.2,1)),\n                     breaks = c(\"Left\", \"Right\")) +\n  annotate(geom = \"text\", \n           x = positions[side==\"Left\"][order(V2), V1],\n           y = positions[side==\"Left\"][order(V2), V2] + .1,\n           label = 1:8, col = rgb(0,0.3,0.3,1)) +\n  annotate(geom = \"text\", \n           x = positions[side==\"Right\"][order(V1), V1] + .05,\n           y = positions[side==\"Right\"][order(V1), V2],\n           label = 1:8, col = rgb(0.3,0,0.3,1)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n\n\n\n\n\n\n\n\n\n\n\nAcceptance and Termination\nFinally, we need to decide whether to accept the new sample from the trajectory. This is done using the Metropolis-Hastings acceptance criterion, comparing the joint probability of the initial state and the proposed state. The process is repeated until the desired number of samples is obtained.\n\n# Metropolis-Hastings acceptance probability\nacceptance_prob &lt;- function(current_logp, proposed_logp) {\n  min(1, exp(proposed_logp - current_logp))\n}\n\nWe will incorporate this acceptance step into the full NUTS sampler in the next section, along with the complete NUTS algorithm and the adaptive step size procedure.\n\n\nAdaptative Step-Size\nOne of the most crucial aspects of NUTS (and HMC in general) is choosing an appropriate step size, \\(\\epsilon\\). A good step size allows the sampler to efficiently explore the posterior distribution. Too small, and the sampler takes tiny, slow steps, like a snail on a marathon. Too large, and it overshoots, bouncing around erratically, like a caffeinated kangaroo on a trampoline.\nNUTS uses an adaptive step size procedure, especially during the warmup phase, to find a good value for \\(\\epsilon\\). The goal is to achieve a target acceptance rate, typically around 0.8. This indicates that the sampler is neither accepting too many nor too few proposals.\nA simple way to adapt the step size is to use the following update rule:\n\\[\n\\epsilon \\leftarrow \\epsilon \\cdot e^{\\gamma \\cdot \\delta} \\\\\n\\]\nWhere \\(\\epsilon\\) is the current step size; \\(\\gamma\\) is a tuning parameter that controls the adaptation rate (how fast \\(\\epsilon\\) will adapt); \\(\\delta\\) is the difference between the current acceptance rate \\(\\lambda_{\\text{CA}}\\) and the target acceptance rate \\(\\lambda_{\\text{TA}}\\), and could be defined like this:\n\\[\n\\delta = \\lambda_{\\text{CA}} - \\lambda_{\\text{TA}}\n\\]\nThis simplified form allow us to reduce epsilon at each step as long as the current acceptance rate \\(\\lambda_{\\text{CA}}\\) is lower than the target acceptance rate \\(\\lambda_{\\text{TA}}\\). Here’s the R code for this adaptation:\n\nadapt_epsilon &lt;- function(epsilon, lambda_ca, lambda_ta = 0.8, gamma = 0.5) {\n  delta &lt;- lambda_ca - lambda_ta\n  epsilon &lt;- epsilon * exp(gamma * delta)\n  epsilon\n}\n\nThis update rule adjusts the step size based on the difference between the observed and target acceptance rates. If the acceptance rate is too low, (\\(\\epsilon\\)) is decreased; if it’s too high, (\\(\\epsilon\\)) is increased. The gamma (\\(\\gamma\\)) parameter controls how aggressively this adjustment is made.\nTo see how this adaptation works in practice, let’s visualize its dynamics:\n\n\nCode\nepsilons &lt;- numeric(50)\n  accept_rates &lt;- seq(0, 0.9, length.out = 50)\n  epsilons[1] &lt;- 1.0\n  gammas &lt;- seq(0, 0.30, by = 0.025)\n  plot_data &lt;- list()\n  \n  for (i in seq_along(gammas)) {\n    for (j in 2:length(epsilons)) {\n      epsilons[j] &lt;- adapt_epsilon(\n        epsilon = epsilons[j - 1], \n        lambda_ca = accept_rates[j], \n        lambda_ta = 0.8,\n        gamma = gammas[i])\n    }\n    plot_data[[i]] &lt;- data.table::data.table(\n      iteration = 1:50,\n      epsilon = epsilons,\n      gamma = gammas[i],\n      accept_rate = accept_rates\n    )\n  }\n  \n  plot_data &lt;- data.table::rbindlist(plot_data)\n  \n  ggplot(plot_data, aes(x = iteration, y = epsilon, col = gamma, group = gamma)) +\n    geom_line(aes(linewidth = accept_rate, alpha = accept_rate)) +\n    labs(title = \"Step-Size Adaptation\", x = \"Iteration\", \n         y = \"Epsilon\", col = expression(\"Adaptation Rate (\"*gamma*\")\")) +\n    geom_hline(yintercept = 1, linewidth = 1/2, color = \"gray50\", linetype = 2) +\n    scale_alpha_continuous(range = c(0.3,1), name = \"Acceptance Rate\",\n                           labels = scales::label_percent(),\n                           limits = c(0, .9), \n                           breaks = c(0, .3, .6, .9)) +\n    scale_linewidth_continuous(range = c(0.1, 1.5), \n                               name = NULL, labels = NULL, breaks = NULL) +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_color_viridis_c(limits = c(0, .31))\n\n\n\n\n\n\n\n\n\nAs the plot shows, the adaptation is more aggressive (epsilon changes more rapidly) with larger values of \\(\\gamma\\). Note that this is a very basic adaptation strategy. More robust methods, such as dual averaging, are typically used in practice, but this simple method illustrates the basic idea.\n\n\n\nImplementation Strategy\nNow that we have all the individual components, it’s time to assemble them into a fully functional NUTS sampler. We’ll also incorporate the adaptive step size tuning, which is crucial for NUTS’s performance.\n\n# Full NUTS sampler\nnuts &lt;- function(log_posterior, grad_log_posterior, \n                 q0, Sigma, mu, \n                 epsilon, lambda_ta, gamma,\n                 n_samples = 1000, n_warmup = 500) {\n  # Initialization\n  nuts_state &lt;- nuts_init(q0, Sigma, mu, epsilon)\n  samples &lt;- matrix(0, nrow = n_samples + n_warmup, ncol = length(q0))\n  \n  # Warmup phase for step size adaptation\n  for (i in 1:n_warmup) {\n    nuts_state$p &lt;- rnorm(length(q0), 0, 1) # Resample momentum\n    trajectory &lt;- build_trajectory(nuts_state$q, nuts_state$p, grad_log_posterior, nuts_state$epsilon, Sigma, mu)\n    \n    #Metropolis Hastings\n    current_logp &lt;- log_posterior(nuts_state$q, Sigma, mu)\n    proposed_logp &lt;- log_posterior(trajectory$q, Sigma, mu)\n    \n    acc_rate &lt;- acceptance_prob(current_logp, proposed_logp)\n    \n    if(runif(1) &lt; acc_rate){\n      nuts_state$q &lt;- trajectory$q\n    }\n    \n    ## Adaptative epsilon\n    nuts_state$epsilon &lt;- adapt_epsilon(epsilon, acc_rate, lambda_ta, gamma)\n    \n    samples[i, ] &lt;- nuts_state$q\n  }\n  \n  # Sampling phase\n  for (i in 1:n_samples) {\n    nuts_state$p &lt;- rnorm(length(q0), 0, 1) # Resample momentum\n    trajectory &lt;- build_trajectory(nuts_state$q, nuts_state$p, grad_log_posterior, nuts_state$epsilon, Sigma, mu)\n    \n    #Metropolis Hastings\n    current_logp &lt;- log_posterior(nuts_state$q, Sigma, mu)\n    proposed_logp &lt;- log_posterior(trajectory$q, Sigma, mu)\n    \n    if(runif(1) &lt; acceptance_prob(current_logp, proposed_logp)){\n      nuts_state$q &lt;- trajectory$q\n    }\n    \n    samples[i + n_warmup, ] &lt;- nuts_state$q\n  }\n  \n  samples\n}\n\nHere’s a breakdown of the full nuts function:\n\nInitialization: We initialize the NUTS state using the nuts_init() function.\nWarmup: The first n_warmup iterations are used for tuning the step size. A very basic adaptation is implemented, increasing the step size in the first half of the warmup and decreasing it in the second half. More sophisticated methods exist (like dual averaging).\nSampling: The main sampling loop runs for n_samples iterations. In each iteration:\n\nWe build a trajectory using build_trajectory().\nWe perform the Metropolis Hastings step, to accept or reject the new proposed sample\nWe store the current position in the samples matrix.\n\n\nThis implementation provides a basic but functional NUTS sampler. In the next section, we’ll discuss diagnostics and validation to ensure our sampler is working correctly.\n\n\n\nIs Our Sampler Behaving Itself?\nIn the following section, we’ll try to estimate the location parameters of a 2-dimensional multivariate normal, we’ll call them \\(\\theta_1\\) and \\(\\theta_2\\) respectively. They will have a true values of \\(\\theta_1 = 1\\) and \\(\\theta_2 = -1\\), and the variance-covariance matrix (\\(\\Sigma\\)) will be defined as:\n\\[\n\\Sigma =\n  \\begin{pmatrix}\n    1.0 & 0.5 \\\\\n    0.5 & 1.0\n  \\end{pmatrix}\n\\]\n\n## Initial parameters\nq0 &lt;- c(0, 0)    # Starting positions\nmu &lt;- c(1, -1)   # Target location of the distribution\nepsilon &lt;- 0.5   # (Initial) Step size for HMC exploring\nlambda_ta &lt;- .75 # Target acceptance rate\ngamma &lt;- 0.5     # Adaptation rate for epsilon\n\n# Variance-Covariance Matrix\nSigma &lt;- matrix(\n  data = c(1, 0.5, \n           0.5, 1), \n  nrow = 2\n)\n\nset.seed(1234) # Seed for reproducibility\n\n# Our main function ,  HMC-NUTS sampler\nsamples &lt;- nuts(\n  log_posterior, grad_log_posterior,         # Distribution functions\n  q0, Sigma, mu,                             # Target parameters\n  epsilon, lambda_ta, gamma,\n  n_samples = 2000, n_warmup = 2000 # Sampler hyperparameter\n)\n\nNow that we have a working NUTS sampler, we need to make sure it’s actually doing what it’s supposed to do. This section will cover several diagnostic techniques, both visual and numerical, to validate our sampler.\n\nVisual Diagnostics\nThe first step in diagnosing our sampler is to visualize the results. This gives us a quick and intuitive understanding of its behavior. Two common visual diagnostics are trace plots and posterior density estimates.\n\nTrace Plots\nTrace plots show the value of each parameter over the course of the sampling process. Ideally, trace plots should look like “white noise” (random fluctuations around a stable mean). This indicates that the sampler is exploring the posterior well and not getting stuck in any particular region. If you see trends, patterns, or long periods of near-constant values, it could indicate problems with convergence.\n\n\nCode\nplot_df &lt;- as.data.table(samples)\nplot_df[, iter := seq_len(.N)]\n\nplot_df &lt;- melt(plot_df, id.vars = \"iter\")\n\nggplot(plot_df, aes(iter, value, col = variable)) +\n  annotate(geom = \"ribbon\", alpha = 1/5,\n           x = c(0, 2000), ymin = -Inf, ymax = Inf) +\n  annotate(geom = \"text\", x = 1950, y = -4, \n           label = \"\\\"Burn-in\\\" Period\", hjust = 1) +\n  geom_line(linewidth = 1/2) +\n  geom_hline(yintercept = c(-1,1), linetype = 2) +\n  labs(color = \"Parameter\", y = expression(theta), x = \"Iterations\",\n       title = \"Traceplot of Parameter Values\",\n       subtitle = \"Over Effective and Burn-in Samples\") +\n  scale_color_manual(values = c(rgb(0,.5,.5,1),rgb(.5,0,.5,1)),\n                     labels = c(\"theta[1]\", \"theta[2]\") |&gt; \n                       scales::label_parse()()) +\n  scale_y_continuous(expand = c(0.1,0)) +\n  scale_x_continuous(expand = c(0,0))\n\n\n\n\n\nTraceplots indicating the evolution and convergence of our model parameters (\\(\\theta_1\\) and \\(\\theta_2\\)) over time.\n\n\n\n\n\n\nPosterior Density Estimates\nThese plots show the estimated density of the samples, giving us an idea of the shape of the posterior distribution. We can compare these estimates to the true posterior distribution (if we know it) to check if our sampler is capturing the correct shape.\n\n\nCode\n#True density\nxy_seq &lt;- seq(-4, 4, by = .1)\ndens_df &lt;- expand.grid(x = xy_seq, y = xy_seq)\ndens_df$z &lt;- mvtnorm::dmvnorm(dens_df, mu, Sigma)\n\nggplot(data = data.frame(samples), aes(x = X1, X2))+\n  geom_contour(data = dens_df, aes(x, y, z = z), \n               col = \"gray40\", linetype = 2) +\n  geom_path(alpha = .2, col = \"gray\") +\n  geom_point(col = rgb(0,0.5,0.5,1), alpha = 1/5, pch = 16) +\n  labs(x = expression(theta[2]), y = expression(theta[1]),\n       title = expression(\"Joint Posterior Distribution of\"~theta))\n\n\n\n\n\nJoint posterior distribution of our model parameters (\\(\\theta_1\\) and \\(\\theta_2\\)). The samples are connected based on the sampling ordering.\n\n\n\n\nBy visually inspecting these plots, we can get a good sense of whether our NUTS sampler is working as expected. However, visual diagnostics are subjective. We also need numerical diagnostics to provide more objective measures of sampler performance.\n\n\n\nNumerical Diagnostics\nWhile visual diagnostics provide a valuable qualitative assessment of our sampler’s performance, we also need quantitative metrics to be more objective. Two key numerical diagnostics are effective sample size (ESS) and R-hat (\\(\\hat{R}\\)).\n\nEffective Sample Size (ESS)\nThe ESS measures the number of independent samples that our sampler has effectively generated. Because MCMC samples are correlated, the ESS is typically smaller than the actual number of samples. A higher ESS indicates better sampler efficiency. A rule of thumb is that you want an ESS of at least a few hundred for each parameter to have reliable estimates of the posterior.\nWe can estimate the ESS using various methods. A common approach is to use the effectiveSize() function from the coda package in R:\n\n# Calculate ESS for each parameter\ncoda::mcmc(samples, start = 2000 + 1) |&gt; \n  coda::effectiveSize()\n\n    var1     var2 \n511.6659 508.3677 \n\n\n\n\nGelman and Rubin’s Convergence Diagnostic (\\(\\hat{R}\\))\nThe \\(\\hat{R}\\) statistic (pronounced “R-hat”) is a convergence diagnostic that compares the variance within multiple chains to the variance between chains. If the chains have converged to the same distribution, the within-chain variance should be similar to the between-chain variance, and \\(\\hat{R}\\) should be close to 1. Values of \\(\\hat{R}\\) much greater than 1 (e.g., above 1.1) indicate that the chains have not converged.\nHowever, to compute \\(\\hat{R}\\) we need more than one chain, so let’s run our HMC-NUTS model four times to get independent runs or “chains”:\n\nn_chains &lt;- 4 # Number of independent chains\n\nset.seed(1234) # Seed for reproducibility\n\n# Our main function within a lapply() statement for looping\nnuts_model &lt;- lapply(seq_len(n_chains), function(x) {\n  samples &lt;- nuts(\n    log_posterior, grad_log_posterior,\n    q0, Sigma, mu,\n    epsilon, lambda_ta, gamma,\n    n_samples = 2000, n_warmup = 2000\n  )\n  `dimnames&lt;-`(samples, list(NULL, c(\"theta_1\", \"theta_2\")))\n})\n\nWe can now calculate \\(\\hat{R}\\) using the gelman.diag() function from the coda package:\n\n# Calculate R-hat\nnuts_model |&gt; \n  lapply(coda::mcmc, start = 2e3 + 1) |&gt; \n  coda::mcmc.list() |&gt; \n  coda::gelman.diag() |&gt; \n  lapply(round,3)\n\n$psrf\n        Point est. Upper C.I.\ntheta_1      1.003      1.009\ntheta_2      1.001      1.003\n\n$mpsrf\n[1] 1.004\n\n\nAs you can see, all \\(\\hat{R}\\) are well below the 1.1 threshold, which suggests that the chains are mixing well enough.\nWith these visual and numerical diagnostics, we can be confident about the performance of our NUTS sampler. These tools allow us to assess convergence, efficiency, and overall sampler behavior, ensuring that we are obtaining reliable samples from our target posterior distribution.\n\n\n\n\nMaking It Sing in the Real World\nNow that we have a basic NUTS sampler, let’s talk about how to fine-tune it for real-world applications. Because, let’s face it, toy examples are fun, but real-world problems are where the real challenges (and rewards) lie.\nWhen applying NUTS to real-world problems, there are a few practical considerations to keep in mind: (1) choosing an initial step size, (2) setting the maximum tree depth, and (3) understanding the computational trade-offs.\n\nChoosing an Initial Step Size\nWhile NUTS adapts the step size during warmup, a good initial step size can significantly improve the efficiency of the adaptation process. A common approach is to perform a short pilot run (a few hundred iterations) and adjust the initial step size based on the acceptance rate. If the acceptance rate is too low (e.g., below 0.6), decrease the initial step size. If it’s too high (e.g., above 0.9), increase it.\nIn our implementation, we used a very simple adaptation strategy. In real-world applications, more robust methods like dual averaging are used. These methods are more stable and efficient at finding a good step size.\n Always be careful on the stepsize you choose! Photo from Coach The Run.\n\n\nSetting the Maximum Tree Depth\nThe maximum tree depth limits the maximum trajectory length that NUTS will explore. A larger tree depth allows for longer trajectories, which can be beneficial for exploring complex posteriors. However, it also increases exponentially the computational cost. A common default value is 10, which allows for trajectories up to \\(2^{10} = 1024\\) leapfrog steps.\nIt is important to notice that a too small tree depth can lead to poor exploration of the parameter space, and a too large tree depth can lead to unnecessary computational burden. Therefore, it is important to choose a tree depth that is appropriate for the problem at hand.\n\nComputational Trade-offs\nNUTS, like other HMC methods, involves computational trade-offs between memory, runtime, and efficiency. Longer trajectories (achieved by larger tree depths or smaller step sizes) can lead to better exploration and lower autocorrelation but also increase the runtime and memory usage. It’s important to find a balance that works for your specific problem and computational resources.\nIn practice, you’ll often need to experiment with different settings to find the optimal balance for your problem. This might involve running several short test runs with different parameter values and comparing the results.\n My computer on an average Bayesian modeling session. Photo from OneSupport.\n\n\n\nWhen Things Go Wrong\nEven with its adaptive nature, NUTS can sometimes struggle with certain types of posterior distributions. This subsection will discuss some common pitfalls and techniques to address them.\n\nPathological Posteriors\nCertain characteristics of posterior distributions can make sampling challenging for any MCMC method, including NUTS. Some of the most common culprits are:\nExtreme Curvature. If the posterior has regions of very high curvature (sharp turns or narrow valleys), NUTS might have trouble navigating these areas. The leapfrog integrator can become inaccurate, leading to poor exploration.\nMultimodality. As we discussed earlier, multimodal posteriors (with multiple peaks) can be difficult for any sampler. NUTS can sometimes get stuck in one mode and fail to explore the others.\nDiscontinuities or Sharp Changes. If the posterior has discontinuities or sharp changes in density, the gradient information used by NUTS can be misleading, leading to inefficient sampling.\n\n\nTechniques to Stabilize NUTS\nWhen faced with these challenging posteriors, there are several techniques you can try to stabilize NUTS:\nReparameterization. Sometimes, reparameterizing the model can make the posterior easier to sample. This involves transforming the parameters of the model to a new set of parameters that result in a smoother or more well-behaved posterior. For example, if you have a parameter that is constrained to be positive, you could reparameterize it using a logarithmic transformation.\nIncreasing Tree Depth. Increasing the maximum tree depth can allow NUTS to explore more complex trajectories and potentially escape from narrow valleys or explore different modes. However, this comes at the cost of increased computational cost.\nUsing a More Robust Step Size Adaptation. As mentioned before, using more sophisticated methods for step size adaptation, like dual averaging, can significantly improve NUTS’s performance. These methods are less sensitive to the initial step size and can adapt more efficiently to the local geometry of the posterior.\nPrior Choice. In some cases, a carefully chosen prior distribution can help to regularize the posterior and make it easier to sample.\nIt’s important to remember that there’s no one-size-fits-all solution for these problems. You’ll often need to experiment with different techniques to find what works best for your specific problem. Careful diagnostics, both visual and numerical, are crucial for identifying potential issues and evaluating the effectiveness of your solutions.\n Photo from imgflip.\n\n\n\nTaking It to the Next Level\nWe’ve covered the basics of NUTS implementation and fine-tuning. Now, let’s explore some advanced tips for using NUTS in more complex scenarios and leveraging existing tools.\n\nStanding on the Shoulders of Giants\nImplementing NUTS from scratch, as we’ve done in this post, is a great way to understand its inner workings. However, for most real-world applications, it’s recommended to use well-established probabilistic programming languages that have highly optimized and thoroughly tested NUTS implementations. Some popular choices include:\nStan. Stan is a powerful probabilistic programming language specifically designed for Bayesian modeling. It features a highly optimized implementation of NUTS, along with many other advanced features like automatic differentiation and robust step size adaptation (using dual averaging). Stan is often the go-to choice for serious Bayesian work.\nPyMC. PyMC is a Python library for Bayesian statistical modeling. It provides a user-friendly interface for building and fitting Bayesian models, including NUTS sampling. PyMC is a good option if you prefer working in Python.\nTuring.jl. Turing.jl is a probabilistic programming language written in Julia. It also provides an efficient implementation of NUTS, and Julia’s performance makes it a good option for computationally intensive models.\nUsing these tools saves you the trouble of implementing NUTS yourself and gives you access to highly optimized and robust implementations.\n\n\nThe Future of NUTS\nThe field of Bayesian computation is constantly evolving, and there are several exciting innovations related to NUTS. Recent research has focused on accelerating NUTS using GPUs (Amaral et al. 2025; Saltas and Oliveri 2025). This can significantly speed up sampling, especially for high-dimensional models. Libraries like NumPyro (built on JAX) and some developments in Stan are exploring this direction (Sountsov, Carroll, and Hoffman 2024).\nResearchers are continually developing more efficient and robust methods for step size adaptation. These methods aim to improve the performance of NUTS in challenging scenarios. There are also variations of NUTS that aim to address specific challenges, such as sampling from posteriors with very high curvature. These advanced tips can help you take your Bayesian modeling to the next level. Using established tools like Stan, PyMC, or Turing.jl will save you time and provide access to highly optimized implementations.\n Photo from xfxwallpapers.\n\n\n\n\nWhere Do We Go From Here?\nWe’ve explored the depths of NUTS, from its underlying principles to its implementation and fine-tuning. But the journey doesn’t end here. NUTS is a powerful tool with broad applicability, and there are still many exciting avenues for exploration and development.\n\nExpanding the Horizons of NUTS\nNUTS truly shines when applied to complex Bayesian models that pose significant challenges for traditional MCMC methods. Here are some key areas where NUTS has proven particularly valuable.\nHierarchical models, with their nested structures and multiple levels of uncertainty, are notoriously difficult to sample from. NUTS’s ability to efficiently explore high-dimensional spaces and adapt to complex geometries makes it an ideal choice for these models. Whether you’re modeling population dynamics, analyzing clinical trial data, or building complex econometric models, NUTS can handle the intricate dependencies inherent in hierarchical structures.\nIn Bayesian modeling, we often choose conjugate priors for mathematical convenience. However, these priors might not always be the most appropriate for our problem. Non-conjugate priors, while more flexible, often lead to intractable posteriors that are difficult to sample from. NUTS, with its gradient-based approach, can effectively handle these non-conjugate scenarios, allowing us to use more realistic and flexible priors.\nAs the number of parameters in our model increases, the dimensionality of the posterior space grows rapidly. This can make sampling extremely challenging for traditional MCMC methods. NUTS’s efficient exploration and adaptive trajectory length make it well-suited for high-dimensional problems. Whether you’re working with image analysis, genomics data, or large-scale machine learning models, NUTS can help you navigate these vast parameter spaces.\nThese are just a few examples of the many areas where NUTS can be applied. Its versatility and efficiency make it a valuable tool for a wide range of Bayesian modeling tasks.\n\n\nYour Turn to Go NUTS!\nNow that you’ve grasped the core concepts and implementation of NUTS, I want to leave you with an open challenge: how would you adapt NUTS to your own specific problems?\nThink about the types of models you work with. Do you encounter any of the challenges we’ve discussed, such as high dimensionality, complex geometries, or non-conjugate priors? How could you leverage NUTS to improve your sampling efficiency and obtain more reliable results?\nConsider this questions:\n\nWhat types of posterior distributions do you typically encounter? Are they relatively simple and well-behaved, or do they exhibit complex features like multimodality or high curvature?\nWhat are the computational constraints of your problems? Do you have limited memory or runtime, which might influence your choice of tree depth or other NUTS parameters?\nCould reparameterization help to improve sampling in your models? Are there any transformations you could apply to your parameters to make the posterior smoother or more amenable to NUTS?\nAre there any specific adaptations or extensions of NUTS that might be relevant to your work? For example, if you’re working with very high-dimensional models, you might consider exploring GPU-accelerated NUTS implementations.\n\nDon’t be afraid to experiment and try different approaches. The best way to learn is by doing. Implement NUTS in your own projects, explore different parameter settings, and carefully analyze the results. You might discover new and creative ways to apply this powerful sampling technique.\n\n\nLooking Ahead\nThis post has provided a deep dive into the No-U-Turn Sampler, from its theoretical foundations to its practical implementation in R. We’ve seen how NUTS addresses the limitations of traditional MCMC methods and fixed-step HMC, offering a powerful and efficient approach to Bayesian inference.\nBut the world of Bayesian statistics is constantly evolving, and there’s always more to learn, like Hierarchical Modeling and demonstrate how NUTS can be effectively applied to these complex structures. Or maybe we can mess around with Stan code directly to gain performance.\nI hope this post has given you a solid understanding of NUTS and inspired you to explore its potential in your own work. Stay tuned for future posts as we continue to dig into the fascinating world of Bayesian statistics!\n\n\n\n\nAppendix\n\nHere is the file with all the R functions used in this blog post. Feel free to use and modify them according to your needs. Happy coding!\n\n\n\nReferences\n\nAmaral, Dorian, Shixiao Liang, Juehang Qin, and Christopher Tunnell. 2025. “Fast Bayesian Inference for Neutrino Non-Standard Interactions at Dark Matter Direct Detection Experiments.” Machine Learning: Science and Technology.\n\n\nSaltas, Ippocratis, and Roberto Oliveri. 2025. “EMRI_MC: A GPU-Based Code for Bayesian Inference of EMRI Waveforms.” SciPost Physics Codebases, 044.\n\n\nSountsov, Pavel, Colin Carroll, and Matthew D Hoffman. 2024. “Running Markov Chain Monte Carlo on Modern Hardware and Software.” arXiv Preprint arXiv:2411.04260.\n\nCitationBibTeX citation:@misc{castillo-aguilar2025,\n  author = {Castillo-Aguilar, Matías},\n  title = {Going {NUTS:} {A} {Step-by-Step} {Guide} to {Adaptive}\n    {Hamiltonian} {Sampling} in {R}},\n  date = {2025-01-21},\n  url = {https://bayesically-speaking.com/posts/2025-01-21 hmc-nuts-from-zero/},\n  doi = {10.59350/hnk21-ggm53},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2025. “Going NUTS: A Step-by-Step Guide\nto Adaptive Hamiltonian Sampling in R.” January 21, 2025. https://doi.org/10.59350/hnk21-ggm53."
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html",
    "href": "posts/2024-05-15 mcmc part 2/index.html",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Introduction\nHey there, fellow science enthusiasts and stats geeks! Welcome back to the wild world of Markov Chain Monte Carlo (MCMC) algorithms. This is part two of my series on the powerhouse behind Bayesian Inference. If you missed the first post, no worries! Just hop on over here and catch up before we dive deeper into the MCMC madness. Today, we’re exploring the notorious Hamiltonian Monte Carlo (HMC), a special kind of MCMC algorithm that taps into the dynamics of Hamiltonian mechanics.\n\nStats Meets Physics?\nHold up, did you say Hamiltonian mechanics? What in the world do mechanics and physics have to do with Bayesian stats? I get it, it sounds like a mashup of your wildest nightmares. But trust me, this algorithm sometimes feels like running a physics simulation in a statistical playground. Remember our chat from the last post? In Bayesian stats, we’re all about estimating the shape of a parameter space, aka the posterior distribution.\n\nFun fact: There’s this whole field called statistical mechanics where scientists mix stats and physics to solve cool problems, mostly related to thermodynamics and quantum mechanics.\n\n\n\nA Particle Rolling Through Stats Land\nPicture this: You drop a tiny particle down a cliff, and it rolls naturally along the landscape’s curves and slopes. Easy, right? Now, swap out the real-world terrain for a funky high-dimensional probability function. That same little particle? It’s cruising through this wild statistical landscape like a boss, all thanks to the rules of Hamiltonian mechanics.\n\n\n\n\n\n\n\n\n\n\nAbout the animation\n\n\n\n\n\nThe previous animation illustrate the Hamiltonian dynamics of a particle traveling a two-dimensional parameter space. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo\n\n\n\n\n\n\nHamiltonian Mechanics: A Child’s Play?\nLet’s break down Hamiltonian dynamics in terms of position and momentum with a fun scenario: Imagine you’re on a swing. When you hit the highest point, you slow down, right? Your momentum’s almost zero. But here’s the kicker: You know you’re about to pick up speed on the way down, gaining momentum in the opposite direction. That moment when you’re at the top, almost motionless? That’s when you’re losing kinetic energy and gaining potential energy, thanks to gravity getting ready to pull you back down.\n\n\n\nSwing Animation\n\n\nSo, in this analogy, when your kinetic energy (think swing momentum) goes up, your potential energy (like being at the bottom of the swing) goes down. And vice versa! When your kinetic energy drops (like when you’re climbing back up), your potential energy shoots up, waiting for gravity to do its thing.\nThis energy dance is captured by the Hamiltonian (\\(H(q, p)\\)), which sums up the total energy in the system. It’s the sum of kinetic energy (\\(K(p)\\)) and potential energy (\\(U(q)\\)):\n\\[\nH(q, p) = K(p) + U(q)\n\\]\nAt its core, Hamiltonian Monte Carlo (HMC) borrows from Hamiltonian dynamics, a fancy term for the rules that govern how physical systems evolve in phase space. In Hamiltonian mechanics, a system’s all about its position (\\(q\\)) and momentum (\\(p\\)), and their dance is choreographed by Hamilton’s equations. Brace yourself, things are about to get a little mathy:\n\\[\n\\begin{align}\n\\frac{{dq}}{{dt}} &= \\frac{{\\partial H}}{{\\partial p}} \\\\\n\\frac{{dp}}{{dt}} &= -\\frac{{\\partial H}}{{\\partial q}}\n\\end{align}\n\\]\n\nWrapping Our Heads Around the Math\nOkay, I know Hamiltonian dynamics can be a real brain-buster, trust me, it took me a hot minute to wrap my head around it. But hey, I’ve got an analogy that might just make it click. Let’s revisit our swing scenario: remember our picture of a kid on a swing, right? The swing’s angle from the vertical (\\(q\\)) tells us where the kid is, and momentum (\\(p\\)) is how fast the swing’s moving.\nNow, let’s break down those equations:\n\\[\n\\frac{{dq}}{{dt}} = \\frac{{\\partial H}}{{\\partial p}}\n\\]\nThis one’s like peeking into the future to see how the angle (\\(q\\)) changes over time. And guess what? It’s all about momentum (\\(p\\)). The faster the swing’s going, the quicker it swings back and forth, simple as that!\nNext up:\n\\[\n\\frac{{dp}}{{dt}} = -\\frac{{\\partial H}}{{\\partial q}}\n\\]\nNow, this beauty tells us how momentum (\\(p\\)) changes over time. It’s all about the energy game here, specifically, how the swing’s position (\\(q\\)) affects its momentum. When the swing’s at the highest point, gravity’s pulling hardest, ready to send him back down.\nSo, picture this:\n\nThe kid swings forward, so the angle (\\(q\\)) goes up thanks to the momentum (\\(p\\)) building until bam, top of the swing.\nAt the top, the swing’s momentarily still, but gravity’s pulling to send him flying back down, hence, he is accumulating potential energy.\nZoom! Back down it goes, picking up speed in the opposite direction, and so, the potential energy is then transferred into kinetic energy.\n\nAll the while, the Hamiltonian (\\(H\\)) is keeping tabs on the swing’s total energy, whether it’s zooming at the bottom (high kinetic energy \\(K(p)\\), as a function of momentum \\(p\\)) or pausing at the top (high potential energy \\(U(q)\\), as a function of position \\(q\\)).\nThis dance between kinetic and potential energy is what we care within Hamiltonian mechanics, and also what we mean when we refer to the phase space, which it’s nothing more than the relationship between position and momentum.\n\n\n\nVisualizing Hamilton’s Equations\nOkay, I know we’re diving into some physics territory here in a stats blog, but trust me, understanding these concepts is key to unlocking what HMC’s all about. So, let’s take a little side trip and get a feel for Hamilton’s equations with a different example. Check out the gif below, see that weight on a string? It’s doing this cool back-and-forth dance thanks to the tug-of-war between the string pulling up and gravity pulling down.\n\n\n\nSimple harmonic oscillator. Within this example we could expect that the potential energy \\(U(q)\\) is the greatest at the bottom or top positions \\(q\\), primarely because is in these positions that the force exerted by the string is greater, affecting in consequence the kinetic energy \\(K(p)\\) of the mass attached at the bottom of the string.\n\n\nNow, let’s get a little hands-on with some code. We’re gonna simulate a simple harmonic oscillator (you know, like that weight on a string) and watch how it moves through phase space.\n\n# Define the potential energy function (U) and its derivative (dU/dq)\nU &lt;- function(q) {\n  k &lt;- 1  # Spring constant\n  return(0.5 * k * q^2)\n}\n\n\ndU_dq &lt;- function(q) {\n  k &lt;- 1  # Spring constant\n  return(k * q)\n}\n\n# Kinetic energy (K) used for later\nK &lt;- function(p, m) {\n  return(p^2 / (2 * m))\n}\n\n# Introduce a damping coefficient\nb &lt;- 0.1  # Damping coefficient\n\n# Set up initial conditions\nq &lt;- -3.0  # Initial position\np &lt;- 0.0   # Initial momentum\nm &lt;- 1.0   # Mass\n\n# Time parameters\nt_max &lt;- 20\ndt &lt;- 0.1\nnum_steps &lt;- ceiling(t_max / dt)  # Ensure num_steps is an integer\n\n# Initialize arrays to store position and momentum values over time\nq_values &lt;- numeric(num_steps)\np_values &lt;- numeric(num_steps)\n\n# Perform time integration using the leapfrog method\nfor (i in 1:num_steps) {\n  # Store the current values\n  q_values[i] &lt;- q\n  p_values[i] &lt;- p\n  \n  # Half step update for momentum with damping\n  p_half_step &lt;- p - 0.5 * dt * (dU_dq(q) + b * p / m)\n  \n  # Full step update for position using the momentum from the half step\n  q &lt;- q + dt * (p_half_step / m)\n  \n  # Another half step update for momentum with damping using the new position\n  p &lt;- p_half_step - 0.5 * dt * (dU_dq(q) + b * p_half_step / m)\n}\n\n\nIn this code:\n\nU is the potential energy function, kinda like how much energy’s stored in that spring, if you will.\nK is the kinetic energy function, telling us how much energy’s tied up in the speed of the weight.\ndU_dq is telling us about how the potential energy changes with the weight’s position, aka the force.\nb is just a fancy way of saying how much energy the system loses over time.\nq and p are the weight’s position and momentum, respectively.\nm is the weight’s mass, nothing fancy.\ndt is the time step, and num_steps? Well, that’s just how long we’re gonna keep this simulation running.\nOh, and that leapfrog integration? It’s like stepping through time, updating the momentum, then the position, and back again, but don’t worry about it, we’ll see it shortly.\n\n\n\n\nCode\nharmonic_data &lt;- data.table(`Position (q)` = q_values, `Momentum (p)` = p_values)\n\nggplot(harmonic_data, aes(`Position (q)`, `Momentum (p)`)) +\n  geom_path(linewidth = .7) +\n  geom_point(size = 2) +\n  geom_point(data = harmonic_data[1,], col = \"red\", size = 3) +\n  labs(title = \"Phase Space Trajectory of Hamiltonian Dynamics\",\n       subtitle = \"Accounting for Energy Loss\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\n\n\nCheck out this funky phase space trajectory of a simple harmonic oscillator! At the furthest points, the potential energy is the highest, that’s what’s driving the oscillator back and forth. But in the middle? That’s where the kinetic energy’s taking over, keeping things moving, like our example of a kid on a swing!\n\nNow, take a look at that graphic. See how the position (\\(q\\)) is all about where the oscillator’s hanging out, and momentum (\\(p\\))? Well, that’s just how fast the weight’s swinging. Put them together, and you’ve got what we call the phase space. Basically, it’s like peeking into the dance floor of these mechanical systems through the lenses of Hamiltonian dynamics.\nNow, in a perfect world, there’d be no energy lost over time. But hey, we like to keep it real, so we added a little something called damping effect (think of it like energy leaking out of the system over time). In the real world, that makes sense, but in our statistical playground, we want to keep that energy locked in tight. After all, losing energy means we’re losing precious info about our target distribution, and nobody wants that.\n\n\nCode\nhamiltonian &lt;- harmonic_data[, list(`Total energy` = U(`Position (q)`) + K(`Momentum (p)`, m),\n                                    `Kinetic energy` = K(`Momentum (p)`, m), \n                                    `Potential energy` = U(`Position (q)`))]\nhamiltonian &lt;- melt(hamiltonian, measure.vars = c(\"Total energy\", \"Kinetic energy\", \"Potential energy\")) \n\nggplot(hamiltonian, aes(rep(1:200, times = 3), value, col = variable)) +\n  geom_line(linewidth = 1) +\n  labs(y = \"Energy\", col = \"Variable\", x = \"Time\",\n       title = \"Fluctuation of the Total Energy in the Oscillator\",\n       subtitle = \"As a Function of Kinetic and Potential Energy\") +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  theme_classic(base_size = 20) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nCheck out this energy rollercoaster! This graph’s showing us how the total energy (made up of kinetic and potential energy) changes over time. And yep, that damping effect? It’s keeping things realistic, but in stats land, we’re all about conserving that energy for our exploration.\n\nSo, what’s the big takeaway here? Well, whether it’s a ball rolling down a hill or a sampler hunting for model coefficients, this framework’s got us covered. In Bayesian land, think of our model’s parameters as position coordinates \\(q\\) in some space, and \\(p\\) is the momentum helping us navigate the twists and turns of this parameter space. And with Hamiltonian dynamics leading the way, we’re guaranteed to find our path through this statistical dimension, one step at a time.\n\n\nBuilding our own Hamiltonian Monte Carlo\nNow that we got some intuition about Hamiltonian mechanics, it’s time we build our own HMC sampler. To accomplish this, imagine that we’re diving into some data to figure out how an independent variable (\\(x\\)) and a dependent variable (\\(y\\)) are related. We’re talking about linear regression (you know, trying to draw a line through those scattered data points to make sense of it all). But hey, this is Bayesian territory, so we’re not just throwing any ol’ line on that plot. No, sir! We’re exploring the parameter space of those regression coefficients (that’s the slope and intercept). Then, when the data rolls in, we’re smashing together that likelihood and those priors using Bayes’ theorem to cook up a posterior distribution (a fancy way of saying our updated beliefs about those coefficients after we’ve seen the data).\n\nCooking Up Some Data\nBut before we dive into the statistical kitchen, let’s whip up some synthetic data. Picture this: we’re mimicking a real-world scenario where relationships between variables are as murky as a foggy morning. So, we’re gonna conjure up a batch of data with a simple linear relationship, jazzed up with a sprinkle of noise. Oh, and let’s keep it small (just 20 subjects), ’cause, hey, science “loves” a manageable sample size.\n\nEmphasis on double quotes on “loves”.\n\n\nset.seed(80) # Set seed for reproducibility\n\n# Define the number of data points and the range of independent variable 'x'\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\n\nOkay, so now let’s get down to business and fit ourselves a nice, cozy linear relationship between an independent variable (\\(x\\)) and a dependent variable (\\(y\\)). We’re talking about laying down a straight line that best describes how \\(y\\) changes with \\(x\\).\nSo, what’s our equation look like? Well, it’s pretty simple:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot x_i \\\\\n\\end{aligned}\n\\]\nHold on, let’s break it down. We’re saying that each \\(y\\) value (\\(y_i\\)) is chillin’ around a mean (\\(\\mu_i\\)), like a bunch of friends at a party. And guess what? They’re all acting like good ol’ normal folks, hanging out with a variance (\\(\\sigma^2\\)) that tells us how spread out they are. Now, the cool part is how we define \\(\\mu_i\\). It’s just a simple sum of an intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)) times \\(x_i\\). Think of it like plotting points on graph paper, each \\(x_i\\) tells us where we are on the \\(x\\)-axis, and multiplying by \\(\\beta_1\\) gives us the corresponding height on the line.\nNow, let’s talk numbers. We’re setting \\(\\beta_0\\) to 2 because, hey, every relationship needs a starting point, right? And for \\(\\beta_1\\), we’re going with 3 (that’s the rate of change we’re expecting for every unit increase in \\(x\\). Oh, and let’s not forget about \\(\\sigma\\)), that’s just a fancy way of saying how much our \\(y\\) values are allowed to wiggle around.\n\n# Define the true parameters of the linear model and the noise level\ntrue_intercept &lt;- 2\ntrue_slope &lt;- 3\nsigma &lt;- 5\n\nWith our model all set up, it’s time to create some data points for our \\(y\\) variable. We’ll do this using the rnorm() function, which is like a magical data generator for normally distributed variables.\n\n# Generate the dependent variable 'y' with noise\nmu_i = true_intercept + true_slope * x\ny &lt;- rnorm(n, mu_i, sigma)\n\n\n\nChoosing a Target Distribution\nAlright, now that we’ve got our hands on some data, it’s time to dive into the nitty-gritty of Bayesian stuff. First up, we’re gonna need our trusty log likelihood function for our linear model. This function’s like the Sherlock Holmes of statistics, it figures out the probability of seeing our data given a specific set of parameters (you know, the intercept and slope we’re trying to estimate).\n\n# Define the log likelihood function for linear regression\nlog_likelihood &lt;- function(intercept, slope, x, y, sigma) {\n  \n  # We estimate the predicted response\n  y_pred &lt;- intercept + slope * x \n  \n  # Then we see how far from the observed value we are\n  residuals &lt;- y - y_pred\n  \n  # Then we estimate the likelihood associated with that error from a distribution\n  # with no error (mean = 0)\n  # (this is the function that we are trying to maximize)\n  log_likelihood &lt;- sum( dnorm(residuals, mean = 0, sd = sigma, log = TRUE) )\n  \n  return(log_likelihood)\n}\n\nSo, what’s the deal with priors? Well, think of them as the background music to our data party. They’re like our initial hunches about what the parameters could be before we’ve even glanced at the data. To keep things simple, we’ll go with flat priors (no favoritism towards any particular values). It’s like saying, “Hey, let’s give everyone a fair shot!”\n\n# Define the log prior function for the parameters\nlog_prior &lt;- function(intercept, slope) {\n  # Assuming flat priors for simplicity\n  # (the log of 0 is 1, so it has no effect)\n  return(0) \n}\n\nNow, here’s where the real magic kicks in. We bring our likelihood and priors together in a beautiful dance to reveal the superstar of Bayesian statistics: the posterior distribution! This bad boy tells us everything we wanna know after we’ve taken the data into account. This allow us to take into account previous knowledge (like past research, and the observed data) let’s say, samples from a new experiment.\nThe posterior it’s nothing more than the probability associated with our parameters of interest (aka. the slope and intercept), given the observed data. We represent this posterior distribution as the following:\n\\[\nP(\\text{X}|\\text{Data}) \\propto P(\\text{Data}|\\text{X}) \\cdot P(\\text{X})\n\\]\nWhich is the same as saying:\n\\[\n\\text{Posterior} \\propto \\text{Likelihood} \\cdot \\text{Prior}\n\\]\n\n# Combine log likelihood and log prior to get log posterior\nlog_posterior &lt;- function(intercept, slope, x, y, sigma) {\n  return(log_likelihood(intercept, slope, x, y, sigma) + log_prior(intercept, slope))\n}\n\n\n\nBuilding the HMC sampler\nAlright, now that we have everything ready, let’vs dig into the guts of HMC and how we put it to work. Remember how HMC takes inspiration from the momentum and potential energy dance in physics? Well, in practice, it’s like having a GPS for our parameter space, guiding us to new spots that are more likely than others.\nBut here’s the thing: our parameter space isn’t some smooth highway we can cruise along endlessly. Nope, it’s more like a rugged terrain full of twists and turns. So, how do we navigate this space? Enter the leapfrog integration method, the backbone of HMC.\n\n\nLeapfrog Integration\nSo, leapfrog integration is basically this cool math trick we use in HMC to play out how a system moves over time using discrete steps, so we don’t have to compute every single value along the way. This integration method also is advantageous by not allowing energy leaks out of the system (remember the Hamiltonian?) which is super important if you don’t want to get stuck in this statistical dimension mid exploration.\nHere’s how it works in HMC-lingo: we use leapfrog integration to move around the parameter space in discrete steps (rather than sliding through a continuum), and grabbing samples from the posterior distribution. The whole process goes like this:\n\nWe give the momentum \\(p\\) a little nudge by leveraging on the gradient info (\\(\\nabla\\)). The gradient or slope of the position (\\(\\nabla U(q)\\)) in this parameter space will determine by how much our momentum will change. Like when we are in the top position in the swing, the potential energy then transfers to kinetic energy (aka. momentum).\nWe adjust the position (or parameters) based on the momentum boost.\nThen we update the momentum based on the gradient on that new position.\nWe repeat the cycle for as many “jumps” we are doing, for each sample of the posterior we intend to draw.\n\nPicture a frog hopping from one lily pad to another, that’s where the name “leapfrog” comes from. It helps us explore new spots in the parameter space by discretizing the motion of this imaginary particle by using Hamiltonian dynamics, using the slope information (\\(\\nabla U(q)\\)) of the current position \\(q\\) to gain/loss momentum \\(p\\) and move to another position \\(q\\) in the parameter space.\n\n\n\nA frog jumping, with a fixed step size, from one point in the phase space into another.\n\n\nWe prefer leapfrogging over simpler methods like Euler’s method because it keeps errors low, both locally and globally. This stability is key, especially when we’re dealing with big, complicated systems. Plus, it’s a champ at handling high-dimensional spaces, where keeping energy in check is a must for the algorithm to converge.\n\n\nTuning Those Hamiltonian Gears\nNow, to get our HMC sampler purring like a kitten, we’ve got to fine-tune a few gears. Think of these as the knobs and dials on your favorite sound system – adjust them just right, and you’ll be grooving to the perfect beat.\nFirst up, we’ve got the number of samples. This determines how many times our sampler will take a peek at the parameter space before calling it a day.\nNext, we’ve got the step size (\\(\\epsilon\\)). Imagine this as the stride length for our leapfrog integrator. Too short, and we’ll be tiptoeing; too long, and we’ll be taking giant leaps – neither of which gets us where we want to go. It’s all about finding that sweet spot.\nThen, there’s the number of steps for the leapfrog to make. Too few, and we risk missing key spots; too many, and we might tire ourselves out.\nLastly, we need an initial guess for the intercept and slope. This is like dropping a pin on a map – it gives our sampler a starting point to begin its journey through the parameter space.\n\n# Initialization of the sampler\nnum_samples &lt;- 5000  # Number of samples\nepsilon &lt;- 0.05  # Leapfrog step size\nnum_steps &lt;- 50  # Number of leapfrog steps\ninit_intercept &lt;- 0  # Initial guess for intercept\ninit_slope &lt;- 0  # Initial guess for slope\n\n# Placeholder for storing samples\nparams_samples &lt;- matrix(NA, nrow = num_samples, ncol = 2)\n\n\n\nStarting the Sampler\nAlright, let’s fire up this Hamiltonian engine and get this party started. Here’s the game plan:\n\nCreate a Loop: We’ll set up a loop to simulate our little particle moving around the parameter space.\nIntegrate Its Motion: Using our trusty leapfrog integrator, we’ll keep track of how our particle moves.\nGrab a Sample: Once our particle has finished its dance, we’ll grab a sample at its final position.\nAccept or Reject: We’ll play a little game of accept or reject – if the new position looks promising, we’ll keep it; if not, we’ll stick with the old one. It’s like Tinder for parameters.\nRepeat: We’ll rinse and repeat this process until we’ve collected as many samples as we need.\n\nNow, to kick things off, we’ll give our imaginary particle a random speed and direction to start with, and drop it down somewhere in the parameter space. This initial kick sets the stage for our parameter exploration, the rest is up to the physics.\nfor (i in 1:num_samples) {\n  # Start with a random momentum of the particle\n  momentum_current &lt;- rnorm(2)\n  \n  # And set the initial position of the particle\n  params_proposed &lt;-c(init_intercept, init_slope) \n  \n  # Next, we will simulate the particle's motion using\n  # leapfrog integration.\n  ...\n\nHere, I’m using the concept of a particle moving through a probability space. But remember, in reality, we’re talking about the parameter space, where each coefficient in a model parameter or any other parameter that we can represent in this way then becomes a coordinate, therefore, a position \\(q\\) in this statistical space.\n\n\n\nSimulating the Particle’s Motion\nNow that our imaginary particle is all geared up with an initial momentum and position, it’s time to let it loose and see where it goes in this parameter playground. We know we’re using Hamiltonian mechanics, but to make it computationally feasible, we’re bringing in our trusty leapfrog integrator. We’ve already seen that this bad boy discretizes the motion of our particle, making it manageable to track its journey without breaking our computers.\n\n\n\nMarble rolling over a surface. In a perfect world, we’d love to follow our particle smoothly gliding through the parameter space, but hey, computing that would take ages. So instead, we chunk its movement into smaller steps, kind of like a flipbook, to keep tabs on its position.\n\n\nSo, here’s the lowdown on what our leapfrog integrator is up to:\n\nEstimating the Slope: We start off with an initial position and estimate the slope of the terrain at that point.\n\n\\[\n\\nabla U(q) = \\frac{\\partial U}{\\partial q}\n\\]\n\nHere, we calculate the gradient \\(\\nabla\\) with respect of the current position \\(q\\)\n\n\nAdjusting Momentum: This slope is like the potential energy, dictating whether our particle speeds up or slows down. So, we tweak the momentum accordingly based on this slope.\n\n\\[\np \\leftarrow p - \\frac{\\epsilon}{2} \\cdot \\nabla U(q)\n\\]\n\nThen, we update the momentum \\(p\\) with the gradient \\(\\nabla U(q)\\) by half step (\\(\\frac{\\epsilon}{2}\\))\n\n\nTaking a Step: With this momentum tweak, we move the particle for a set distance \\(\\epsilon\\), updating its position from the starting point.\n\n\\[\nq \\leftarrow q + \\epsilon \\cdot p\n\\]\n\nRepeat: Now that our particle has a new spot, we rinse and repeat – estimating the slope and adjusting momentum.\n\n\\[\np \\leftarrow p - \\frac{\\epsilon}{2} \\cdot \\nabla U(q)\n\\]\n\nHere, we update momentum \\(p\\) by the other half step size on the new position \\(q\\)\n\n\nKeep Going: We keep this cycle going for as many steps as we want to simulate, tracking our particle’s journey through the parameter space.\n\nfor (j in 1:num_steps) {\n    # Gradient estimation\n    grad &lt;- c(\n      # Gradient of the intercept of X: -sum(residuals / variance)\n      ...\n      # Gradient of the slope of X: -sum(residuals * X / variance)\n      ...\n    )\n    \n    # Momentum half update\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n    \n    # Full step update for parameters\n    params_proposed &lt;- params_proposed + epsilon * momentum_current\n    \n    # Recalculate gradient for another half step update for momentum\n    grad &lt;- c(\n      # Gradient of the intercept of X: -sum(residuals / variance)\n      ...\n      # Gradient of the slope of X: -sum(residuals * X / variance)\n      ...\n    )\n    \n    # Final half momentum update\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n  }\n\n\nSample Evaluation and Acceptance\nAfter our particle has taken its fair share of steps through the parameter space, it’s decision time – should we accept or reject its proposed new position? We can’t just blindly accept every move it makes; we’ve got to be smart about it.\nThat’s where the Metropolis acceptance criteria come into play. This handy rule determines whether a proposed new position is a good fit or not. The idea is to weigh the probability of the new position against the probability of the current one. If the new spot looks promising, we’ll move there with a certain probability, ensuring that our samples accurately reflect the shape of the distribution we’re exploring. But if it’s not a better fit, we’ll stick with where we are.\nThe formula for this acceptance probability (\\(A(q', q)\\)) when transitioning from the current position (\\(q\\)) to a proposed position (\\(q'\\)) is straightforward:\n\\[\nA(q',q) = min(1,\\frac{p(q')}{p(q)})\n\\]\nHere, \\(p(q')\\) is the probability density of the proposed position \\(q'\\), and \\(p(q)\\) is the probability density of the current position \\(q\\). We’re essentially comparing the fitness of the proposed spot against where we’re currently at. If the proposed position offers a higher probability density, we’re more likely to accept it. This ensures that our samples accurately represent the target distribution.\nHowever, when dealing with very small probability values, we might run into numerical underflow issues. That’s where using the log posterior probabilities comes in handy. By taking the logarithm of the probabilities, we convert the ratio into a difference, making it easier to manage. Here’s how the acceptance criteria look with logarithms:\n\\[\n\\begin{aligned}\n\\alpha &= \\log(p(q')) - \\log(p(q)) \\\\\nA(q',q) &= min(1,exp(\\alpha))\n\\end{aligned}\n\\]\nThis formulation is equivalent to the previous one but helps us avoid numerical headaches, especially when working with complex or high-dimensional data. We’re still comparing the fitness of the proposed position with our current spot, just in a more log-friendly way.\n# Calculate log posteriors and acceptance probability\nlog_posterior_current &lt;- log_posterior( ...current parameters... )\nlog_posterior_proposed &lt;- log_posterior( ...proposed parameters... )\n\nalpha &lt;- min(1, exp(log_posterior_proposed - log_posterior_current))\n\n# Accept or reject the proposal\nif (runif(1) &lt; alpha) {\n  init_intercept &lt;- params_proposed[1]\n  init_slope &lt;- params_proposed[2]\n}\n\n\nMixing All Toghether\nNow that we’ve broken down each piece of our HMC puzzle, it’s time to put them all together and see how the full algorithm works.\n\nfor (i in 1:num_samples) {\n  # Randomly initialize momentum\n  momentum_current &lt;- rnorm(2)\n  \n  # Make a copy of the current parameters\n  params_proposed &lt;- c(init_intercept, init_slope)\n  \n  # Perform leapfrog integration\n  for (j in 1:num_steps) {\n    # Half step update for momentum\n    grad &lt;- c(\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) / sigma^2),\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) * x / sigma^2)\n    )\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n    \n    # Full step update for parameters\n    params_proposed &lt;- params_proposed + epsilon * momentum_current\n    \n    # Recalculate gradient for another half step update for momentum\n    grad &lt;- c(\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) / sigma^2),\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) * x / sigma^2)\n    )\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n  }\n  \n  # Calculate the log posterior of the current and proposed parameters\n  log_posterior_current &lt;- log_posterior(init_intercept, init_slope, x, y, sigma)\n  log_posterior_proposed &lt;- log_posterior(params_proposed[1], params_proposed[2], x, y, sigma)\n  \n  # Calculate the acceptance probability\n  alpha &lt;- min(1, exp(log_posterior_proposed - log_posterior_current))\n  \n  # Accept or reject the proposal\n  if (runif(1) &lt; alpha) {\n    init_intercept &lt;- params_proposed[1]\n    init_slope &lt;- params_proposed[2]\n  }\n  \n  # Save the sample\n  params_samples[i, ] &lt;- c(init_intercept, init_slope)\n}\n\n\n\n\nVisualizing the Final Result\nAlright, folks, let’s wrap this up with a peek at how our little algorithm fared in estimating the true intercept and slope of our linear model.\n\nSamples of Intercept and Slope\n\n\nCode\ncolnames(params_samples) &lt;- c(\"Intercept\", \"Slope\")\nposterior &lt;- as.data.table(params_samples)\nposterior[, sample := seq_len(.N)]\nmelt_posterior &lt;- melt(posterior, id.vars = \"sample\")\n\nggplot(melt_posterior, aes(sample, value, col = variable)) +\n  facet_grid(rows = vars(variable), scales = \"free_y\") +\n  geom_line(show.legend = FALSE) +\n  geom_hline(data = data.frame(\n    hline = c(true_intercept, true_slope),\n    variable = c(\"Intercept\", \"Slope\")\n  ), aes(yintercept = hline), linetype = 2, linewidth = 1.5) +\n  labs(x = \"Samples\", y = NULL,\n       title = \"Convergence of parameter values\",\n       subtitle = \"Traceplot of both the Intercept and Slope\") +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  scale_y_continuous(n.breaks = 3) +\n  scale_x_continuous(expand = c(0,0)) +\n  theme_classic(base_size = 20) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nIn this plot, we’re tracking the evolution of the intercept and slope parameters over the course of our sampling process. Each line represents a different sample from the posterior distribution, showing how these parameters fluctuate over time. The dashed lines mark the true values of the intercept and slope that we used to generate the data. Ideally, we’d like to see the samples converging around these true values, indicating that our sampler is accurately capturing the underlying structure of the data.\n\n\nData with True and Estimated Regression Line\n\n\nCode\nids &lt;- posterior[,sample(sample, size = 200)]\n\nggplot() +\n  geom_abline(slope = posterior$Slope[ids], intercept = posterior$Intercept[ids], col = \"steelblue\", alpha = .5) +\n  geom_abline(slope = true_slope, intercept = true_intercept, col = \"white\", lwd = 1.5) +\n  geom_point(aes(x, y), size = 4) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  labs(title = \"Data with Regression Line\",\n       subtitle = \"True and HMC-estimated parameter values\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\n\nThis plot gives us a bird’s-eye view of our data, overlaid with both the true regression line (in white) and the estimated regression lines from our HMC sampler (in blue). The true regression line represents the ground truth relationship between our independent and dependent variables, while the estimated regression lines are sampled from the accepted values of the intercept and slope parameters sampled from the posterior distribution. By comparing these two, we can assess how well our model has captured the underlying trend in the data.\n\n\nPosterior Samples of Intercept and Slope\n\n\nCode\nggplot(posterior, aes(Slope, Intercept)) +\n  geom_density2d_filled(contour_var = \"density\", show.legend = FALSE) +\n  geom_hline(yintercept = true_intercept, linetype = 2, col = \"white\") +\n  geom_vline(xintercept = true_slope, linetype = 2, col = \"white\") +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  scale_fill_viridis_d(option = \"B\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\n\nIn this plot, we’re visualizing the joint posterior density of the intercept and slope parameters sampled from our model. The contours represent regions of higher density, with brighter zones indicating areas where more samples are concentrated. The white dashed lines mark the true values of the intercept and slope used to generate the data, providing a reference for comparison. Ideally, we’d like to see the contours align closely with these true values, indicating that our sampler has accurately captured the underlying distribution of the parameters.\n\n\n\nWrapping It Up\nSo, let’s take a moment to marvel at the marvels of Hamiltonian Monte Carlo (HMC). It’s not every day you see physics rubbing shoulders with statistics, but here we are, with HMC straddling both worlds like a boss.\n\nBridging Physics and Stats\nWhat makes HMC so darn fascinating is how it borrows tools from physics to tackle complex problems in statistics. It’s like watching two old pals team up to solve a mystery, each bringing their own unique skills to the table. With HMC, we’re not just crunching numbers; we’re tapping into the underlying principles that govern the physical world. I mean, seriously, it gives me goosebumps just thinking about it.\nBut it’s not just HMC that’s shaking up the stats scene. Nope, the whole world of science is pivoting towards simulation-based statistics and Bayesian methods faster than you can say “p-value.” Why? Because in today’s data-rich landscape, traditional methods just can’t keep up. We need tools like HMC to navigate the choppy waters of high-dimensional data, to tease out the subtle patterns hiding in the noise.\nNow, here’s the kicker: understanding how HMC works isn’t just some academic exercise. Oh no, it’s the key to unlocking the true power of Bayesian inference. Sure, you can run your models without ever peeking under the hood, but where’s the fun in that? Knowing how HMC works gives you this intuitive grasp of what your model is up to, what might be tripping it up, and how to steer it back on course when things go sideways.\nSo, here’s to HMC, one of the big algorithms of modern statistics, blurring the lines between physics and stats, and paving the way for a brave new world of simulation-based inference. Cheers to the leapfrogging pioneers of Bayesian stats, charting new territories and uncovering hidden truths, sliding one simulated step at a time.\n\n\n\n\n\nCitationBibTeX citation:@misc{castillo-aguilar2024,\n  author = {Castillo-Aguilar, Matías},\n  title = {The {Good,} {The} {Bad,} and {Hamiltonian} {Monte} {Carlo}},\n  date = {2024-05-15},\n  url = {https://bayesically-speaking.com/posts/2024-05-15 mcmc part 2/},\n  doi = {10.59350/fa26y-xa178},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2024. “The Good, The Bad, and\nHamiltonian Monte Carlo.” May 15, 2024. https://doi.org/10.59350/fa26y-xa178."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "",
    "text": "Let’s be honest, being a statistician or a scientist isn’t just about crunching numbers all day. It’s more like being a detective, a problem solver, and yeah, throwing in some math for good measure. When we get into non-linear models, things really start to get interesting. We’re not just drawing straight lines anymore; we’re wrestling with curves, untangling complicated relationships, and trying to figure out what’s really going on behind the scenes.\nTake pharmacokinetics, for example. Sounds fancy, right? But at the core, it’s just about what happens to a drug inside the body, and for us, that’s where the real statistical fun begins. Predicting how a drug’s concentration changes in the bloodstream over time isn’t just about plotting some points and calling it a day. It’s about understanding how the data dances with biology, and then figuring out the best way to describe that dance. And let’s not forget the little thrill of choosing your weapon: Frequentist or Bayesian? It’s like deciding between coffee or mate (and let’s be real, I’m down for both). Each one has its perks, but the choice depends on the situation, and maybe how you’re feeling that day.\nIn this post, we’re going to roll up our sleeves and dig into building a non-linear model to predict how something, let’s say, a drug disappears from the bloodstream. But we’re not stopping there. Nope, we’re also going to throw in a showdown between two big players: the frequentist and Bayesian methods. Think of it like a friendly face-off between two old rivals, each with its own style, strengths, and die-hard fans.\nBut here’s the thing: this isn’t just about which method wins on paper. It’s about the real-life, day-to-day grind of working with data. It’s about those moments when you’re staring at your screen, trying to make sense of a stubborn parameter that just won’t cooperate. It’s about knowing when to trust the numbers and when to rely on your gut, your experience, and maybe even a bit of luck.\nSo whether you’re a seasoned pro who’s been around the block or someone just dipping their toes into the world of applied stats, this post is for you. We’re going to dive in, compare, and yeah, maybe even have a little fun along the way. Because in the world of science and stats, the journey is half the adventure."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#designing-a-solution",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#designing-a-solution",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "Designing a solution",
    "text": "Designing a solution\nIt appears that the concentration of indomethacin decreases according to an exponential function of time. We can model this relationship with:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot \\exp(\\lambda \\cdot time_i)\n\\end{aligned}\n\\]\nIn this model, \\(\\lambda\\) represents the rate of decay in the plasma concentration of indomethacin (\\(y_i\\)), starting from a baseline concentration (\\(\\beta\\)). The term \\(\\alpha\\) estimates the minimum plasma concentration level over the observed time frame. We assume that the variance is constant across expected indomethacin plasma concentrations (\\(\\mu\\)) for each subject (\\(i\\)).\nHowever, given that we have repeated measurements from multiple individuals, we need to account for variability in the initial plasma concentrations across subjects. To address this, we introduce a random effect (\\(\\phi_i\\)) into the model:\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + (\\beta + \\phi_i) \\cdot \\exp(\\lambda \\cdot time_i)\n\\end{aligned}\n\\]\nHere, \\(\\phi_i\\) represents the deviation from the baseline plasma level (\\(\\beta\\)) for subject \\(i\\). We keep \\(\\lambda\\) and \\(\\alpha\\) fixed because we’re interested in estimating population parameters to describe the pharmacokinetics of the drug."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#nlme-approach",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#nlme-approach",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "nlme approach",
    "text": "nlme approach\nLet’s dive into fitting a non-linear model using the nlme R package, which allows us to specify custom model forms:\n\nnlm_freq &lt;- nlme::nlme(\n  ## Model previously described\n  model = conc ~ alpha + (beta + phi) * exp(lambda * time),\n  data = Indometh,\n  ## Fixed effects\n  fixed = alpha + beta + lambda ~ 1,\n  ## Random effects\n  random = phi ~ 1 | Subject, \n  ## Starting proposal values\n  start = list(fixed = c(0, 2, -1)))\n\n## Confidence intervals for fixed effects\nnlme::intervals(nlm_freq, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n             lower       est.      upper\nalpha   0.09050133  0.1311638  0.1718262\nbeta    2.36887258  2.8249635  3.2810543\nlambda -1.77509197 -1.6140483 -1.4530047\n\n\nLet’s kick things off by looking at where we stand with indomethacin’s plasma levels. At time zero, we’re seeing baseline concentrations hovering around 2.74 mcg/ml. Not too shabby, right? But this little molecule doesn’t stick around for long, our decay rate clocks in at about \\(\\exp(-1.61) \\approx 0.20\\), which translates to a swift 80% drop in those levels each hour. Eventually, the levels bottom out at around 0.13 mcg/ml, where the decline takes a bit of a breather.\nNow, if you were paying close attention to the code, you might have noticed something interesting: when we’re playing in the frequentist sandbox, particularly with non-linear models, we’ve got to give the algorithm a little nudge with some starting values. It’s like setting up the board for a game, these initial values are where the algorithm begins its quest through the likelihood landscape, hunting down the most likely parameters that explain our data. Remember that previous post about Hamiltonian Monte Carlo? Well, this is a bit like rolling a ball down a hill of parameter space, but here we’re aiming to land on the single spot that maximizes our chances of observing the data we have.\nBut enough with the theory, let’s dive back into our non-linear model and see how these predicted plasma levels measure up against the real-world data we’ve got in hand:\n\nfreq_pred &lt;- predict(nlm_freq)\n\nggplot(cbind(Indometh, pred = freq_pred), aes(time, conc)) +\n  facet_wrap(~ Subject) +\n  labs(y = \"Plasma levels (mcg/ml)\", x = \"Time (hours)\") +\n  geom_point(aes(col = Subject), cex = 3) +\n  geom_line(aes(y = pred, col = Subject), linewidth = 1)\n\n\n\n\n\n\n\n\nOur model does a decent job fitting the observed data. But what’s the story beyond standard errors? How do we quantify uncertainty in our model parameters? What’s the likelihood of observing a specific decay rate or baseline level? With frequentist methods, our insights are somewhat limited to point estimates and standard errors. We need a broader view."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#brms-approach",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#brms-approach",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "brms approach",
    "text": "brms approach\nTo harness the power of the Bayesian framework, we need to not only define our model but also incorporate prior beliefs about the parameters. Let’s revisit our parameters:\n\n\\(\\alpha\\): Minimum plasma levels of the drug.\n\\(\\beta\\): Baseline plasma levels at time zero.\n\\(\\phi\\): Subject-specific deviation from the population \\(\\beta\\).\n\\(\\lambda\\): The amount of exponential decay.\n\nWe’ll assign prior distributions based on prior knowledge and results from our frequentist model. Here’s the prior setup:\n\nFor \\(\\alpha\\), we’ll use a normal distribution centered around 0.1 mcg/ml with a standard deviation of 0.5 mcg/ml. We’ll truncate this prior at zero, since negative plasma levels aren’t physically meaningful.\nFor \\(\\beta\\), we’ll specify a normal distribution centered around 2.5 mcg/ml with a standard deviation of 3 mcg/ml to avoid overly restricting the parameter space. We’ll also set a lower bound of zero.\nFor \\(\\phi\\), we’ll use a normal prior centered on 0.5 mcg/ml with a moderate standard deviation of 2 mcg/ml to capture variability around baseline levels.\nFor \\(\\lambda\\), we’ll set a weakly informative prior centered around -1 with a standard deviation of 3. This reflects our expectation of a negative decay rate, with the upper bound fixed at zero to prevent increases in plasma levels.\n\nThe priors for our model parameters are:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\mathcal{N}(0.1, 0.5) \\\\\n\\beta &\\sim \\mathcal{N}(2.5, 3.0) \\\\\n\\phi &\\sim \\mathcal{N}(0.5, 2.0) \\\\\n\\lambda &\\sim \\mathcal{N}(-1.0, 3.0) \\\\\n\\end{aligned}\n\\]\nSo now that we have what we need we can already proceed to fit our bayesian non-linear model:\n\nnlme_brms &lt;- brm(\n  ## Formula\n  formula = bf(conc ~ alpha + (beta + phi) * exp(lambda * time),\n               alpha + beta + lambda ~ 1, phi ~ 1 | Subject,\n               nl = TRUE),\n  data = Indometh,\n  ## Priors\n  prior = prior(normal(0.1, 0.5), nlpar = \"alpha\", lb = 0) +\n    prior(normal(2.5, 3.0), nlpar = \"beta\", lb = 0) +\n    prior(normal(0.5, 2.0), nlpar = \"phi\") +\n    prior(normal(-1.0, 3.0), nlpar = \"lambda\", ub = 0),\n  ## MCMC hyperparameters\n  chains = 5, iter = 4000, \n  warmup = 2000, cores = 5,\n  ## More flexible exploration parameters\n  control = list(adapt_delta = 0.99, \n                 max_treedepth = 50),\n  ## For reproducibility\n  seed = 1234, file = \"nlme_brms.RDS\"\n)\n\nfixef(nlme_brms)\n\n                   Estimate  Est.Error        Q2.5      Q97.5\nalpha_Intercept   0.1346163 0.02173654  0.09162845  0.1768141\nbeta_Intercept    2.6519527 1.44843180  0.24992601  5.7345653\nlambda_Intercept -1.6448947 0.09467393 -1.83556844 -1.4659436\nphi_Intercept     0.2150963 1.44471628 -2.84981889  2.6512578\n\n\nIn this Bayesian model, we get not just point estimates but full distributions for each parameter. This approach allows us to explore the probable range of parameter values and answer probabilistic questions. But first, let’s see how well our Bayesian model fits the observed data:\n\nbmrs_pred &lt;- predict(nlme_brms)\n\nggplot(cbind(Indometh, bmrs_pred), aes(time, conc)) +\n  facet_wrap(~ Subject) +\n  labs(y = \"Plasma levels (mcg/ml)\", x = \"Time (hours)\") +\n  geom_point(aes(col = Subject), cex = 3) +\n  geom_line(aes(y = Estimate, col = Subject), linewidth = 1) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = Subject), alpha = .3)\n\n\n\n\n\n\n\n\nThe Bayesian model’s fitted effects align nicely with the observed data, and the uncertainty around the expected plasma concentrations is well-represented. To explore the range of parameters compatible with our data, we can plot the posterior distributions:\n\nplot(nlme_brms, variable = \"^b_\", regex = TRUE)\n\n\n\n\n\n\n\n\nThese plots reveal a spectrum of parameter values that fit the observed data. For the decay parameter \\(\\lambda\\), we can expect a 78% (\\(1 - \\exp(-1.5)\\)) to 83% (\\(1 - \\exp(-1.8)\\)) decrease in indomethacin plasma concentrations per hour.\nWe can further explore the posterior distributions. For example, transforming the \\(\\lambda\\) parameter into a percentage decay scale:\n\nposterior_dist &lt;- as_draws_df(nlme_brms, variable = \"b_lambda_Intercept\")\nposterior_dist$prob_decay &lt;- (1 - exp(posterior_dist$b_lambda_Intercept))\n\nggplot(posterior_dist, aes(x = prob_decay)) +\n  tidybayes::stat_halfeye(fill = \"lightblue\") +\n  labs(y = \"Density\", x = \"Decay of plasma levels per hour (%)\") +\n  scale_x_continuous(labels = scales::label_percent(), n.breaks = 8)\n\n\n\n\n\n\n\n\nThis flexibility in the Bayesian framework allows us to interpret the decay rate in more intuitive terms, reflecting a range of plausible rates consistent with our data. We can now communicate the percent decay of indomethacin plasma levels in a more accessible manner, considering the variability captured by our model."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#comparing-models",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#comparing-models",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "Comparing models",
    "text": "Comparing models\nNow that we’ve implemented linear, non-linear, and Bayesian non-linear models, it’s time to compare their performances. It’s important to remember that each model has its own set of performance metrics, which can make direct comparisons tricky. However, by calculating the root mean square error (RMSE), we can get a sense of the average error each model makes when predicting plasma levels. RMSE gives us a tangible measure of error on the same scale as our predictor, helping us gauge how well each model is performing:\n\ndata.frame(\n  lm = performance::performance_rmse(lm_freq),\n  nlme = performance::performance_rmse(nlm_freq),\n  brms = performance::performance_rmse(nlme_brms)\n)\n\n         lm      nlme      brms\n1 0.4417804 0.1080546 0.1077213\n\n\nHere, we can see that both the frequentist and Bayesian non-linear models outperformed the simple linear model by a significant margin. The lower RMSE values indicate a better overall fit. Interestingly, the Bayesian model edged out the frequentist model by a tiny margin, with a difference of just 0.000402 mcg/ml in RMSE (\\(\\text{RMSE}(M_{nlme}) - \\text{RMSE}(M_{brms})\\)). Given that the standard deviation of the plasma levels is 0.63 mcg/ml, this difference is practically negligible and unlikely to be meaningful in a real-world context."
  },
  {
    "objectID": "posts/2024-12-23 modeling-lymphocyte-dynamics/index.html",
    "href": "posts/2024-12-23 modeling-lymphocyte-dynamics/index.html",
    "title": "Modeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations",
    "section": "",
    "text": "Introduction\n\nThe Immunological Challenge\nLet’s face it, the immune system is a bit of a drama queen (or king, depending on the situation). It’s constantly patrolling our bodies, acting as a vigilant security force, looking for trouble (like invading pathogens, damaged cells, or rogue cancer cells). When it finds something suspicious, it initiates a complex and tightly regulated immune response (a carefully orchestrated), multi-cellular reaction of activation, proliferation, differentiation, and effector functions (like killing infected cells). It’s not just a party; it’s a meticulously planned military operation. But like any complex system, things can go wrong.\n The principal mechanisms of innate immunity and adaptive immunity. NK, natural killer. Figure from Robbins Basic Pathology, 10th Edition\nTake cancer immunotherapy, for example. These therapies, like immune checkpoint blockade, aim to harness and enhance the immune system’s natural ability to fight cancer. The basic idea behind checkpoint blockade is to block inhibitory signals (immune checkpoints) that tumors exploit to evade immune destruction (Liu et al. 2022). These checkpoints are crucial for preventing autoimmunity (the immune system attacking the body’s own tissues), but tumors hijack them to create an immunosuppressive microenvironment (Chang et al. 2015). By blocking these checkpoints, the “brakes” on anti-tumor immunity are released, allowing T cells (and other immune cells) to attack the cancer cells more effectively. Sounds simple in theory, right?\nWell, not quite. Predicting who will respond to these therapies and who won’t is a significant challenge in the field of immuno-oncology. Why? Because the interactions between tumor cells and immune cells (especially lymphocytes, which include T cells and other important players like NK cells) are incredibly complex and context-dependent. It’s not just about counting the number of T cells present in a tumor (although that’s important); it’s about understanding their dynamic behavior: how they traffic to the tumor, how they get activated within the tumor microenvironment, how they interact with tumor cells (e.g., recognizing tumor-associated antigens), how they interact with other immune cells (both promoting and suppressing the immune response), and how they eventually either effectively eliminate the tumor or become exhausted (losing their effector functions). It’s a dynamic and multifaceted system, not a static snapshot (Taefehshokr et al. 2022). Factors like tumor heterogeneity, the presence of immunosuppressive cells (like myeloid-derived suppressor cells or regulatory T cells), and the tumor’s mutational burden also play critical roles in determining the outcome of immunotherapy.\n Differentiation of naive T cells in response to stimuli present at the time of antigen recognition. APC, antigen-presenting cells; TH, Helper T cells. Figure from Robbins Basic Pathology, 10th Edition\n\n\nBecause Straight Lines are for Geometry, Not Biology\nTraditional methods of studying the immune system, like taking a single blood sample and counting cell types (a bit like taking a census of a bustling city at one specific time), are useful for getting a basic snapshot. They tell us who is present, but not what they are doing. Similar to taking a single still photograph of a bustling city, you might see how many buildings and people are there, but you’d miss all the action, the traffic jams, the hustle and bustle of people going about their day, the dynamic flow of life. You’d miss the whole story.\nBiological systems, especially complex ones like the immune system, rarely behave in a linear, predictable fashion. A small change in one factor (say, a slight increase in the concentration of a signaling molecule) can have a huge, disproportionate, and often unexpected effect on the overall response.  Photo from CDC in Unsplash\nThe first reason is feedback loops. These are like the thermostats of biological systems. A positive feedback loop amplifies a signal (like a snowball rolling downhill, getting bigger and bigger), while a negative feedback loop dampens a signal (like a thermostat turning off the heating when the desired temperature is reached). These feedback loops create complex, dynamic behaviors that can’t be captured by simple straight lines. It’s like trying to describe the stock market with a straight line, you’d miss all the booms and busts.\nAdditionally, there’s something called saturation effect. This occur when a system reaches its maximum capacity. Imagine a sponge soaking up water, it can only hold so much. Similarly, a biological response might increase with a stimulus up to a certain point, after which further increases in the stimulus have little or no additional effect. It’s like turning up the volume on your stereo, past a certain point, it just gets louder, not clearer.\nSimilarly, biological systems tends to function based on threshold responses. These are like all-or-nothing switches. A response only occurs after a certain level of stimulus is reached. In the immune system, this might mean that a certain concentration of antigen is required to trigger T cell activation. It’s not a gradual increase in activation; it’s a sudden jump (Joglekar and Li 2021). Think of a light switch: you can press it lightly, and nothing happens, but once you press it hard enough, the light turns on.\nTrying to model these non-linear phenomena with simple straight lines (like in basic linear regression) is like trying to describe a roller coaster ride with a ruler, you’d miss all the exciting ups and downs, the twists and turns. It just doesn’t work. We need models that can capture these non-linear relationships, models that can bend and curve to fit complex biological systems. That’s where the Hill function and logistic growth equations come into play (Li et al. 2023), they’re our tools for capturing these essential curves in our cellular stories.\n Photo from the National Institute of Allergy and Infectious Diseases in Unsplash\n\n\n\nSimulating Cellular Mayhem\n\nDigital Petri Dishes and Tiny Cellular Actors\nThis is where agent-based modeling (ABM) swoops in like a superhero. ABM lets us create a virtual world filled with individual “agents”, but in our case, these actors are lymphocytes. Each agent gets its own set of rules, like a tiny cellular script to follow, dictating how it behaves and reacts to its environment (Azarov et al. 2019).\nABMs offer a powerful tool for studying lymphocyte dynamics, allowing us to explore how factors like clonal expansion, contraction, and differentiation are influenced by antigen exposure and immune system regulation (An et al. 2021).\nThe fun part is that we can then sit back and watch the cellular drama unfold, observing how these agents interact with each other and their digital surroundings over time (just like a movie!). It’s something like building a digital petri dish, a virtual stage where we can run experiments that would be impossible (given monetary or ethical reasons), or just plain messy to perform in real life. imagine trying to track the movement of millions of individual cells in a living organism, not exactly a walk in the park\n\n\n\n\n\n\nAdvantages of Agent-Based Modeling and Inference\n\n\n\n\n\nThe real beauty of ABM, in comparison to traditional modeling approaches, lies in its ability to:\nAccount for heterogeneity. Not all lymphocytes are exact copies of each other. They come in different flavors, with different functions, activation states, and responses to stimuli. By simulating virtual cells, we can represent this cellular diversity, giving each cell its own unique personality (or at least its own set of parameters).\nSimulate stochasticity. Biology is inherently noisy. Random events, chance encounters, and unpredictable fluctuations can play a significant role in immune responses. ABM allows us to incorporate this randomness, making our simulations much more realistic.\nModel inter-individual interactions. We can define rules (as complicate as we want them to be) that determines how lymphocytes interact with each other, with other cell types (like antigen-presenting cells or tumor cells), and with signaling molecules (like cytokines, the cellular equivalent of social media posts). We can simulate complex communication networks, feedback loops, and other intricate interactions that drive immune responses.\n\n\n\nAnd what’s absolutely essential for making these interactions realistic to the one of biological systems? You guessed it: non-linear equations. They allow us to define those complex biological processes with far greater accuracy than simple linear relationships ever could. It’s like using a finely tuned paintbrush to capture the subtle shades of a sunset, rather than just using a single, broad stroke of color.\n Photo from ThisisEngineering in Unsplash\n\n\nR as a Tool for Immunological Simulation\nR, the beloved language of statisticians, data scientists, and now immunologists (at least for this blog post!), is the perfect tool for building these ABM simulations. It’s powerful, flexible, and comes with a treasure of packages specifically designed for statistical computing, simulation, and data visualization. Plus, it’s open-source, which means it’s free (always a welcome bonus), especially if you’re on a student budget or just like free stuff. With powerful visualization packages like ggplot2, we can create stunningly beautiful and informative visualizations of our simulation results, transforming raw data into compelling stories that are easy to understand. It’s like turning a jumbled mess of numbers into a captivating infographic.\nIn this post, we’ll swim in oceans of lymphocyte dynamics by building a simplified ABM simulation in R. We’ll harness the power of non-linear equations to model key processes like lymphocyte activation, proliferation, and apoptosis. We’ll start with simple, easy-to-digest examples and gradually add more complexity, showing you step-by-step how to implement these models in R and, crucially, how to interpret the results. By the end of this post, you’ll have a solid understanding of how ABM, combined with the magic of non-linear equations, can provide valuable insights into the inner workings of the immune system. Think of it as a crash course in digital immunology, minus the lab coats, the pipettes, and the actual petri dishes (but with plenty of code, insightful explanations, and, of course, some cool plots!).\n\n\n\nFrom Code to Cellular Chaos\n Photo from the National Cancer Institute in Unsplash\nNow that we’ve established why we need to go beyond simple linear models to understand the immune system’s intricate dance, let’s get our hands dirty with some code and build our own lymphocyte simulation in R. Don’t worry if you’re not an immunologist; we’ll keep the biology simple and focus on the modeling aspects.\n\nSetting the Stage for Cellular Action\nOur model will focus on a simplified scenario involving T lymphocytes (a type of white blood cell crucial for adaptive immunity). We’ll consider two main types of T cells:\n\nNaive T cells: These are the rookies of the immune system, waiting for their marching orders. They haven’t encountered their specific target (an antigen) yet.\nActivated T cells: These are the seasoned warriors, ready to fight after being activated by encountering their specific antigen.\n\nWe’ll also include a representation of the antigen, which is any substance that can trigger an immune response (like a piece of a virus or a bacterial protein). We won’t explicitly model other cell types (like antigen-presenting cells) in this simplified version, but we’ll account for their role in the activation process.\n\n\nThe Rules of Engagement: From Intuition to Equations\nHere’s where the magic of non-linearity comes in. We’ll use a few key equations to dictate the behavior of our T cell agents.\nThe activation of a naive T cell upon encountering an antigen isn’t a simple binary event; it’s not just “on” or “off,” like a light switch. Instead, it’s a graded response, much like a dimmer switch. The more antigen a T cell encounters, the higher the chance it gets activated (Joglekar and Li 2021). But this increase in activation probability isn’t perfectly linear. There’s often a threshold effect: at low antigen concentrations, not much happens, but above a certain concentration, the activation probability shoots up dramatically. This “switch-like” behavior is what equations used in biochemistry like the Hill function elegantly captures.\n Checkpoints that regulate T cell response from their naive state to their final differentiated state. This shows how distinct mechanisms, including quiescence/ignorance, anergy, exhaustion/senescence, and various forms of cell death, act at T cell development and activation. Figure from Nature Reviews Immunology 21, 257–267 (2021)\n\n\nActivation Function: The “On” Switch\nLet’s build up the intuition behind this function. Imagine a T cell receptor (TCR) on the surface of a T cell. For simplicity, let’s consider that the interaction of the TCR with the antigen is a complex process that can involve multiple steps and interactions (Dushek et al. 2011). While we might imagine that \\(n\\) antigen molecules are required to bind for full activation, the Hill equation doesn’t directly represent this exact number of binding sites. Instead, it describes the overall cooperativity of the binding process (Stefan and Le Novère 2013).\nNow, let’s think about the probability of activation as a function of antigen concentration. If the antigen concentration is low, the probability of any productive interaction between the antigen and TCR is low, and therefore, the probability of activation is also low. As the antigen concentration increases, the probability of productive interactions also increases. This increase is often non-linear, with a steeper rise at certain concentrations (Dushek et al. 2011). This non-linearity is captured by the concept of cooperativity: the binding of one antigen molecule can influence the likelihood of subsequent binding events or downstream signaling events, leading to a more pronounced response.\nThis intuition leads us to the Hill function:\n\\[\n\\Pr(\\text{Activation}) = \\frac{\\text{[Antigen]}^n}{K^n + \\text{[Antigen]}^n}\n\\]\nLet’s break down this equation piece by piece, connecting it to our intuitive understanding:\n\\(\\Pr(\\text{Activation})\\) is the probability that a naive T cell becomes activated. It ranges from 0 (no activation) to 1 (certain activation).\n\\(\\text{[Antigen]}\\) represents the concentration of the antigen. The more antigen present, the higher the chance of binding and activation.\n\\(n\\) is the Hill coefficient. It is a phenomenological parameter that describes the steepness or cooperativity of the response. A higher \\(n\\) indicates a steeper curve and a more switch-like response. It reflects how much the binding of one antigen influences the binding of subsequent antigens or the downstream signaling events.\n\\(K\\) is the dissociation constant. It represents the antigen concentration at which the probability of activation is 50%. It’s a measure of the affinity between the antigen and the TCR. A smaller \\(K\\) means a higher affinity (stronger binding), meaning that less antigen is needed to achieve 50% activation. You can think of \\(K\\) as the “difficulty” of binding, a smaller \\(K\\) means it’s easier to bind.\n\nHow Does this Equation Capture the Switch-Like Behavior?\nOkay but, how does this equation capture the switch-like behavior? Let’s see how:\n\nAt very low antigen concentrations (\\(\\text{[Antigen]} &lt;&lt; K\\)), the term \\(\\text{[Antigen]}^n\\) is much smaller than \\(K^n\\), so the probability of activation is close to zero.\nAt very high antigen concentrations (\\(\\text{[Antigen]} &gt;&gt; K\\)), the term \\(\\text{[Antigen]}^n\\) dominates, and the probability of activation approaches 1.\n\nThe transition between these two extremes is sharper when \\(n\\) is larger. This steep transition is what we refer to as the “switch-like” behavior.\nIt’s crucial to understand that \\(n\\) is an effective or apparent cooperativity coefficient. It describes the overall cooperativity of the binding process, which can be influenced by multiple factors beyond just the number of binding sites. It can reflect complex downstream signaling events or allosteric effects.\nHere’s the R function:\n\nactivation_probability &lt;- function(antigen, K, n) {\n  (antigen^n) / (K^n + antigen^n)\n}\n\nThis R function is a direct translation of the Hill equation. It takes the antigen concentration (antigen), the dissociation constant (K), and the Hill coefficient (n) as inputs and returns the calculated activation probability. It’s our little digital “activation calculator.”\nAnd a quick visualization to show the effect of the Hill coefficient:\n\n\nCode\nantigen_conc &lt;- seq(0, 2, by = 0.001)\n\ndf &lt;- lapply(c(\"0.2\" = 0.2, \"0.5\" = 0.5, \n         \"1.0\" = 1.0, \"2.0\" = 2.0, \n         \"4.0\" = 4.0), function(x) {\n           data.table(\n             Pr = activation_probability(antigen_conc, K = 0.7, n = x),\n             Antigen = antigen_conc\n           )\n         }) |&gt; rbindlist(idcol = \"Hill coefficient\")\n\nggplot(df, aes(Antigen, Pr)) +\n  geom_line(aes(color = `Hill coefficient`), linewidth = 1) +\n  labs(x = \"Antigen Concentration\", y = \"Probability of Activation\", color = \"Hill Coefficient\", title = \"Effect of Hill Coefficient on Activation Probability\") +\n  scale_color_ordinal()\n\n\n\n\n\nHill coefficient (n) on T cell activation. When n = 1, the response is nearly linear. However, as n increases (n = 2, n = 4), the activation curve becomes increasingly steep, demonstrating enhanced cooperativity and a more pronounced threshold effect. This highlights how small changes in antigen concentration can trigger large changes in activation probability when cooperativity is high. It’s like comparing a gentle slope to a cliff, a small step can make a big difference when you’re near the edge of the cliff (high n).\n\n\n\n\n\n\n\nProliferation: The Population Explosion\nOnce a T cell is activated, it’s time for action (and lots of it). This action comes in the form of proliferation, where a single activated T cell divides and creates many identical copies of itself, building an army to fight the infection (Kumar, Abbas, and Aster 2012). If there were no limits, this growth would be exponential (like compound interest in a bank account), but with cells instead of money. Imagine starting with one cell, and it doubles every hour. After just a day, you’d have millions!\nBut, of course, in the real world (and even in our digital one), such unrestrained growth is impossible. There are limited resources: space, nutrients, signaling molecules, all the things a T cell needs to survive and multiply. Eventually, the growth must slow down and plateau. This is where the logistic growth equation comes in, providing a much more realistic model of population dynamics.\nLet’s start with the continuous form of the logistic growth equation:\n\\[\n\\frac{dN}{dt} = rN \\left(1- \\frac{N}{K}\\right)\n\\] This equation has previously been used to model T cell dynamics in both mouse and human models (Morris, Farber, and Yates 2019). Additionally, it’s commonly used to model cell growth in general in discrete-time approximations (Jin, McCue, and Simpson 2018).\nLet’s break down this equation and connect it to our intuitive understanding of population growth:\n\n\\(\\frac{\\partial N}{\\partial t}\\): This term represents the rate of change in the number of T cells (\\(N\\)) over time (\\(t\\)). It’s how quickly the population is growing or shrinking. Think of it as the “speedometer” of our population.\n\\(r\\): This is the intrinsic growth rate, sometimes called the per capita growth rate. It represents how quickly the population would grow if there were no limitations. It’s the “ideal” growth rate, like the speed limit on a highway.\n\\(N\\): This represents the current number of T cells. It’s the current “position” of our population.\n\\(K\\): This is the carrying capacity. It represents the maximum number of T cells that the environment can sustain. It’s the “maximum occupancy” of our battlefield, there’s only so much room for T cells to operate.\n\nNow, let’s see how this equation captures the idea of limited growth.\nAt low population densities (N &lt;&lt; K), when the number of T cells (\\(N\\)) is much smaller than the carrying capacity (\\(K\\)), the term \\((1 - \\frac{N}{K})\\) is close to 1. The equation then simplifies to \\(\\frac{\\partial N}{\\partial t} = rN\\), which is the equation for exponential growth. This makes sense: when there are plenty of resources and space, the population grows rapidly, like those initial rabbits in a wide open field.\nAt high population densities (N ≈ K), as the number of T cells (\\(N\\)) approaches the carrying capacity (\\(K\\)), the term \\((1 - \\frac{N}{K})\\) approaches 0. This causes the rate of change \\(\\frac{\\partial N}{\\partial t}\\) to also approach 0, meaning the population growth slows down and eventually stops. This also makes sense, given that as the battlefield becomes crowded, resources become scarce, and the T cell population can’t grow much more.\nSince our simulation is discrete-time (we update the population at specific intervals, not continuously), we use a discrete-time approximation of the logistic growth equation:\n\\[\nN(t+1) = N(t) + r * N(t) * \\left(1 - \\frac{N(t)}{K} \\right)\n\\]\nThis equation tells us how to calculate the number of T cells at the next time step (\\(N(t+1)\\)) based on the current number of T cells (\\(N(t)\\)), the growth rate (\\(r\\)), and the carrying capacity (\\(K\\)). It’s like taking snapshots of the population at regular intervals and updating the numbers based on the growth equation.\nHere’s the R function that implements this discrete-time approximation:\n\nproliferation &lt;- function(N, r, K) {\n  N + r * N * (1 - N / K)\n}\n\nThis R function takes N, r, and K as inputs and returns the updated number of T cells. It’s our digital “population updater,” simulating the growth and eventual stabilization of the T cell army.\nAnd here’s a visualization of the proliferation behavior of our function implementation:\n\n\nCode\n## Function to simulate cell proliferation\nproliferating_cells &lt;- function(n_steps, n_cells, r, k) {\n  cells &lt;- numeric(n_steps)\n  cells[1L] &lt;- n_cells\n  for (t in 2:n_steps) {\n    cells[t] &lt;- proliferation(N = cells[t-1], r = r, K = k)\n  }\n  data.table(Time = seq_len(n_steps), Cells = cells)\n}\n\n## Testing different Rate values\ndf &lt;- lapply(X = seq(0.1, 0.6, by = 0.1), function(x) {\n  proliferating_cells(100, 1, x, 200)\n}) |&gt; rbindlist(idcol = \"Rate\")\n\n## Pretty labels for plotting\ndf[, Rate := ordered(Rate, levels = 1:6, labels = seq(0.1, 0.6, by = 0.1))]\n\n## ggplot2 plot\nggplot(df, aes(Time, Cells, color = Rate)) +\n  geom_hline(yintercept = c(0, 200), color = \"gray50\", \n             linewidth = 1/2, linetype = 2) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Time Steps\",\n       y = \"Number of Cells\", \n       title = \"Simulation of Cell Proliferation over Time\")\n\n\n\n\n\nDynamics of T cell proliferation using the logistic growth model. Simulations with varying growth rates (r) show that faster proliferation leads to a more rapid initial increase in cell numbers. However, the carrying capacity (K = 200) acts as a limiting factor, causing all populations to eventually reach a plateau, highlighting the constraint of resources on cell growth. It’s like a race between different rabbit populations, the faster breeders get to the maximum population size quicker, but they all eventually hit the limit of their field.\n\n\n\n\n\n\nApoptosis (Constant Probability): The Programmed Exit\nEven the most valiant soldiers can’t fight forever. T cells, like all living cells, have a finite lifespan. They eventually undergo apoptosis, a process of programmed cell death. It’s like a built-in self-destruct mechanism that ensures cells don’t become a burden or pose a risk to the organism. In our model, we’ll represent apoptosis with a constant probability. This means that at each time step, every activated T cell has the same chance of undergoing apoptosis, regardless of how long it’s been activated or what it’s been doing.\nLet’s represent this with a simple equation:\n\\[\n\\Pr(\\text{apoptosis}) = p_{\\text{apoptosis}}\n\\]\nThis equation is pretty straightforward:\n\n\\(\\Pr(\\text{apoptosis})\\): This represents the probability that a single activated T cell will undergo apoptosis during a given time step. It’s a number between 0 (no chance of apoptosis) and 1 (certain apoptosis).\n\\(p_{\\text{apoptosis}}\\): This is the constant probability of apoptosis. It’s a parameter of our model that we can set to represent different rates of cell death.\n\nThis constant probability model assumes that apoptosis is a random event that occurs independently for each cell (which is a simplication of real-life biological systems). It’s like each cell flipping a weighted coin at every time step. If the coin lands on “heads” (with a probability of \\(p_{\\text{apoptosis}}\\)), the cell undergoes apoptosis.\nNow, let’s translate this into R code:\n\napoptosis &lt;- function(N, p_apoptosis) {\n  n_apoptosis &lt;- rbinom(1, N, p_apoptosis) # Number of cells undergoing apoptosis\n  N - n_apoptosis\n}\n\nThis function simulates apoptosis. It takes two arguments: N (the current number of T cells) and p_apoptosis (the probability of apoptosis). It uses rbinom(1, N, p_apoptosis) to generate a random number of cells that undergo apoptosis. rbinom() simulates a binomial distribution, which is perfect for modeling the number of “successes” (apoptosis events) in a given number of “trials” (cells). It’s like rolling a bunch of dice, where each die has a probability p_apoptosis of landing on “apoptosis”. The function then returns the remaining number of cells after apoptosis.\nAnd here’s a visualization simulating the effect of different p_apoptosis values on the total number of cells over time:\n\n\nCode\n## Function to simulate cell dead\napoptotic_cells &lt;- function(n_steps, n_cells, p) {\n  cells &lt;- numeric(n_steps)\n  cells[1L] &lt;- n_cells\n  for (t in 2:n_steps) {\n    cells[t] &lt;- apoptosis(N = cells[t-1], p_apoptosis = p)\n  }\n  data.table(Time = seq_len(n_steps), Cells = cells)\n}\n\n## Testing different Rate values\ndf &lt;- lapply(X = seq(0.1, 0.5, by = 0.1), function(x) {\n  apoptotic_cells(50, 1000, x)\n}) |&gt; rbindlist(idcol = \"Probability\")\n\n## Pretty labels for plotting\ndf[, Probability := ordered(Probability, levels = 1:5, labels = seq(0.1, 0.5, by = 0.1))]\n\n## ggplot2 plot\nggplot(df, aes(Time, Cells, color = Probability)) +\n  geom_hline(yintercept = c(0), color = \"gray50\", \n             linewidth = 1/2, linetype = 2) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Time Steps\",\n       y = \"Number of Cells\", \n       title = \"Simulation of Cell Apoptosis over Time\",\n       color = expression(P[Apoptosis]))\n\n\n\n\n\nEffect of apoptosis probability on cell population size over time. Simulations show that increasing the probability of apoptosis (\\(p_{    ext{apoptosis}}\\)) results in a more rapid decrease in the number of cells, illustrating the direct relationship between apoptosis rate and population decline. It’s like watching a sandcastle being washed away by the tide, the stronger the tide (higher p_apoptosis), the faster the sandcastle disappears.\n\n\n\n\n\n\n\nSetting Up the Simulation\nBefore we launch our digital immune response, we need to set the stage. This means defining the initial conditions and parameters for our simulation. Think of it as setting up a virtual battlefield where our T cells will fight their digital battles.\n\n## T Cell population\nn_steps &lt;- 100         # Number of time steps\nn_naive &lt;- 100         # Initial number of naive T cells\n\n## T Cell activation function\nantigen_level &lt;- 0.2   # Antigen concentration\nK_activation &lt;- 0.5    # Dissociation constant for activation\nn_hill &lt;- 3            # Hill coefficient for activation\n\nHere’s what we’re setting up:\n\nn_steps: This is the number of time steps our simulation will run for. Think of it as the duration of the battle. We’ve chosen 100 time steps, but you can adjust this to simulate longer or shorter periods.\nn_naive: This is the initial number of naive T cells we start with. These are our fresh recruits, eager to join the fight. We’re starting with 100 naive T cells.\nantigen_level: This is the concentration of the antigen, the enemy our T cells are fighting against. It’s like the strength of the enemy army. We’ve set it to 0.2, but you can change this to simulate different infection scenarios.\nK_activation: This is the dissociation constant for activation, a parameter of the Hill function. Remember, this determines how strongly the antigen binds to the T cell receptor.\nn_hill: This is the Hill coefficient, which controls the steepness of the activation curve. A higher n_hill means a more switch-like response.\n\nThese parameters are like the initial settings of our simulation, determining the starting conditions of our virtual immune response.\n\nSimulating Activation and Early Response: Lighting the Fire\nNow that we have our equations and R functions ready, let’s simulate the initial activation of naive T cells in response to antigen. This section will focus on the early stages of the immune response, where naive T cells are first encountering the antigen and transitioning into their activated state. It’s like the first spark that ignites the immune fire, turning our rookie T cells into seasoned warriors.\nTo simulate the activation and early response of T cells, we will implement the following function:\n\n## Simulation function\nsim_t_activation &lt;- function(n_steps, n_naive, antigen_level, K_activation, n_hill) {\n  # Initialize data structures\n  naive_t_cells &lt;- numeric(n_steps)\n  activated_t_cells &lt;- numeric(n_steps)\n  \n  # Initial number of naive T cells\n  naive_t_cells[1L] &lt;- n_naive\n  \n  ## Probability of activation\n  prob &lt;- activation_probability(antigen_level, K_activation, n_hill)\n  \n  # Loop of activation cycle\n  for (t in 2:n_steps) {\n    # Probabilistic Activation of T cells\n    n_activated &lt;- 0\n    n_activated &lt;- sum(runif(n = naive_t_cells[t-1]) &lt; prob)\n    \n    ## Update population of T cells\n    naive_t_cells[t] &lt;- naive_t_cells[t-1] - n_activated\n    activated_t_cells[t] &lt;- activated_t_cells[t-1] + n_activated\n  }\n  \n  # Preparing the data from the simulation\n  df &lt;- data.table(Time = seq_len(n_steps), \n                   Naive = naive_t_cells, \n                   Activated = activated_t_cells)\n  \n  # In long format\n  melt(df, id.vars = \"Time\")\n}\n\nLet’s break down what’s happening inside this function:\n\nInitializing Data Structures: naive_t_cells &lt;- numeric(n_steps) and activated_t_cells &lt;- numeric(n_steps) create empty vectors to store the number of naive and activated T cells at each time step. It’s like setting up empty scoreboards to track the progress of the battle.\nSetting Initial Conditions: naive_t_cells[1L] &lt;- n_naive sets the initial number of naive T cells at the beginning of the simulation (time step 1). This is where we deploy our initial batch of rookie T cells onto the battlefield.\nCalculating Activation Probability: prob &lt;- activation_probability(antigen_level, K_activation, n_hill) calculates the probability of activation using our previously defined Hill function. This is like assessing the likelihood of our T cells encountering and recognizing the enemy.\nThe Activation Loop (The Battle Begins!): The for (t in 2:n_steps) loop simulates the passage of time, with each iteration representing a time step. Inside this loop, we have:\n\nProbabilistic Activation: n_activated &lt;- sum(runif(n = naive_t_cells[t-1]) &lt; prob) is where the magic of stochastic activation happens. For each naive T cell at the previous time step, we generate a random number between 0 and 1 using runif(). If this random number is less than the calculated activation probability (prob), the T cell is considered activated. It’s like each T cell flipping a weighted coin – the higher the probability, the more likely they are to “win” and become activated. The sum() function counts how many T cells were activated in this time step.\nUpdating Cell Populations: naive_t_cells[t] &lt;- naive_t_cells[t-1] - n_activated and activated_t_cells[t] &lt;- activated_t_cells[t-1] + n_activated update the number of naive and activated T cells. The number of naive T cells decreases as they become activated, and the number of activated T cells increases accordingly. It’s like tracking the casualties and new recruits on both sides of the battle.\n\nPreparing Data for Plotting: The code then prepares the simulation results into a data frame suitable for plotting using data.table and melt.\n\nNow, let’s visualize the results:\n\n\nCode\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill)\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Early T Cell Activation Response\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nEarly stages of T cell activation in response to antigen. The simulation demonstrates a decline in the naive T cell population coupled with a corresponding rise in the activated T cell population, reflecting the transition from naive to activated state. It’s like turning up the heat on the battlefield, the stronger the enemy (higher antigen level), the quicker our T cells get mobilized.\n\n\n\n\n\n\nSimulating Proliferation and Population Dynamics: The Army Grows\nNow that we have some activated T cells itching for action, let’s simulate their proliferation, the process where they multiply like, well, like very enthusiastic cells. We’ll extend our simulation and incorporate the logistic growth equation to keep things realistic (no infinite armies here!).\nFirst, let’s define some parameters that control the proliferation dynamics:\n\n## T Cell Proliferation and Apoptosis\nr_proliferation &lt;- 0.1 # Proliferation rate\nK_carrying &lt;- 150      # Carrying capacity\n\n\nr_proliferation: This is the proliferation rate, which determines how quickly the activated T cell population grows. It’s like the “birth rate” of our T cell army. A higher r_proliferation means faster growth.\nK_carrying: This is the carrying capacity, which represents the maximum number of activated T cells that the environment can support. It’s like the size of the battlefield, there’s only so much space and resources available.\n\nLet’s visualize the results:\n\n\nCode\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill, r_proliferation, K_carrying)\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"T Cell Activation and Proliferation\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nCombined processes of T cell activation and proliferation. Following initial activation driven by antigen (as seen by the decrease in naive T cells and increase in activated T cells), the activated population undergoes logistic growth. This is characterized by an initial phase of rapid, near-exponential expansion, which then slows down as the population approaches the carrying capacity (K = 150), demonstrating resource limitation. It’s like a population of rabbits in a field, they multiply quickly at first, but eventually, they run out of space and resources, and the population stabilizes.\n\n\n\n\n\n\nCombining Processes: A Full Simulation\nNow, the moment we’ve all been waiting for! Let’s combine all the processes we’ve discussed (activation, proliferation, and apoptosis) into a single, more complete simulation. This will give us a more realistic picture of how T cell populations behave during an immune response.\nWe’ll also make a couple of tweaks: we’ll set the apoptosis probability to 10% at each time step and increase our initial number of naive T cells to 300 and a carrying capacity of 500, because a bigger army is always more fun to simulate.\nAnd the final visualization:\n\n\nCode\n## Parameters\np_apoptosis = 0.1 # Probability of apoptosis\nn_naive = 300 # Number of initial naive T cells\nK_carrying = 500 # Carrying capacity\nr_proliferation = 0.5 # Proliferation rate\n\n# Simulation loop with activation, proliferation, and apoptosis\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill, r_proliferation, K_carrying, p_apoptosis)\n\n# Plot the full simulation\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Full T Cell Dynamics: Activation, Proliferation, and Apoptosis\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nIntegration of all key T cell processes: activation, proliferation, and apoptosis. Following antigen-driven activation, activated T cells proliferate, reaching a peak population size. Subsequently, apoptosis leads to a decline in the activated T cell population, demonstrating the dynamic balance between expansion and contraction of the immune response. It’s like watching a battle unfold, there’s an initial surge of troops, but eventually, attrition takes its toll, and the battle either reaches a stalemate or one side prevails.\n\n\n\n\n\n\n\nWhat Happens if We Tweak the Knobs?\nA crucial step in any modeling study is sensitivity analysis. It’s like playing with the knobs and dials of your model to see how it responds. We want to understand how changes in our input parameters (like the Hill coefficient or the carrying capacity) affect the outputs of our simulation (like the number of activated T cells). This helps us identify which parameters have the biggest influence on the model’s behavior, which knobs are the most important to turn if we want to change the outcome.\nFor instance, we can show a quick example of sensitivity analysis by varying the Hill coefficient (\\(n\\)), which, as we know, controls how switch-like the activation is:\n\n\nCode\n# Sensitivity analysis for Hill coefficient\nn_hill &lt;- c(\"n = 1\" = 1, \"n = 2\" = 2, \"n = 3\" = 3, \"n = 4\" = 4)\ndf &lt;- lapply(n_hill, function(x) {\n  sim_t_activation(n_steps = 100, n_naive = 300, \n                   antigen_level = 10, K_activation = 25, n_hill = x, \n                   r_proliferation = 0.5, K_carrying = 500, p_apoptosis = 0.10)\n}) |&gt; rbindlist(idcol = \"n_hill\")\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n    geom_line(aes(alpha = ordered(n_hill)), linewidth = 1) +\n    labs(title = \"Sensitivity Analysis for Hill Coefficient\", \n         x = \"Time Steps\", \n         y = \"Number of Activated Cells\", \n         color = \"Carrying Capacity\", \n         alpha = \"Hill Coefficient\",\n         caption = \"With [Antigen] = 10, and K = 25\") +\n  theme(plot.caption = element_text(size = 12))\n\n\n\n\n\nImpact of the Hill coefficient on the dynamics of T cell activation and recruitment. By varying n from 1 to 4, we observe a transition from a gradual, almost linear response (n=1) to a highly switch-like response (n=4).\n\n\n\n\nHere’s a closer look at what’s going on behind the scenes of this code.\nThe resulting plot is where we see the fruits of our labor. It shows how the dynamics of the activated T cell population and the recruitment of naive T cells change depending on the Hill coefficient. Tipically, higher values of \\(n\\) lead to a more rapid and pronounced increase in activated T cells. However, this is only true when \\(\\text{[Antigen]}\\) are above the \\(K\\) threshold.\nIn this case this relationship is inverted (recall the plot of the hill function we saw earlier), which might seem counterintuitive at first, but remember that the Hill coefficient affects the steepness of the activation curve. A higher Hill coefficient is like a very sharp switch, a small increase in antigen concentration leads to a big jump in activation after the \\(K\\) threshold is surpassed. Conversely, a lower Hill coefficient is like a more gradual dimmer switch.\nThis sensitivity analysis is incredibly valuable because it helps us understand just how important are the parameters in the equation controlling how T cells get activated. It shows that this parameter can significantly impact the overall immune response. By playing with these parameters, we can get a much better feel for how the immune system works and how it might react to different challenges, it’s like fine-tuning an instrument to get the perfect sound.\n\n\nSo, What Did Our Digital Cells Actually Do?\nNow that we’ve unleashed our digital immune system and watched our simulated lymphocytes playing out in the digital petri dish, let’s take a step back and figure out what it all means. Did we just waste a bunch of CPU cycles, or did we actually learn something?\n\nBiological Interpretation and Relevance: Decoding the Digital Chatter\nOur simplified model, while admittedly a bit of a caricature of the real thing, manages to capture some fundamental aspects of how lymphocytes behave.\nThe Hill function for activation, that fancy equation with the exponents, perfectly illustrates the importance of cooperative binding and threshold responses in kicking off an immune response (Dushek et al. 2011; Stefan and Le Novère 2013). It’s like a party, one person arriving might not start a dance floor, but once a few more join in, suddenly everyone’s grooving. That’s cooperativity!\nThe logistic growth equation, our population-limiting equation, shows how limited resources (like space, food, and Wi-Fi in the body) put a limit on lymphocyte proliferation, preventing them from turning into an unstoppable, resource-devouring horde (Morris, Farber, and Yates 2019; Jin, McCue, and Simpson 2018). The addition of stochasticity (randomness), that sprinkle of randomness we added, is a reminder that biological systems are inherently noisy. Even with identical starting conditions, the outcome of a real immune response can vary, it’s like two identical twins might react differently to the same flu virus.\nFor example, let’s consider what happens when we change the antigen level. In our simulation, a higher antigen concentration (more enemies on the battlefield) leads to a faster and stronger activation of T cells. This, in turn, drives a larger proliferative response (up to the carrying capacity, the maximum number of T cells the body can support). This makes perfect sense biologically: a larger invasion requires a larger counterattack. It’s like sending in more troops when facing a bigger enemy army. Similarly, the Hill coefficient (\\(n\\)) plays a crucial role. A higher Hill coefficient implies a more sensitive response to changes in antigen concentration. It’s like having a hair-trigger alarm, a tiny change in antigen concentration sets off a massive response from the T cells.\nNow, let’s bring this back to our initial motivation: cancer immunotherapy. Our model suggests that the effectiveness of checkpoint blockade therapies (which unleash the immune system against cancer cells) might depend not only on the number of T cells hanging around in the tumor but also on factors like the local antigen concentration (how recognizable the cancer cells are) and the sensitivity of T cells to that antigen (represented by our Hill coefficient). So, it’s not just about having enough soldiers; it’s also about making sure they can see and react to the enemy properly.\n Photo from Richard C. Smith in CE&N\n\n\nWhere Our Digital Cells Fall Short\nIt’s crucial to acknowledge the limitations of our simplified model. We’ve made some necessary simplifications to make it manageable for this post (and to prevent my computer from melting down). Think of it as drawing a cartoon of a person, it captures the basic idea, but it’s not a photorealistic portrait.\nSimplified cell types. We’ve only considered two types of T cells (naive and activated), which is a gross oversimplification. The real immune system has a whole alphabet soup of immune cells (B cells, dendritic cells, regulatory T cells, and many more), each with its own specialized role. Incorporating more cell types would allow us to model more complex interactions and immune responses, like teamwork between different parts of the immune system. It’s like modeling a sports team with only two types of players, you’re missing a lot of the strategic complexity.\nNo spatial dynamics. Our model doesn’t account for the spatial organization of the immune system. In reality, immune cells move around the body, interacting in specific locations like lymph nodes or tissues. Modeling the movement of cells within tissues would add another layer of realism. Similar to simulating a battle without considering the terrain, where the fight happens matters!\nSimplified interactions. We’ve only modeled a few key processes (activation, proliferation, apoptosis). In reality, there are countless interactions between immune cells, including cytokine signaling (chemical messages between cells) and direct cell-cell contact. Modeling cytokine networks and other signaling molecules would allow us to simulate more complex feedback loops and interactions. It’s like trying to recreate a conversation with only three words, you’re missing a lot of detail.\nConstant parameters. We’ve assumed that parameters like the growth rate and apoptosis probability are constant over time. In reality, these parameters can change depending on various factors in the cell’s environment. Modeling these parameters as dependent on the cell environment would allow us to simulate more realistic biological scenarios. Similar assuming that a car’s speed is always constant, regardless of whether it’s going uphill or downhill.\nThese simplifications are necessary for a blog post (and to avoid a complete descent into computational madness), but they mean our model doesn’t capture the full complexity of the real immune system. More complex models, incorporating these missing elements, could be used to address a wider range of immunological questions, from predicting vaccine efficacy to designing personalized immunotherapies.\n\n\nFrom Pixels to Patients\nIn this post, we’ve taken a wild tour around the topic of agent-based modeling and non-linear equations in R, showing how they can be used to simulate the dynamic world of lymphocyte populations. We’ve built a simplified model that captures key processes like activation, proliferation, and apoptosis, and we’ve seen how these processes interact to shape the overall immune response.\nWhile our model is a simplified representation of reality (a digital doodle compared to a biological masterpiece), it provides a valuable framework for understanding the fundamental principles of lymphocyte dynamics. It demonstrates the power of computational modeling in immunology, offering a way to explore complex biological systems in a way that would be difficult or impossible with traditional experimental methods alone. By building these digital models, we can gain insights that could eventually lead to more effective treatments and interventions for a wide range of diseases, bringing us closer to a future where digital cells translate into real-world impact for patients.\n\n\n\n\n\n\nReferences\n\nAn, Li, Volker Grimm, Abigail Sullivan, BL Turner Ii, Nicolas Malleson, Alison Heppenstall, Christian Vincenot, et al. 2021. “Challenges, Tasks, and Opportunities in Modeling Agent-Based Complex Systems.” Ecological Modelling 457: 109685.\n\n\nAzarov, Ivan, Kirill Peskov, Gabriel Helmlinger, and Yuri Kosinsky. 2019. “Role of t Cell-to-Dendritic Cell Chemoattraction in t Cell Priming Initiation in the Lymph Node: An Agent-Based Modeling Study.” Frontiers in Immunology 10: 1289.\n\n\nChang, Chih-Hao, Jing Qiu, David O’Sullivan, Michael D Buck, Takuro Noguchi, Jonathan D Curtis, Qiongyu Chen, et al. 2015. “Metabolic Competition in the Tumor Microenvironment Is a Driver of Cancer Progression.” Cell 162 (6): 1229–41.\n\n\nDushek, Omer, Milos Aleksic, Richard J Wheeler, Hao Zhang, Shaun-Paul Cordoba, Yan-Chun Peng, Ji-Li Chen, et al. 2011. “Antigen Potency and Maximal Efficacy Reveal a Mechanism of Efficient t Cell Activation.” Science Signaling 4 (176): ra39–39.\n\n\nJin, Wang, Scott W McCue, and Matthew J Simpson. 2018. “Extended Logistic Growth Model for Heterogeneous Populations.” Journal of Theoretical Biology 445: 51–61.\n\n\nJoglekar, Alok V, and Guideng Li. 2021. “T Cell Antigen Discovery.” Nature Methods 18 (8): 873–80.\n\n\nKumar, Vinay, Abul K Abbas, and Jon C Aster. 2012. Robbins Basic Pathology e-Book. Elsevier Health Sciences.\n\n\nLi, Long, Jing Ji, Fan Song, and Jinglei Hu. 2023. “Intercellular Receptor-Ligand Binding: Effect of Protein-Membrane Interaction.” Journal of Molecular Biology 435 (1): 167787.\n\n\nLiu, Chenglong, Mengxuan Yang, Daizhou Zhang, Ming Chen, and Di Zhu. 2022. “Clinical Cancer Immunotherapy: Current Progress and Prospects.” Frontiers in Immunology 13: 961805.\n\n\nMorris, Sinead E, Donna L Farber, and Andrew J Yates. 2019. “Tissue-Resident Memory t Cells in Mice and Humans: Towards a Quantitative Ecology.” The Journal of Immunology 203 (10): 2561–69.\n\n\nStefan, Melanie I, and Nicolas Le Novère. 2013. “Cooperative Binding.” PLoS Computational Biology 9 (6): e1003106.\n\n\nTaefehshokr, Sina, Aram Parhizkar, Shima Hayati, Morteza Mousapour, Amin Mahmoudpour, Liliane Eleid, Dara Rahmanpour, Sahand Fattahi, Hadi Shabani, and Nima Taefehshokr. 2022. “Cancer Immunotherapy: Challenges and Limitations.” Pathology-Research and Practice 229: 153723.\n\nCitationBibTeX citation:@misc{castillo-aguilar2024,\n  author = {Castillo-Aguilar, Matías},\n  title = {Modeling {Lymphocyte} {Dynamics:} {Agent-Based} {Simulations}\n    and {Non-Linear} {Equations}},\n  date = {2024-12-23},\n  url = {https://bayesically-speaking.com/posts/2024-12-23 modeling-lymphocyte-dynamics/},\n  doi = {10.59350/aqa1r-e2v59},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2024. “Modeling Lymphocyte Dynamics:\nAgent-Based Simulations and Non-Linear Equations.” December 23,\n2024. https://doi.org/10.59350/aqa1r-e2v59."
  },
  {
    "objectID": "posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/index.html",
    "href": "posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/index.html",
    "title": "So, You Think You Can Model a Heartbeat?",
    "section": "",
    "text": "Introduction\nLet’s talk about your heart. Not the metaphorical one that’s still recovering from that season finale of Stranger Things, but the actual meat-and-potatoes organ pumping in your chest. You know, the thing that goes haywire when you sprint for the bus or binge-watch Squid Game while eating a family-sized pizza all by yourself (that’s what I call self determination). Turns out, scientists (like yours truly) have been obsessed with understanding how this biological drum solo responds to exercise. And let me tell you, it’s not as straightforward as your Fitbit’s cheerful little heart icon would have you believe.\nYour heart isn’t just a pump, it’s a poet. Every beat writes a tiny stanza in the epic of your life, and the R-R interval is the rhythm of that poetry. Technically, it’s the time (in milliseconds) between the peaks of consecutive heartbeats on an ECG, those iconic “R waves” that look like little skyscrapers. Shorter R-R intervals mean your heart is drumming faster (hello, panic-induced Zoom meetings), while longer R-R intervals mean it’s lounging (Netflix marathons, anyone?). Heart rate (beats per minute) is just the inverse of this: divide 60,000 by your RR interval, and voilà, you’ve got your BPM. But here’s the plot twist: your heart isn’t a metronome.\n Example of an electrocardiographic curve, showing your heartbeats; additionally to the time between them, which we denote as R-R intervals.\nEnter heart rate variability (HRV), the hero of cardiac health. HRV measures the subtle variations between those R-R intervals. Think of it as your heart’s improv skills, jazz, not classical. High HRV means your autonomic nervous system is flexing its adaptability, seamlessly switching between “fight-or-flight” mode (sympathetic) and “chill-out” mode (parasympathetic). Low HRV? Your body’s stuck in a stress loop, like a hamster wheel of cortisol. But traditional HRV metrics average these fluctuations over minutes or hours, turning your heart’s freestyle rap into elevator music.\nThat’s where R-R intervals steal the spotlight. By analyzing them beat-to-beat, we catch the micro-dramas, how your heart panics during a sprint, recovers post-burpee, or side-eyes your third coffee.\nNow imagine your heart rate during a workout is like the plot of a Christopher Nolan movie, full of twists, turns, and moments where you’re not entirely sure what’s going on but feel like it’s profound. Traditional methods for analyzing HRV are like trying to explain Inception using only a flip phone: technically possible, but missing all the nuance. For decades, researchers have relied on linear models and aggregated HRV metrics, which are about as useful for capturing the chaos of exercise-induced heart rhythms as a screen door on a submarine. Sure, they tell you something, but it’s like summarizing The Lord of the Rings as “some guys walked to a volcano.” Missing the point? Just a bit.\n Photo from Ketut Subiyanto\nBut hey! I have good news. We developed and published a new non-linear model, the Tony Stark of cardiac analysis, flashy, precise, and built in a cave (okay, a lab) with a bunch of math you’ll almost understand. Instead of forcing your heart’s beat-to-beat intervals (lovingly called R-R intervals) into the rigid straightjacket of linear equations, we’ve embraced the chaos with logistic functions. Think of it as upgrading from a dial-up modem to 5G. These logistic functions are the Beyoncé of math, versatile, dynamic, and capable of handling sudden shifts (like when you go from couch potato to HIIT warrior in 0.2 seconds). They model the heart’s nosedive during exercise and its slow crawl back to Netflix-and-chill mode afterward, all while spitting out parameters that actually mean something to your body. “Baseline R-R interval”? That’s your heart’s resting vibe. “Recovery proportion”? How quickly it forgives you for those burpees.\n Check the paper where we discuss in-depth the model to get all the technicalities. You can visit it here.\nNow, let’s address the elephant in the room: why should you care? Well, unless you’re a cyborg (looking at you, Boston Dynamics), your autonomic nervous system, the puppet master behind your heart’s rhythm, is a drama queen. During exercise, it’s a tug-of-war between the “fight-or-flight” sympathetic system (the one yelling GO!) and the “chill-out” parasympathetic system (the one whispering maybe just one more episode?). Traditional models treat this like a seesaw with one kid glued to the ground. The model that we developed? It’s the entire playground, complete with swing sets, monkey bars, and that one kid who definitely ate too much sugar.\nHere’s where it gets spicy: we tested this bad boy on 272 elderly folks. Why seniors? Because if your heart can handle Zumba at 70, it’s basically the Dwayne Johnson of organs. Spoiler alert: the model nailed it. With an R2 of 0.868 (translation: it’s scarily accurate) and RMSE of 32.6 ms (translation: it’s precise enough to detect your existential crisis during plank holds), it’s like giving doctors and trainers a cheat code for your cardiovascular health. Oh, and we used Hamiltonian Monte Carlo for parameter estimation, which sounds like a spell from Harry Potter but is really just math jedi tricks for “we didn’t guess, we calculated”.\nBut wait, there’s more! Using Sobol sensitivity analysis (a fancy way of asking, “Which part of this equation is doing the heavy lifting?”), we discovered that baseline R-R intervals and recovery proportion are the MVPs of heart rate dynamics. Translation: your heart’s default setting and its ability to bounce back post-workout are the LeBron and Jordan of this game. The other parameters? They’re the benchwarmers. Useful, but not stealing the spotlight.\nNow, let’s talk real-world applications. Imagine a future where your smartwatch doesn’t just shame you for sitting too long but actually understands your heart’s tantrums during spin class. Or where rehab programs are tailored not just to your fitness level but to your autonomic nervous system’s mood swings. This model isn’t just academic nerdery, it’s a gateway to personalized health tech that doesn’t suck.\n Photo from Marta Branco.\nOf course, no hero’s journey is complete without flaws. Our study’s cohort was mostly elderly women, which is like testing a new sports car exclusively on retirees driving to bingo night. Future work? Let’s throw in some college athletes, sleep-deprived parents, and that one friend who does ultramarathons “for fun”. Also, shoutout to R, the coding language we used to simulate this model. If R were a person, it’d be that friend who’s brilliant but insists on explaining everything in memes.\nSo, buckle up. In the next sections, we’ll dive into the code, the curves, and the “aha!” moments that made this paper possible. Whether you’re a cardio junkie, a data science geek, or just someone who wants to know why your heart hates burpees, this ride’s for you. And hey, if you get lost, just remember: it’s not chaos, it’s science.\n\n\nThe Math, Code, and Cardiac Drama Behind the Model\nLet’s peel back the layers of this model like it’s an onion, except instead of tears, you’ll get math, R code, and a newfound appreciation for your heart’s melodramatic tendencies. Buckle up; we’re going full Sherlock Holmes on this equation.\n\nThe Grand Equation: A Symphony of Sigmoids\nAt the heart of our model lies this beauty:\n\\[\n\\text{RRi}(t) = \\alpha + \\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}} + \\frac{-c \\cdot \\beta}{1 + e^{\\phi(t-\\tau-\\delta)}}\n\\]\nThis isn’t just alphabet soup. It’s a carefully orchestrated dance between two logistic functions (the S-shaped curves you’d recognize from population growth or zombie apocalypse models). Together, they capture your heart’s journey from Zen-like rest to exercise-induced panic and back to Netflix mode. Let’s dissect each term like it’s a Breaking Bad episode.\n\n1. Baseline R-R interval (\\(\\alpha\\)): The Calm Before the Storm\n\\(\\alpha\\) (alpha) is your heart’s resting state, the R-R interval when you’re sprawled on the couch debating whether to rewatch The Office again. Mathematically, it’s the vertical shift of the entire curve. Think of it as the “neutral” setting of your heart rate, typically around 800–1000 ms for healthy adults. If \\(\\alpha\\) were a person, it’d be that friend who insists on taking “mental health days” every Monday.\nIn the code, \\(\\alpha\\) is straightforward. When we standardized the RRi data, we centered each individual’s data around their mean (\\(\\text{RRi}_i\\)) and scaled it by their standard deviation (\\(S_{\\text{RRi}_i}\\)). This let us compare Grandma Ethel’s heart rate to Gym Bro Chad’s without bias. In R, this might look like reversing a z-score. No magic here, just arithmetic to keep everyone’s heart rates in their lane.\nCheck how the transformed distribution of R-R intervals maintains its shape but only changes the reference scale. The standardized scale will always be in terms of standard deviations (also known as Z-score) as unit of measurement:\n\nx &lt;- bayestestR::distribution_normal(5000, 800, 30)\n\nplot_data &lt;- data.table(\n  `R-R interval (ms)` = x,\n  `Z-Score (standardized units)` = (x - mean(x))/sd(x)\n) |&gt; melt.data.table(measure.vars = 1:2)\n\nggplot(plot_data, aes(value, fill = variable)) +\n  facet_wrap(~ variable, scales = \"free\") +\n  geom_density(show.legend = FALSE) +\n  scale_y_continuous(expand = c(0,0,0.2,0), breaks = NULL) +\n  scale_x_continuous(expand = c(0.1,0)) +\n  scale_fill_brewer(type = \"qual\", palette = 3)\n\n\n\n\n\n\n\n\n\n\n2. The Panic Parameter (\\(\\beta\\)): When Exercise Hits Like a Plot Twist\n\\(\\beta\\) is where the drama begins. This term controls the magnitude of the R-R interval drop when you start exercising. Negative values mean your heart is freaking out, like when you realize you left the stove on mid-workout. The bigger the absolute value of \\(\\beta\\), the steeper the nosedive. In our study, \\(\\beta = -345\\) ms meant participants’ hearts were basically doing a Free Solo cliff drop during exercise.\nThe first logistic term, \\(\\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}}\\), is the mathematical equivalent of your sympathetic nervous system slamming the gas pedal. The denominator \\(1 + e^{\\lambda(t-\\tau)}\\) ensures the drop isn’t instant, it’s a smooth(ish) transition, like a rollercoaster cresting the first hill.\nIn R, simulating this is a breeze:\n\nf1 &lt;- function(t, alpha, beta, lambda, tau) {\n  alpha + beta / (1 + exp(lambda * (t - tau)))\n}\n\nPlug in \\(\\alpha = 860\\) ms, \\(\\beta = -345\\), \\(\\lambda = -3.05\\), and \\(\\tau = 6.71\\), and you’ve got a curve that drops like Bitcoin in 2022.\n\n\n3. The Drop Rate (\\(\\lambda\\)) and Timing (\\(\\tau\\)): How Fast and When the Chaos Unfolds\n\\(\\lambda\\) (lambda) dictates how steep the R-R interval plummets. A more negative \\(\\lambda\\) means a sharper drop, picture your heart rate during a sprint versus a leisurely stroll. In our model, \\(\\lambda = -3.05\\) is like your heart yelling, “ABANDON SHIP!” the moment exercise starts.\n\\(\\tau\\) (tau) is the inflection point, the exact moment the drop begins. At \\(\\tau = 6.71\\) minutes, it’s the split second your heart realizes you’re not joking about that burpee challenge. This parameter ensures the model doesn’t assume everyone panics at the same time (because let’s face it, some of us start sweating just thinking about exercise).\nThe code snippet for this term is deceptively simple, but the real magic is in the exponent: \\(\\lambda(t - \\tau)\\). This scaling ensures the drop accelerates exponentially once triggered, mimicking the body’s all-hands-on-deck response to physical stress.\n\n\n4. The Recovery Squad (\\(c\\), \\(\\phi\\), \\(\\delta\\)): Redemption Arc Post-Exercise\nAfter the storm comes the calm, or at least, your heart’s attempt at it. The second logistic term, \\(\\frac{-c \\cdot \\beta}{1 + e^{\\phi(t - \\tau - \\delta)}}\\), is where your parasympathetic nervous system (the “rest and digest” crew) steps in to clean up the mess.\n\n\\(c\\): The recovery proportion. If \\(c = 0.84\\), your heart claws back 84% of the drop. It’s like forgiving a partner for eating your leftovers, most of the damage is undone, but you’re still side-eyeing them.\n\n\\(\\phi\\) (phi): The speed of recovery. \\(\\phi = -2.6\\) means your heart rebounds faster than a Marvel hero post-snap. The more negative \\(\\phi\\), the quicker the bounce-back.\n\n\\(\\delta\\) (delta): The lag before recovery begins. \\(\\delta = 3.24\\) minutes is your heart’s version of “I need a minute to process this” after exercise.\n\nIn code:\n\nf2 &lt;- function(t, beta, c, phi, delta, tau) {\n  (-c * beta) / (1 + exp(phi * (t - tau - delta)))\n}\n\nThis term is essentially a mirror image of the first logistic curve, flipped upside down (thanks to the \\(-c \\cdot \\beta\\)) and delayed by \\(\\delta\\). Together, they form a U-shaped curve that’s the hallmark of cardiac recovery, like a phoenix rising from the ashes of your HIIT session.\n\n\n5. The Full Picture: From Rest to Chaos to Redemption\nCombine both terms, and you get the complete RRi trajectory, a symphony of stress and recovery. The first logistic function models the sympathetic nervous system’s hold my beer moment, while the second captures the parasympathetic system’s I got you, fam.\nPlotting this in R is where the magic happens:\n\ntime &lt;- seq(0, 20, by = 0.05)\ntotal &lt;- f1(time, 860, -345, -3.05, 6.71) + \n         f2(time, -345, 0.84, -2.6, 3.24, 6.71)\n\nplot_data &lt;- data.table(\n  time = time,\n  total = total\n)\n\nset.seed(123)\n\nggplot(plot_data, aes(time)) +\n  geom_line(aes(y = total + rnorm(201, 0, 30)), col = \"purple\", lwd = 1/4) +\n  geom_line(aes(y = total + 30), col = \"purple\", lwd = 1/2, lty = 2) +\n  geom_line(aes(y = total - 30), col = \"purple\", lwd = 1/2, lty = 2) +\n  geom_line(aes(y = total), col = \"purple\", lwd = 1) +\n  geom_vline(xintercept = c(6.71, 6.71 + 3.24), lty = 2, col = \"gray50\") +\n  labs(y = \"RRi (ms)\", x = \"Time (min)\",\n       title = \"Your Heart's Epic Journey\",\n       subtitle = \"Simulated and True RRi Signal\")\n\n\n\n\n\n\n\n\nThis plot isn’t just a curve, it’s a story. The dip represents your heart’s “oh crap” phase during exercise, while the rebound is its slow return to sanity. The lag (\\(\\delta\\)) ensures the model doesn’t assume recovery starts the millisecond you stop moving (because let’s be real, your heart needs a breather too).\n\n\nWhy Logistic Functions? Because Your Heart Isn’t Basic\nLinear models are the vanilla ice cream of math, safe, predictable, and kinda boring. But your heart? It’s more of a salted caramel espresso swirl. Logistic functions excel here because they handle saturation effects: the idea that your heart rate can’t drop infinitely (you’re not The Flash) or recover instantly (you’re not Wolverine).\nThe sigmoid shape of \\(\\frac{1}{1 + e^{-x}}\\) is perfect for modeling transitions between states. During exercise, it captures the gradual takeover of sympathetic dominance. Post-exercise, the mirrored sigmoid reflects the parasympathetic system’s cautious return. Together, they’re the yin and yang of cardiac control, like Thor and Loki, but with fewer explosions and stabbings.\n\n\nFrom Math to Medicine: What These Parameters Really Mean\n\nHigh \\(\\alpha\\): Your heart’s baseline is chill AF. Maybe you’re a yogi or just really good at naps.\n\nLow \\(\\beta\\): Your heart panics hard during exercise. Congrats, you’re the Jason Bourne of cardio.\n\nSluggish \\(\\phi\\): Recovery takes forever. Your parasympathetic system is basically a sloth on Ambien.\n\nLong \\(\\delta\\): Your heart takes its sweet time to recover. It’s the George R.R. Martin of organs, delays are inevitable.\n\nThese parameters aren’t just academic, they’re actionable. A low \\(c\\) (recovery proportion) could signal poor autonomic resilience, hinting at conditions like diabetes or hypertension. A wild \\(\\lambda\\) (drop rate) might mean your sympathetic system is overzealous, like a puppy seeing a squirrel.\n\n\nBehind the Curtain: Stan, HMC, and Bayesian Jedi Tricks\nFitting this model isn’t for the faint of heart. We used Stan (a probabilistic programming language) and Hamiltonian Monte Carlo (HMC) to estimate parameters. Think of HMC as a GPS for navigating the jagged terrain of parameter space, it avoids dead ends and U-turns (thanks, No-U-Turn Sampler!) to find the optimal values.\nTranslation: We told Stan, “Here’s our equation, here’s our data, and please don’t blow up.” The result? Posterior distributions for each parameter that tell us not just best guesses but uncertainty, like weather forecasts for your heart rate.\n\n\nWhy This Matters?\nThis model isn’t just a party trick for stats nerds. It’s a bridge between cold, hard data and the messy reality of human physiology. By linking parameters to real-world mechanisms, like sympathetic activation or vagal tone, we can:\n\nPersonalize rehab: Tailor exercises to your autonomic “personality.”\n\nDetect early risks: Spot sluggish recovery before it becomes a problem.\n\nOptimize training: Adjust workout intensity based on real-time heart rate dynamics.\n\nImagine a Fitbit that doesn’t just count steps but whispers, “Hey, your \\(\\phi\\) is low, skip the espresso today.” That’s the future we’re building.\nYour heart isn’t a metronome. It’s a jazz drummer, improvisational, dynamic, and occasionally chaotic. This model embraces that chaos, using logistic functions to map the rhythm of rest, panic, and recovery. Whether you’re a data scientist, a clinician, or someone who just wants to know why burpees feel like death, remember: behind every heartbeat is a story. And now, we’ve got the math to read it.\n\n\n\n\nImplementing Hamiltonian Monte Carlo (HMC)\nSo, you want to estimate parameters for a fancy heart rate model using HMC? Buckle up, this is like teaching a self-driving car to navigate a maze, but instead of a car, it’s math, and instead of a maze, it’s your heart’s chaotic response to exercise. Let’s break it down without the PhD jargon.\n\nStep 1: What Even Is HMC?\nImagine you’re hiking up a mountain (the “posterior distribution” of parameters you want to explore). Traditional methods like Metropolis-Hastings are like taking random steps blindfolded, you’ll eventually reach the peak, but it’ll take forever. HMC? It’s strapping on a jetpack that uses physics (Hamiltonian dynamics) to glide you smoothly toward high-probability areas.\nHere’s the twist:\n\nParameters = Positions: Your model’s unknowns (like \\(\\alpha\\), \\(\\beta\\)) are coordinates on a map.\n\nMomentum = Auxiliary Variables: Fake “physics” variables that give your jetpack thrust.\n\nHamiltonian = Total Energy: Combines “potential energy” (how badly your model fits the data) and “kinetic energy” (your momentum’s oomph).\n\nYou simulate sliding around this energy landscape, guided by gradients (math’s version of GPS), to find the best parameter values.\n\n\nStep 2: The No-U-Turn Sampler (NUTS): Autopilot for HMC\nHMC’s Achilles’ heel? Picking the right step size (\\(\\epsilon\\)) and number of steps (\\(L\\)). Get it wrong, and you’re either crawling or overshooting the peak. Enter NUTS, the Marie Kondo of MCMC, it automates tuning so your sampling “sparks joy.”\nWe have already cover the fundamentals of NUTS in a previous post. But, for the sake of time, let’s quickly remember how it works (in case you obviously read my previous post on NUTS):\n\nBuild a Tree: NUTS grows a binary tree of potential steps forward and backward in time.\n\nStop at U-Turns: If the path starts doubling back (a “U-turn”), it stops. No wasted steps!\n\nAdapt Step Size: During warm-up, NUTS adjusts \\(\\epsilon\\) to hit a ~80% acceptance rate, Goldilocks’ “just right.”\n\nIn our paper, we used Stan (via R’s brms package) to handle this. Stan is like a robot butler that does the math while you sip coffee (or mate1).\n1 It’s like coffee but 10 times more intense and face-wrenching\n\nStep 3: Defining the Model, Plugging in the Equation\nOkay, all fun and stuff but what is the model? Our paper’s model is a beast:\n\\[\n\\text{RRi}(t) = \\alpha + \\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}} + \\frac{-c \\cdot \\beta}{1 + e^{\\phi(t-\\tau-\\delta)}}\n\\]\nIn brms, you’d write this as a non-linear formula:\n\nlibrary(brms)\nmodel_formula &lt;- bf(\n  RRi ~ alpha + beta / (1 + exp(lambda * (time - tau))) +\n                (-c * beta) / (1 + exp(phi * (time - tau - delta))),\n  alpha ~ 1, beta ~ 1, lambda ~ 1, tau ~ 1, c ~ 1, phi ~ 1, delta ~ 1,\n  nl = TRUE\n)\n\nTranslation: “Hey Stan, here’s our equation. The parameters are \\(\\alpha, \\beta, \\lambda, \\tau, c, \\phi, \\delta\\). Go nuts”.\n\n\nStep 4: Setting Priors, Because Even Math Needs Boundaries\nPriors keep the parameters from going rogue. For example:\n- \\(\\beta\\) (exercise-induced drop) should be negative, no one’s R-R interval increases when sprinting.\n- \\(\\tau\\) (panic start time) can’t be negative (unless your heart panics before exercise, which… same).\nIn brms, you’d set these like:\n\npriors &lt;- c(\n  prior(normal(800, 100), nlpar = \"alpha\"),   # Baseline RR ~800 ms\n  prior(normal(-300, 30), nlpar = \"beta\"),    # Drop magnitude ~-300 ms\n  prior(normal(0.8, 0.01), nlpar = \"c\"),      # Recovery ~80%\n  prior(normal(-3, 0.5), nlpar = \"lambda\"),   # Drop rate ~-3\n  prior(normal(-2, 0.5), nlpar = \"phi\"),      # Recovery rate ~-2\n  prior(normal(6, 0.5), nlpar = \"tau\"),       # Panic starts ~6 min\n  prior(normal(3, 0.5), nlpar = \"delta\")      # Recovery lag ~3 min\n)\n\nThese priors aren’t wild guesses, they’re based on domain knowledge (or in this case, the paper’s cohort data).\n\n\nStep 5: Simulating Data, The Noisier the Better\nIn order to illustrate how our model work and how we can estimate parameters from RRi data, we need data in the first place. Let’s create some fictional RRi data for teaching purposes:\n\n## We create a data.frame with time intervals\ndata &lt;- data.frame(\n  time = seq(0, 20, 0.1)\n)\n\n## Then we estimate some arbitrary RRi pattern with added noise\ndata &lt;- within(data, {\n  RRi &lt;- 800 - \n    300 / (1 + exp(-3 * (time - 6))) +\n    0.8 * 300 / (1 + exp(-2 * (time - 6 - 3))) +\n    rnorm(n = length(time), 0, 30)\n})\n\nAnd now that we have cooked some data, let’s see how our simulated RRi pattern looks like:\n\nggplot(data, aes(time, RRi)) +\n  geom_line() +\n  geom_vline(xintercept = c(6,9), col = \"gray\", lty = 2, lwd = .5) +\n  labs(x = \"Time (min)\", y = \"RRi (ms)\", title = \"Simulated Heart Mayhem\")\n\n\n\n\n\n\n\n\n\n\nStep 6: Fitting the Model, Let Stan Do the Heavy Lifting\nWith the formula, priors, and data ready, fitting the model is anticlimactic:\n\nfit &lt;- brm(\n  formula = model_formula,\n  data = data,\n  prior = priors,\n  family = gaussian(),\n  chains = 4,         # Run 4 parallel MCMC chains\n  iter = 2000,        # 2000 iterations per chain\n  warmup = 1000,      # First 1000 are warmup (tuning)\n  control = list(adapt_delta = 0.95),  # Be extra careful\n  file = \"my_rri_model.RDS\"\n)\n\nStan will churn through the data, using HMC (via NUTS) to explore the posterior. It’s like training a neural network, but instead of cat pictures, it’s heart rate curves.\n\n\nStep 7: Checking Convergence, Is the Robot Butler Drunk?\nAfter sampling, you need to check if the chains (parallel runs) agree. Key diagnostics:\n- R-hat ( \\(\\hat{R}\\) ): Should be ≤1.01. If it’s 1.5, your chains are arguing like siblings.\n- Effective Sample Size (ESS): Should be &gt;1000. Low ESS? Your jetpack sputtered.\nIn R:\n\nsummary(fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RRi ~ alpha + beta/(1 + exp(lambda * (time - tau))) + (-c * beta)/(1 + exp(phi * (time - tau - delta))) \n         alpha ~ 1\n         beta ~ 1\n         lambda ~ 1\n         tau ~ 1\n         c ~ 1\n         phi ~ 1\n         delta ~ 1\n   Data: data (Number of observations: 201) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept    800.75      3.45   794.18   807.53 1.00     3052     2914\nbeta_Intercept    -291.16     11.52  -314.42  -269.81 1.00     2077     2512\nlambda_Intercept    -2.88      0.32    -3.54    -2.30 1.00     3034     2480\ntau_Intercept        5.87      0.05     5.77     5.98 1.00     2581     2467\nc_Intercept          0.80      0.01     0.79     0.82 1.00     4280     3403\nphi_Intercept       -2.02      0.26    -2.57    -1.57 1.00     3172     2514\ndelta_Intercept      3.17      0.12     2.94     3.39 1.00     2401     2769\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    29.27      1.48    26.59    32.40 1.00     4401     2760\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook for Bulk_ESS and Tail_ESS in the output. If they’re solid, proceed. If not, cry (or increase iter).\n\n\nWhy This Matters?\nHMC isn’t just academic flexing. By leveraging gradients (derivatives of the log-posterior), it efficiently explores complex, high-dimensional spaces. Traditional methods would get lost; HMC glides through like it’s on rails.\nFor the paper, this meant accurately capturing:\n\nHow quickly someone’s heart panics (\\(\\lambda\\)).\n\nHow much it recovers (\\(c\\)).\n\nWhen the panic starts (\\(\\tau\\)).\n\nThese parameters aren’t just numbers, they’re biomarkers. Low \\(c\\) could flag poor recovery in heart patients. High \\(\\lambda\\) might indicate hyper-reactive stress responses.\nImplementing HMC is like teaching a robot to dance. It’s intricate, requires tuning, and occasionally feels like magic. But with tools like Stan and brms, you don’t need to be a physicist, just a data-savvy human with a problem to solve.\nSo next time your heart races during a workout, remember: there’s an entire universe of math and code working to understand its tantrums. And that’s kinda beautiful.\n\n\n\nVisualizing the Posterior\nLet’s face it, statistics can be drier than a saltine cracker. But when you visualize the posterior distributions of your model parameters, magic happens. Suddenly, abstract numbers transform into a story about your heart’s tantrums, resilience, and secret grudges against burpees. Here’s how to decode the math into physiological insights, complete with R code and plots that even your cardio-phobic cousin would understand.\n\n1. Baseline R-R interval (\\(\\alpha\\)): The Heart’s Resting Vibe\nWe’ll start with the posterior density plot for \\(\\alpha\\), the resting R-R interval. Using brms and bayesplot, we can visualize where this parameter likely lives:\n\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n# Extract posterior samples\nposterior_draws &lt;- as_draws_df(fit)\n\n# Plot density with 90% credible interval\nmcmc_areas(posterior_draws, pars = \"b_alpha_Intercept\", prob = 0.9) +\n  labs(title = \"Baseline R-R interval (α)\",\n       x = \"R-R interval (ms)\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\n\n\n\n\n\n\n\nWhat This Means:\n\nA peak around 800 ms suggests a resting heart rate of ~75 bpm (since RRi ≈ 60,000 / heart rate).\n\nA narrow 90% CI (e.g., 795–805 ms) means little uncertainty.\n\nPhysiological Insight: Low \\(\\alpha\\) (short R-R intervals) could hint at chronic stress or lower fitness (lower R-R intervals, means faster heart rate). High \\(\\alpha\\)? Maybe your participants are fitness pros.\n\n\n\n2. Exercise Panic (\\(\\beta\\)) and Drop Rate (\\(\\lambda\\)): The “Oh Crap” Phase\nNext, let’s plot \\(\\beta\\) (drop magnitude) and \\(\\lambda\\) (drop steepness):\n\n# Combine plots\np1 &lt;- mcmc_areas(posterior_draws, pars = \"b_beta_Intercept\", prob = 0.9) +\n  labs(title = \"Exercise-Induced Drop (β)\",\n       x = \"Δ R-R interval (ms)\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\np2 &lt;- mcmc_areas(posterior_draws, pars = \"b_lambda_Intercept\", prob = 0.9) +\n  labs(title = \"Drop Rate (λ)\",\n       x = \"Rate (per min)\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\nggpubr::ggarrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nWhat This Means:\n\nA \\(\\beta\\) posterior centered at -290 ms means hearts dropped ~290 ms during exercise, like going from 75 bpm (~800 ms) to 120 bpm (~500 ms).\n\nA \\(\\lambda\\) of -3 suggests a sharp drop in R-R intervals; it took ~15 seconds to bottom out. Think of it as your heart yelling, “This is happening?!”\n\nPhysiological Insight: A steeper \\(\\lambda\\) (more negative) implies rapid sympathetic activation, great for athletes, worrisome if paired with poor recovery.\n\n\n\n3. Recovery Parameters (\\(c\\), \\(\\phi\\), \\(\\delta\\)): The Redemption Arc\nNow, the recovery phase, where your heart forgives (or doesn’t) your life choices:\n\n# Plot all three\np1 &lt;- mcmc_areas(posterior_draws, pars = \"b_c_Intercept\", prob = 0.9) +\n  labs(title = \"Recovery Proportion (c)\",\n       x = \"Proportion of Drop Recovered\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\np2 &lt;- mcmc_areas(posterior_draws, pars = \"b_phi_Intercept\", prob = 0.9) +\n  labs(title = \"Recovery Rate (φ)\",\n       x = \"Rate (per min)\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\np3 &lt;- mcmc_areas(posterior_draws, pars = \"b_delta_Intercept\", prob = 0.9) +\n  labs(title = \"Recovery Lag (δ)\",\n       x = \"Lag Time (min)\", y = \"Density\") +\n  theme(axis.text.y.left = element_blank(),\n        axis.ticks.y.left = element_blank())\n\nggpubr::ggarrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\n\nWhat This Means:\n\n\\(c = 0.80\\): Hearts recovered 80% of the exercise drop. The 90% CI (0.79–0.82) means high confidence, which could translate the discussion into autonomic resilience.\n\\(\\phi = -2\\): Recovery was brisk but not instant (about 20-25 seconds). A more negative \\(\\phi\\) would mean faster parasympathetic reactivation.\n\\(\\delta = 3.2\\) min: Hearts waited ~3 minutes post-exercise to start recovering. Longer lads might indicate autonomic fatigue.\n\nPhysiological Insight: Low \\(c\\), \\(\\phi\\) or long \\(\\delta\\) could flag impaired recovery, common in aging or conditions like diabetes.\n\n\n\n4. Simulating RRi Curves: From Posteriors to Predictions\nPosteriors aren’t just pretty plots, they let us simulate real heartbeats. Let’s generate RRi trajectories using parameter samples:\n\n# Get 100 posterior samples\nposterior_samples &lt;- posterior_draws[sample.int(100),]\nnames(posterior_samples) &lt;- names(posterior_samples) |&gt; \n  gsub(pattern = \"^b_\", replacement = \"\") |&gt; \n  gsub(pattern = \"_Intercept$\", replacement = \"\")\n\n# Simulate RRi curves\nsimulated_rri &lt;- purrr::map_dfr(1:100, function(i) {\n  params &lt;- posterior_samples[i, ]\n  tibble::tibble(\n    time = seq(0, 20, by = 0.1),\n    RRi = params$alpha + \n          params$beta / (1 + exp(params$lambda * (time - params$tau))) +\n          (-params$c * params$beta) / (1 + exp(params$phi * (time - params$tau - params$delta))),\n    sim_id = i\n  )\n})\n\n# Plot spaghetti\nggplot(simulated_rri, aes(time, RRi, group = sim_id)) +\n  geom_line(data = data, aes(group = 1), color = \"gray\") +\n  geom_line(alpha = 0.1, color = \"purple\") +\n  labs(title = \"100 Simulated RRi Trajectories\",\n       subtitle = \"Compatible with simulated RRi\",\n       x = \"Time (min)\", y = \"R-R interval (ms)\")\n\n\n\n\n\n\n\n\nWhat This Means:\n\nThis “spaghetti” plot shows the uncertainty in predictions. Thick bands = high confidence. Gaps = “We’re not very sure what happens here.”\n\nThe U-shape is consistent: sharp drop, lag, then recovery. But the timing and depth vary, reflecting individual differences. Compare it to the simulated R-R signal.\nPhysiological Insight: Wider spreads post-exercise (recovery phase) suggest some hearts struggle to bounce back, highlighting candidates for closer monitoring.\n\n\n\n5. Correlations Between Parameters: The Autonomic Tug-of-War\nFinally, let’s explore how parameters interact using a pairwise correlation plot:\n\nmcmc_pairs(fit, pars = c(\"b_alpha_Intercept\", \"b_beta_Intercept\",\n                         \"b_c_Intercept\", \"b_phi_Intercept\"),\n           diag_fun = \"dens\",\n           off_diag_fun = \"hex\")\n\n\n\n\n\n\n\n\nWhat This Means:\n\n\\(\\alpha\\) vs. \\(\\beta\\): Negative correlation? High resting RRi (chill hearts) might panic harder during exercise.\n\n\\(c\\) vs. \\(\\phi\\): Positive correlation? Faster recovery (\\(\\phi\\)) often pairs with fuller recovery (\\(c\\)).\n\nPhysiological Insight: These relationships hint at autonomic coupling, how sympathetic and parasympathetic systems coordinate.\n\n\n\nWhy This Matters?\nThese visualizations aren’t just academic eye candy. They’re tools for:\n- Personalized Medicine: Spotting outliers in \\(\\alpha\\) or \\(c\\) could flag at-risk patients.\n- Training Optimization: Athletes with steeper \\(\\lambda\\) might handle higher intensities.\n- Aging Research: The paper’s elderly cohort had slower recovery (\\(phi\\)), comparing to young cohorts could unravel age-related decline.\nImagine a clinic where these plots pop up during a stress test, guiding real-time interventions. Or a coach adjusting workouts based on a runner’s recovery lag (\\(\\delta\\)). That’s the power of marrying math with physiology.\nYour heart’s story is written in logistic curves and probability densities. By visualizing the posteriors, we’re not just crunching numbers, we’re decoding the language of life itself. And honestly, that’s way cooler than any Fitbit dashboard.\n\n\n\nDiscussion: The Bigger Picture\nLet’s cut to the chase: hearts are messy. They’re not metronomes, they’re jazz musicians, improvisational, dynamic, and occasionally chaotic. For decades, we’ve tried to shove their rhythms into neat little boxes labeled “heart rate variability” or “linear models,” like forcing a wild stallion into a petting zoo. This paper’s non-linear model isn’t just a math flex; it’s a rebellion against oversimplification. By embracing the chaos of R-R intervals with logistic functions, we’re finally acknowledging that your heart’s response to exercise isn’t a straight line, it’s a rollercoaster. And that matters.\nThe implications here are bigger than your last Amazon impulse buy. Traditional HRV metrics, while useful, are like summarizing War and Peace as “some Russians had feelings.” They collapse the entire story of your autonomic nervous system into a single number, losing the plot twists, the moment your sympathetic system slams the gas pedal during a sprint, or the parasympathetic system’s slow-mo victory lap post-exercise. Our model captures these nuances, offering a high-definition lens into cardiac dynamics. For clinicians, this could mean spotting early signs of autonomic dysfunction, think diabetes or hypertension, before they escalate. For athletes, it’s a roadmap to optimize recovery. For the rest of us? It’s proof that burpees are, in fact, the devil’s exercise.\n Photo from Joachim Schnürle in Unsplash\nBut let’s not pop champagne yet. The study’s cohort, elderly individuals, mostly women, is like testing a new sports car on a group of golf cart enthusiasts. While their hearts are marvels of resilience (shoutout to the 70-year-olds out-Zumba-ing millennials), aging inherently alters autonomic function. Younger hearts, with their spry vagal tone and metabolic efficiency, might dance to a different beat. Imagine applying this model to college athletes: would their recovery lag (\\(\\delta\\)) be shorter? Would their panic parameter (\\(\\beta\\)) be more dramatic? We don’t know, and that’s a problem. The model’s current demographic blind spot limits its universal swagger. Future work needs to throw a wider net: young adults, elite athletes, sleep-deprived parents, and yes, even that one friend who does ultramarathons “for fun”. Only then can we see if this framework is the Swiss Army knife of cardiac analysis or a niche tool for bingo night regulars.\nThen there’s the elephant in the room: real-world noise. The study’s controlled setting, clean data, preprocessed R-R intervals, is the scientific equivalent of a TikTok dance studio. But life isn’t a lab. Stress, caffeine, and that third espresso you shouldn’t have had all jumble the heart’s signals. Imagine deploying this model in the wild, where wearables battle motion artifacts and Wi-Fi dead zones. Can it handle the chaos? The paper’s synthetic data tests suggest yes, but until it’s stress-tested on Apple Watches dangling off sweaty wrists during CrossFit hell, we’re cautiously optimistic. Integrating real-time environmental factors, temperature, humidity, cortisol levels, could turn this model from a neat trick into a clinical powerhouse.\nSpeaking of wearables, let’s talk tech. The current Fitbit experience is like having a backseat driver who only knows two phrases: “You’re doing great!” and “Maybe sit down?” Our model could upgrade that to a co-pilot who actually understands your heart’s tantrums. Imagine your smartwatch whispering, “Your recovery proportion (\\(c\\)) is low, skip the burpees today,” or “Your \\(\\phi\\) is stellar, go crush that PR.” This isn’t sci-fi; it’s the logical next step. But to get there, we need seamless integration with wearable APIs, edge computing to handle real-time analysis, and user interfaces that don’t look like they were designed in Excel. Oh, and battery life that doesn’t die mid-workout. Priorities, people.\n Photo from Saradasish Pradhan in Unsplash\nNow, let’s address the model’s Achilles’ heel: interpretability. Sure, logistic functions are elegant, but explaining them to a cardiologist over coffee is like teaching quantum physics to a golden retriever. The parameters, \\(\\alpha\\), \\(\\beta\\), \\(\\lambda\\), are meaningful to math nerds, but how do we translate “recovery rate (\\(\\phi\\)) = -2.6” into actionable advice? The answer: better visualization tools. Think interactive dashboards where doctors slide parameters and watch simulated heart rate curves morph in real time. Or apps that gamify autonomic resilience, turning “improve your \\(c\\)” into a quest worthy of Zelda. Bridging the gap between equations and empathy is the next frontier.\nAnd what about the autonomic nervous system’s dark corners? The model assumes a tidy battle between sympathetic and parasympathetic systems, but biology is messy. Emerging research suggests the “sympathovagal balance” is more of a frenemies dynamic, sometimes they collaborate, sometimes they throw shade (Storniolo et al. 2021). Can the model capture that? Not yet. Future iterations might need coupled differential equations or network analysis to untangle the web. Plus, factors like respiratory sinus arrhythmia (where breathing tweaks heart rate) need to be controlled for in this framework. Incorporating these could transform the model from a two-character play into an ensemble drama.\nLet’s not forget the ethical tightrope. Personalized health data is a goldmine for innovation, and a minefield for privacy. If your smartwatch knows your heart’s deepest secrets, who else gets a backstage pass? Insurance companies? Employers? The model’s potential is undeniable, but without robust data ethics, we’re one leak away from a Black Mirror episode. Transparency, consent, and encryption aren’t buzzwords, they’re the price of admission.\n Photo from Maxim Hopman in Unsplash\nSo, where does this leave us? The paper is a leap forward, but the marathon’s just begun. The road ahead is paved with unanswered questions: (1) Can the model predict cardiac events? (2) How does it interact with other biomarkers like blood pressure or glucose levels? (3) Can it adapt to pathologies like atrial fibrillation? Each step requires collaboration, mathematicians, clinicians, engineers, and yes, even ethicists, to turn this from a cool paper into a lifesaving tool.\nIn the end, this work isn’t about equations or R2 values. It’s about honoring the heart’s complexity. Your heart isn’t a machine; it’s a storyteller. Each beat whispers tales of stress, joy, fatigue, and resilience. By listening, truly listening, to its non-linear narrative, we’re not just doing science. We’re learning a new language, one that could rewrite the future of healthcare. And honestly, that’s a plot twist worth sticking around for.\n\n\n\n\nAppendix\n\nCheck the original paper where the model is presented here.\n\n\n\nReferences\n\nStorniolo, Jorge L, Beatrice Cairo, Alberto Porta, and Paolo Cavallari. 2021. “Symbolic Analysis of the Heart Rate Variability During the Plateau Phase Following Maximal Sprint Exercise.” Frontiers in Physiology 12: 632883.\n\nCitationBibTeX citation:@misc{castillo-aguilar2025,\n  author = {Castillo-Aguilar, Matías},\n  title = {So, {You} {Think} {You} {Can} {Model} a {Heartbeat?}},\n  date = {2025-03-24},\n  url = {https://bayesically-speaking.com/posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2025. “So, You Think You Can Model a\nHeartbeat?” March 24, 2025. https://bayesically-speaking.com/posts/2025-03-24\nso-you-think-you-can-model-a-heartbeat/."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Services"
  },
  {
    "objectID": "services.html#every-problem-is-an-opportunity",
    "href": "services.html#every-problem-is-an-opportunity",
    "title": "Services",
    "section": "Every problem is an opportunity",
    "text": "Every problem is an opportunity\nIf you are having issues with your statistical analyses, research or thesis project, say no more. Here at Bayesically-Speaking we believe in a collaborative and supporting environment, providing consulting services throughout your stats problem.\nWhether you need assistance solving methodological issues, sample size power, interpreting complex models or just in need to wrap your head around statistical concepts, you are in the right place."
  },
  {
    "objectID": "services.html#why-choose-us",
    "href": "services.html#why-choose-us",
    "title": "Services",
    "section": "Why choose us?",
    "text": "Why choose us?\n\n\nExperience\nWe have worked across diverse industries, from healthcare to political sciences.\n\n\nCollaboration\nWe partner with you to understand your unique needs and tailor solutions.\n\n\nResults-Driven\nWe don’t just crunch numbers; we deliver actionable insights.\n\n\nPeople-Centered\nWe are compromised to deliver solutions for non-statisticians."
  },
  {
    "objectID": "services.html#lets-work-together",
    "href": "services.html#lets-work-together",
    "title": "Services",
    "section": "Let’s work together!",
    "text": "Let’s work together!\n Send us an email!"
  },
  {
    "objectID": "posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/spanish/index.html",
    "href": "posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/spanish/index.html",
    "title": "¿Crees saber cómo modelar el ritmo cardíaco?",
    "section": "",
    "text": "Introducción\nHablemos de tu corazón. No el metafórico que todavía se está recuperando del final esa temporada de Stranger Things, sino el órgano de carne y papas que bombea en tu pecho. Ya sabes, esa cosa que se acelera cuando corres para alcanzar la micro o cuando te tragas Squid Game con una pizza familiar solito (eso le llamo autodeterminación gourmet). Resulta que los científicos (como este humilde servidor) llevamos obsesionados con entender cómo este solo de batería biológico reacciona al ejercicio. Y créeme, no es tan simple como el iconito feliz del corazón de tu Fitbit quiere hacerte creer.\nTu corazón no es solo una bomba, es un poeta. Cada latido es un verso en el poema épico de tu vida, y el intervalo R-R es el ritmo de esa poesía. Técnicamente, es el tiempo (en milisegundos) entre los picos de dos latidos en un ECG, esas icónicas “ondas R” que parecen mini rascacielos. Intervalos R-R cortos = tu corazón late más rápido (hola, reuniones por Zoom que dan ansiedad), mientras que intervalos largos = está en modo reposo (¿alguien dijo maratón de 31 minutos?). La frecuencia cardíaca (latidos por minuto) es simplemente el inverso: divide 60,000 por tu intervalo R-R y voilà, tienes tus latidos por minuto. Pero aquí viene el plot twist: tu corazón no es metrónomo.\n\nEjemplo de una curva en un electrocardiograma, mostrando tus latidos cardíacos; así como el tiempo entre latidos, al cual denominamos intervalo R-R.\nEntra en escena la variabilidad de la frecuencia cardíaca (VFC), el héroe anónimo de la salud cardíaca. La VFC mide las sutiles variaciones entre esos intervalos R-R. Imagínala como el talento improvisador de tu corazón, jazz, no música clásica. Una VFC alta significa que tu sistema nervioso autonómico es flexible, cambiando al tiro entre el modo “pelea o huida” (simpático) y el modo “relax” (parasimpático). ¿VFC baja? Tu cuerpo está atrapado en un ciclo de estrés, como una rueda de hámster llena de cortisol. Pero las métricas tradicionales de VFC promedian estas fluctuaciones, convirtiendo el freestyle de tu corazón en música de ascensor.\nAhí es donde los intervalos R-R roban el protagonismo. Analizándolos latido a latido, capturamos los microdramas: cómo tu corazón se asusta en una corrida, se recupera post-burpee o le hace cara fea a tu tercer café del día.\nAhora imagina que tu frecuencia cardíaca durante el ejercicio es como el guión de una película de Christopher Nolan: llena de vueltas, giros y momentos en que no sabes qué está pasando, pero sientes que es profundo. Los métodos tradicionales para analizar la variabilidad de la frecuencia cardíaca (VFC) son como tratar de explicar Inception con un celular del 2005: técnicamente posible, pero pierdes toda la onda. Por décadas, los investigadores han usado modelos lineales y métricas de VFC resumidas, que sirven tanto para capturar el caos del corazón durante el ejercicio como un paraguas en un terremoto. Sí, te dicen algo, pero es como resumir El Señor de los Anillos como “unos tipos caminaron a un volcán”. ¿Perdiendo el punto? Un poquito nomás.\n\nFoto de Ketut Subiyanto\n¡Pero atención! Tengo buenas noticias. Desarrollamos y publicamos un nuevo modelo no lineal, el Tony Stark de los análisis cardíacos: brillante, preciso y hecho en una cueva (bueno, un laboratorio) con matemáticas que casi entenderías. En vez de meter los intervalos latido a latido (llamados intervalos R-R para los amigos) en el corsé rígido de ecuaciones lineales, abrazamos el caos con funciones logísticas. Imagínatelo como cambiar del internet con módem al 5G. Estas funciones logísticas son la Beyoncé de las matemáticas: versátiles, dinámicas y capaces de manejar cambios bruscos (como cuando pasas de modo vago a HIIT en 0.2 segundos). Modelan la caída del corazón durante el ejercicio y su lento retorno al modo Netflix-relajo después, todo mientras escupen parámetros que importan para tu cuerpo. ¿“Intervalo R-R basal”? Es el ritmo base de tu corazón. ¿“Proporción de recuperación”? Cuán rápido te perdona por esos burpees.\n\nRevisa el paper donde discutimos en profundidad el modelo para todos los tecnicismos. Puedes verlo acá.\nAhora, hablemos de lo obvio: ¿para qué te sirve esto? Bueno, a menos que seas un ciborg (te estamos mirando, Boston Dynamics), tu sistema nervioso autónomo, el titiritero tras el ritmo cardíaco, es una reina del drama. Durante el ejercicio, es una pulseada entre el sistema simpático “pelea-o-huye” (el que grita ¡DALE!) y el parasimpático “relájate un rato” (el que susurra ¿un capitulito más no más?). Los modelos tradicionales tratan esto como un sube-y-baja con un cabro pegado al suelo. ¿Nuestro modelo? Es la plaza entera, con columpios, pasamanos y ese cabro que claramente se pasó con las bebidas.\nAquí viene lo bueno: probamos este prodigio en 272 personas mayores. ¿Por qué viejitos? Porque si tu corazón aguanta Zumba a los 70, es básicamente el Dwayne Johnson de los órganos. Spoiler: el modelo la rompió. Con un R2 de 0.868 (traducción: es terriblemente preciso) y un RMSE de 32.6 ms (traducción: detecta hasta tu crisis existencial durante las planchas), es como darle a médicos y entrenadores un truco de God Mode para tu salud cardiovascular. Ah, y usamos Hamiltonian Monte Carlo para estimar parámetros, que suena como un hechizo de Harry Potter pero en realidad es matemática nivel jedi para decir “no adivinamos, lo calculamos”.\n¡Pero espera, hay más! Con un análisis de sensibilidad de Sobol (o sea, preguntar ¿qué parte de la ecuación hace la pega?), descubrimos que el intervalo R-R basal y la proporción de recuperación son los cracks de la dinámica cardíaca. Traducción: la configuración default de tu corazón y su capacidad para recuperarse después del ejercicio son el Messi y el Alexis de esta película. Los otros parámetros? Son la banca. Útiles, pero no la llevan.\nHablemos de aplicaciones reales. Imagina un futuro donde tu reloj inteligente no solo te reta por estar sentado, sino que entiende los berrinches de tu corazón durante el spinning. O donde los programas de rehab se ajustan no solo a tu condición física, sino a los cambios de ánimo de tu sistema nervioso. Este modelo no es solo paja académica, es la puerta a tecnología de salud personalizada que no da pena.\n\nFoto de Marta Branco.\nClaro, ningún héroe es perfecto. Nuestro estudio se hizo principalmente con mujeres mayores, como probar un auto deportivo solo con abuelitos yendo al bingo. ¿Próximos pasos? Meter atletas universitarios, padres sin dormir y ese amigo que hace ultramaratones “para relajarse”. Además, un saludo a R, el lenguaje de programación que usamos. Si R fuera persona, sería ese amigo que es seco pero te explica todo con memes.\nAsí que ajústense el cinturón. En las próximas secciones, nos meteremos en el código, las curvas y los momentos “¡eureka!” que hicieron este paper posible. Ya seas fanático del cardio, ñoño de datos o solo alguien que quiere saber por qué tu corazón odia los burpees, esto es para ti. Y si te pierdes, acuérdate: no es caos, es ciencia.\n\n\nLas Matemáticas, el Código y el Drama Cardíaco Detrás del Modelo\nVamos a pelar las capas de este modelo como una cebolla, pero en vez de lágrimas, te saldrán mates, código R y una nueva apreciación por las tendencias melodramáticas de tu corazón. Ajústate el cinturón; vamos a ponernos modo Sherlock Holmes con esta ecuación.\n\nLa Gran Ecuación: Una Sinfonía de Sigmoides\nEn el corazón del modelo está esta belleza:\n\\[  \n\\text{RRi}(t) = \\alpha + \\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}} + \\frac{-c \\cdot \\beta}{1 + e^{\\phi(t-\\tau-\\delta)}}  \n\\]\nEsto no es solo sopa de letras. Es un puro caso coordinado entre dos funciones logísticas (esas curvas en forma de S que conocís de modelos de población o apocalipsis zombi). Juntas, capturan el viaje de tu corazón desde el modo zen hasta el pánico del ejercicio y de vuelta al Netflix. Vamos a diseccionar cada término como un episodio de Breaking Bad.\n\n1. Intervalo RR Basal (\\(\\alpha\\)): La Calma Antes de la Tormenta\n\\(\\alpha\\) (alfa) es el estado de reposo de tu corazón, el intervalo R-R cuando esta tirado en el sillón pensando si ver The Office por quinta vez. Matemáticamente, es el corrimiento vertical de toda la curva. Imagínatelo como el amigo que insiste en tomarse “días de salud mental” todos los lunes.\nEn el código, \\(\\alpha\\) es simple. Cuando estandarizamos los datos RRi, centramos cada dato en su media (\\(\\text{RRi}_i\\)) y lo escalamos por su desviación estándar (\\(S_{\\text{RRi}_i}\\)). Así comparamos el corazón de la abuela Rosa con el del Gym Bro Chad sin sesgos. En R, esto sería como revertir un z-score. Nada de magia, solo aritmética para que cada corazón se quede en su cancha.\nFíjate cómo la distribución transformada de los intervalos R-R mantiene su forma pero cambia la escala de referencia. La escala estandarizada siempre estará en desviaciones estándar (también conocido como Z-score):\n\nx &lt;- bayestestR::distribution_normal(5000, 800, 30)  \n\nplot_data &lt;- data.table(  \n  `Intervalo R-R (ms)` = x,  \n  `Z-Score (unidades estandarizadas)` = (x - mean(x))/sd(x)  \n) |&gt; melt.data.table(measure.vars = 1:2)  \n\nggplot(plot_data, aes(value, fill = variable)) +  \n  facet_wrap(~ variable, scales = \"free\") +  \n  geom_density(show.legend = FALSE) +  \n  scale_y_continuous(expand = c(0,0,0.2,0), breaks = NULL) +  \n  scale_x_continuous(expand = c(0.1,0)) +  \n  scale_fill_brewer(type = \"qual\", palette = 3)  \n\n\n\n\n\n\n\n\n\n\n2. El Parámetro del Pánico (\\(\\beta\\)): Cuando el Ejercicio Pegue Como Twist de Telenovela\n\\(\\beta\\) es donde empieza el drama. Este término controla la caída del intervalo RR cuando empiezas a ejercitarte. Valores negativos significan que tu corazón se está agitando, como cuando sabes que dejaste la estufa prendida haciendo sentadillas. Mientras más grande el valor absoluto de \\(\\beta\\), más brusca la caída en los intervalos R-R. En el estudio, \\(\\beta = -345\\) ms significó que los corazones hicieron un Free Solo en pleno ejercicio.\nEl primer término logístico, \\(\\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}}\\), es el equivalente matemático de tu sistema simpático apretando el acelerador. El denominador \\(1 + e^{\\lambda(t-\\tau)}\\) asegura que la caída no sea instantánea, sino suave (más o menos), como una montaña rusa subiendo la primera cuesta.\nEn R, simular esto es pan comido:\n\nf1 &lt;- function(t, alpha, beta, lambda, tau) {  \n  alpha + beta / (1 + exp(lambda * (t - tau)))  \n}  \n\nPoné \\(\\alpha = 860\\) ms, \\(\\beta = -345\\), \\(\\lambda = -3.05\\), y \\(\\tau = 6.71\\), y tienes una curva que cae como el Bitcoin en 2022.\n\n\n3. Tasa de Caída (\\(\\lambda\\)) y Timing (\\(\\tau\\)): ¿Cuán Rápido y Cuándo se Arma el Lío?\n\\(\\lambda\\) (lambda) dicta qué tan rápido se desploma el intervalo RR. Un \\(\\lambda\\) más negativo significa una caída más brusca, como tu corazón en una corrida versus una caminata tranquila. En el modelo, \\(\\lambda = -3.05\\) es como si tu corazón gritara “¡SÁLVESE QUIEN PUEDA!” al empezar el ejercicio.\n\\(\\tau\\) (tau) es el punto de inflexión, el momento exacto en que empieza el caos. Con \\(\\tau = 6.71\\) minutos, es el segundo en que tu corazón notó que hablabas en serio con los burpees. Este parámetro evita que el modelo asuma que todos entran en pánico al mismo tiempo (porque, seamos honestos, a algunos nos transpira la mano solo de pensar en ejercicios).\nEl código para este término es engañosamente simple, pero la magia está en el exponente: \\(\\lambda(t - \\tau)\\). Esto hace que la caída se acelere exponencialmente una vez activada, imitando la respuesta de “todos a la bodega” del cuerpo al estrés físico.\n\n\n4. El Equipo de Recuperación (\\(c\\), \\(\\phi\\), \\(\\delta\\)): La Redención Post-Ejercicio\nDespués del caos, la calma… o al menos el intento. El segundo término logístico, \\(\\frac{-c \\cdot \\beta}{1 + e^{\\phi(t - \\tau - \\delta)}}\\), es donde tu sistema parasimpático (el “modo siesta”) llega a limpiar el desastre.\n\n\\(c\\): La proporción de recuperación. Si \\(c = 0.84\\), tu corazón recupera el 84% de la caída. Es como perdonar a tu pololo por comerse tus completos: se arregla casi todo, pero igual le tiras miradas asesinas.\n\n\\(\\phi\\) (phi): Velocidad de recuperación. \\(\\phi = -2.6\\) significa que tu corazón vuelve a la normalidad más rápido que un héroe de Marvel después del chasquido. Mientras más negativo, más rápido.\n\n\\(\\delta\\) (delta): El retraso antes de recuperar. \\(\\delta = 3.24\\) minutos es el “dame un minuto para respirar” de tu corazón post-ejercicio.\n\nEn código:\n\nf2 &lt;- function(t, beta, c, phi, delta, tau) {  \n  (-c * beta) / (1 + exp(phi * (t - tau - delta)))  \n}  \n\nEste término es un reflejo invertido de la primera curva logística, demorado por \\(\\delta\\). Juntos forman una curva en U, la huella dactilar de la recuperación cardíaca, como un fénix saliendo de las cenizas de tu sesión de HIIT.\n\n\n5. El Panorama Completo: Del Reposo al Caos y la Redención\nCombinando ambos términos, tienes la trayectoria completa del RRi: una sinfonía de estrés y recuperación. La primera función logística modela el momento simpático de “yo me la juego”, y la segunda captura el parasimpático de “tranquilo, aquí estoy”.\nGraficar esto en R es donde sale la magia:\n\ntime &lt;- seq(0, 20, by = 0.05)  \ntotal &lt;- f1(time, 860, -345, -3.05, 6.71) +   \n         f2(time, -345, 0.84, -2.6, 3.24, 6.71)  \n\nplot_data &lt;- data.table(  \n  time = time,  \n  total = total  \n)  \n\nset.seed(123)  \n\nggplot(plot_data, aes(time)) +  \n  geom_line(aes(y = total + rnorm(201, 0, 30)), col = \"purple\", lwd = 1/4) +  \n  geom_line(aes(y = total + 30), col = \"purple\", lwd = 1/2, lty = 2) +  \n  geom_line(aes(y = total - 30), col = \"purple\", lwd = 1/2, lty = 2) +  \n  geom_line(aes(y = total), col = \"purple\", lwd = 1) +  \n  geom_vline(xintercept = c(6.71, 6.71 + 3.24), lty = 2, col = \"gray50\") +  \n  labs(y = \"RRi (ms)\", x = \"Tiempo (min)\",  \n       title = \"La Epopeya de Tu Corazón\",  \n       subtitle = \"Señal RRi Simulada y Real\")  \n\n\n\n\n\n\n\n\nEste gráfico no es solo una curva, es un relato. La caída es la fase “¡ah, la cagué!” de tu corazón durante el ejercicio, y el rebote es su lento regreso a la cordura. El retraso (\\(\\delta\\)) evita que el modelo asuma que la recuperación empieza al tiro (porque, claro, tu corazón también necesita su minuto).\n\n\n¿Por Qué Funciones Logísticas? Porque Tu Corazón No Es George Harris\nLos modelos lineales son el helado de vainilla de la matemática: seguros, predecibles y medio aburridos. ¿Pero tu corazón? Es más como un helado de manjar-ronrón con chispas de terremoto. Las funciones logísticas sirven porque manejan efectos de saturación: tu corazón no puede caer infinitamente (no sos Flash) ni recuperarse al tiro (no sos Wolverine).\nLa forma sigmoide de \\(\\frac{1}{1 + e^{-x}}\\) es perfecta para modelar transiciones entre estados. Durante el ejercicio, captura la toma de control del sistema simpático. Post-ejercicio, el sigmoide invertido refleja el retorno cauteloso del parasimpático. Juntos, son el yin y yang del control cardíaco, como Thor y Loki, pero con menos explosiones.\n\n\nDe las Matemáticas a la Medicina: Lo Que Realmente Importa\n\n\\(\\alpha\\) alto: Tu corazón es pura calma. Quizás sos yogui o simplemente un rey de las siestas.\n\n\\(\\beta\\) bajo: Tu corazón se asusta brigido con el ejercicio. Felicitaciones, sos el Jason Bourne del cardio.\n\n\\(\\phi\\) lento: La recuperación toma siglos. Tu parasimpático es un perezoso en modo zombie.\n\n\\(\\delta\\) largo: Tu corazón se toma su tiempo para recuperarse. Es el George R.R. Martin de los órganos: demoras garantizadas.\n\nEstos parámetros no son solo académicos, son útiles. Un \\(c\\) bajo (proporción de recuperación) podría indicar mala resiliencia autonómica, señalando diabetes o hipertensión. Un \\(\\lambda\\) salvaje (tasa de caída) sugiere que tu sistema simpático es más intenso que perro viendo una liebre.\n\n\nTras Bambalinas: Stan, HMC y Trucos de Jedi Bayesiano\nAjustar este modelo no es para pusilánimes. Usamos Stan (lenguaje de programación probabilística) y Hamiltonian Monte Carlo (HMC) para estimar parámetros. Imagina HMC como un Waze para navegar el terreno escarpado del espacio paramétrico: evita callejones sin salida (¡gracias, No-U-Turn Sampler!) para encontrar los valores óptimos.\nTraducción: Le dijimos a Stan, “acá está la ecuación, los datos, y porfa no explotes.” ¿Resultado? Distribuciones posteriores que nos dan no solo mejores estimaciones sino incertidumbre, como pronósticos del tiempo para tu corazón.\n\n\n¿Por Qué Importa Esto?\nEste modelo no es solo un truco para ñoños de estadísticas. Es un puente entre datos fríos y el caos de la fisiología humana. Al vincular parámetros con mecanismos reales (como activación simpática o tono vagal), podemos:\n\nPersonalizar rehabilitación: Adaptar ejercicios a tu “personalidad” autonómica.\n\nDetectar riesgos temprano: Reconocer cuando tu recuperación lenta antes de que sea grave.\nOptimizar entrenamiento: Ajustar intensidad según dinámicas cardíacas en tiempo real.\n\nImagina un Fitbit que no solo cuente pasos, sino que te diga: “Oye, tu \\(\\phi\\) está bajo, mejor no tomes café hoy.” Ese es el futuro que estamos creando.\nTu corazón no es un metrónomo. Es un baterista de jazz, improvisando, dinámico y a veces caótico. Este modelo abraza ese caos, usando funciones logísticas para mapear el ritmo de reposo, pánico y recuperación. Ya seas científico de datos, médico o alguien que solo quiere saber por qué los burpees dan ganas de llorar, acuérdate: detrás de cada latido hay una historia. Y ahora, tenemos las mates para leerla.\n\n\n\n\nImplementando el Hamiltonian Monte Carlo (HMC)\n¿Quieres estimar parámetros para un modelo fancy de ritmo cardíaco usando HMC? Ajústate el cinturón, esto es como enseñarle a un auto autónomo a navegar un laberinto, pero en vez de auto, son ecuaciones, y en vez de laberinto, es el caos que hace tu corazón con el ejercicio. Vamos a simplificarlo sin jerga de doctorado.\n\nPaso 1: ¿Qué Chu*** es HMC?\nImagina que estas escalando un cerro (la “distribución posterior” de parámetros que quieres explorar). Métodos tradicionales como Metropolis-Hastings son como caminar a ciegas dando pasos al azar: eventualmente llegas a la cima, pero te demoras una eternidad. ¿HMC? Es como ponerte una mochila jet que usa física (dinámica hamiltoniana) para deslizarte suave hacia las zonas más probables.\nEl truco:\n\nParámetros = Posiciones: Tus incógnitas (\\(\\alpha\\), \\(\\beta\\)) son coordenadas en un mapa.\n\nMomento = Variables Auxiliares: Variables “físicas” ficticias que le dan impulso a tu mochila jet.\n\nHamiltoniano = Energía Total: Combina “energía potencial” (qué tan malo es el ajuste del modelo) y “energía cinética” (el impulso del momento).\n\nSimula deslizarte por este paisaje de energía, guiado por gradientes (el GPS de las matemáticas), para encontrar los mejores valores.\n\n\nPaso 2: El No-U-Turn Sampler (NUTS): Piloto Automático para el HMC\n¿El talón de Aquiles del HMC? Elegir el tamaño de paso (\\(\\epsilon\\)) y el número de pasos (\\(L\\)). Si te equivocas, o avanzas a paso de tortuga o te pasái de largo. Ahí entra NUTS, la Marie Kondo de los MCMC: ordena el ajuste automático para que tu muestreo genere alegría.\nYa cubrimos lo básico de NUTS en un post anterior. Pero, para ahorrar tiempo, recordemos rápidamente cómo funciona (ya que, obviamente, ya leíste mi post anterior de NUTS):\n\nConstruye un Árbol: NUTS crea un árbol binario de pasos hacia adelante y atrás.\n\nDetención en U-Turn: Si el camino empieza a retroceder (un “giro en U”), se detiene. ¡Sin pasos perdidos!\n\nAjuste de Paso: Durante el calentamiento, NUTS ajusta \\(\\epsilon\\) para lograr una tasa de aceptación ~80%, el “justo” de Ricitos de Oro.\n\nEn nuestro paper, usamos Stan (vía el paquete brms de R). Stan es como un mayordomo robot que hace las mates mientras tomas café (o mate1).\n1 Es como el café pero 10 veces más intenso y amargo\n\nPaso 3: Definiendo el Modelo, Metiendo la Ecuación\nBueno, todo lindo, pero ¿cuál es el modelo? Nuestra ecuación es una bestia:\n\\[  \n\\text{RRi}(t) = \\alpha + \\frac{\\beta}{1 + e^{\\lambda(t-\\tau)}} + \\frac{-c \\cdot \\beta}{1 + e^{\\phi(t-\\tau-\\delta)}}  \n\\]\nEn brms, escribirías esto como una fórmula no lineal:\n\nlibrary(brms)  \nmodel_formula &lt;- bf(  \n  RRi ~ alpha + beta / (1 + exp(lambda * (tiempo - tau))) +  \n                (-c * beta) / (1 + exp(phi * (tiempo - tau - delta))),  \n  alpha ~ 1, beta ~ 1, lambda ~ 1, tau ~ 1, c ~ 1, phi ~ 1, delta ~ 1,  \n  nl = TRUE  \n)  \n\nTraducción: “Oye Stan, acá está la ecuación. Los parámetros son \\(\\alpha, \\beta, \\lambda, \\tau, c, \\phi, \\delta\\). ¡Métete con todo!”.\n\n\nPaso 4: Poniendo priors, Porque Hasta las Ecuaciones Necesitan Límites\nLos priors evitan que los parámetros se descontrolen. Por ejemplo:\n- \\(\\beta\\) (caída por ejercicio) debe ser negativo; nadie tiene intervalos R-R que aumenten al correr.\n- \\(\\tau\\) (inicio del pánico) no puede ser negativo (a menos que tu corazón entre en pánico antes de ejercitarte… igual, te entiendo).\nEn brms, los definirías así:\n\npriors &lt;- c(  \n  prior(normal(800, 100), nlpar = \"alpha\"),   # RR basal ~800 ms  \n  prior(normal(-300, 30), nlpar = \"beta\"),    # Caída ~-300 ms  \n  prior(normal(0.8, 0.01), nlpar = \"c\"),      # Recuperación ~80%  \n  prior(normal(-3, 0.5), nlpar = \"lambda\"),   # Tasa de caída ~-3  \n  prior(normal(-2, 0.5), nlpar = \"phi\"),      # Tasa de recuperación ~-2  \n  prior(normal(6, 0.5), nlpar = \"tau\"),       # Pánico empieza ~6 min  \n  prior(normal(3, 0.5), nlpar = \"delta\")      # Retraso recuperación ~3 min  \n)  \n\nEstos priors no son inventos, vienen de conocimiento del área (o, en este caso, de datos del estudio).\n\n\nPaso 5: Simulando Datos, Entre Más Ruido, Mejor\nPara ilustrar cómo funciona el modelo y estimar parámetros, necesitamos datos. Creemos unos ficticios:\n\n## Creamos un data.frame con intervalos de tiempo  \ndata &lt;- data.frame(  \n  tiempo = seq(0, 20, 0.1)  \n)  \n\n## Generamos un patrón RRi con ruido agregado  \ndata &lt;- within(data, {  \n  RRi &lt;- 800 -   \n    300 / (1 + exp(-3 * (tiempo - 6))) +  \n    0.8 * 300 / (1 + exp(-2 * (tiempo - 6 - 3))) +  \n    rnorm(n = length(tiempo), 0, 30)  \n})  \n\nY ahora que tenemos datos cocinados, veamos cómo se ve el caos cardíaco simulado:\n\nggplot(data, aes(tiempo, RRi)) +  \n  geom_line() +  \n  geom_vline(xintercept = c(6,9), col = \"gray50\", lty = 2, lwd = .5) +  \n  labs(x = \"Tiempo (min)\", y = \"RRi (ms)\", title = \"Caos Cardíaco Simulado\")  \n\n\n\n\n\n\n\n\n\n\nPaso 6: Ajustando el Modelo, Que Stan Haga la Pega Fuerte\nCon la fórmula, priors y datos listos, ajustar el modelo es poco emocionante:\n\nfit &lt;- brm(  \n  formula = model_formula,  \n  data = data,  \n  prior = priors,  \n  family = gaussian(),  \n  chains = 4,         # 4 cadenas MCMC en paralelo  \n  iter = 2000,        # 2000 iteraciones por cadena  \n  warmup = 1000,      # Primeras 1000 son calentamiento  \n  control = list(adapt_delta = 0.95),  # Sé cuidadoso  \n  file = \"../my_rri_model.RDS\"  \n)  \n\nStan procesará los datos usando HMC (vía NUTS) para explorar el posterior. Es como entrenar una red neuronal, pero en vez de fotos de gatos, son curvas cardíacas.\n\n\nPaso 7: Verificando Convergencia, ¿El Robot Mayordomo Estaba Curado?\nTras muestrear, debes verificar si las cadenas (ejecuciones paralelas) están de acuerdo. Diagnósticos clave:\n- R-hat ( \\(\\hat{R}\\) ): Debe ser ≤1.01. Si es 1.5, tus cadenas se pelean como hermanos.\n- Tamaño de Muestra Efectivo (ESS): Debe ser &gt;1000. ¿ESS bajo? Tu mochila jet falló.\nEn R:\n\nsummary(fit)  \n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RRi ~ alpha + beta/(1 + exp(lambda * (time - tau))) + (-c * beta)/(1 + exp(phi * (time - tau - delta))) \n         alpha ~ 1\n         beta ~ 1\n         lambda ~ 1\n         tau ~ 1\n         c ~ 1\n         phi ~ 1\n         delta ~ 1\n   Data: data (Number of observations: 201) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept    800.75      3.45   794.18   807.53 1.00     3052     2914\nbeta_Intercept    -291.16     11.52  -314.42  -269.81 1.00     2077     2512\nlambda_Intercept    -2.88      0.32    -3.54    -2.30 1.00     3034     2480\ntau_Intercept        5.87      0.05     5.77     5.98 1.00     2581     2467\nc_Intercept          0.80      0.01     0.79     0.82 1.00     4280     3403\nphi_Intercept       -2.02      0.26    -2.57    -1.57 1.00     3172     2514\ndelta_Intercept      3.17      0.12     2.94     3.39 1.00     2401     2769\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    29.27      1.48    26.59    32.40 1.00     4401     2760\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBusca Bulk_ESS y Tail_ESS en los resultados. Si están bien, sigue. Si no, llora (o aumenta iter).\n\n\nPor Qué Esto Importa\nEl HMC no es solo alarde académico. Al usar gradientes (derivadas del log-posterior), explora espacios complejos de manera eficiente. Los métodos tradicionales se perderían; el HMC navega como sobre rieles.\nPara el paper, esto permitió capturar:\n\nQué rápido entra en pánico el corazón (\\(\\lambda\\)).\n\nCuánto se recupera (\\(c\\)).\n\nCuándo empieza el pánico (\\(\\tau\\)).\n\nEstos parámetros no son solo números, son biomarcadores. Un \\(c\\) bajo podría indicar mala recuperación en pacientes cardíacos. Un \\(\\lambda\\) alto sugiere respuestas al estrés hiperreactivas.\nImplementar HMC es como enseñarle a un robot a bailar. Es complejo, requiere ajustes y a veces parece magia. Pero con herramientas como Stan y `brms, no necesitas ser físico, solo un humano con datos y un problema que resolver.\nAsí que la próxima vez que tu corazón se acelere en el gimnasio, acuérdate: hay todo un universo de ecuaciones y código trabajando para entender sus pataletas. ¡Y eso es lo bonito!\n\n\n\nVisualizando el la Distribución Posterior\nSeamos honestos: la estadística puede ser más seca que galleta de agua. Pero cuando visualizas las distribuciones posteriores de los parámetros del modelo, ocurre la magia. De repente, números abstractos se transforman en una historia sobre los berrinches, resiliencia y rencores secretos de tu corazón contra los burpees. Aquí te mostramos cómo transformar las matemáticas en insights fisiológicos, con código R y gráficos que hasta tu primo que le huye al cardio entendería.\n\n1. Intervalo R-R basal (\\(\\alpha\\)): El Ritmo de Reposo\nComenzamos con el gráfico de densidad posterior para \\(\\alpha\\), el intervalo R-R en reposo. Usando brms y bayesplot, visualizamos dónde vive este parámetro:\n\nlibrary(bayesplot)  \nlibrary(ggplot2)  \n\n# Extraer muestras posteriores  \nmuestras_posteriores &lt;- as_draws_df(fit)  \n\n# Graficar densidad con intervalo creíble del 90%  \nmcmc_areas(muestras_posteriores, pars = \"b_alpha_Intercept\", prob = 0.9) +  \n  labs(title = \"Intervalo R-R basal (α)\",  \n       x = \"Intervalo R-R (ms)\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\n\n\n\n\n\n\n\n¿Qué Significa?:\n\nUn pico alrededor de 800 ms sugiere una frecuencia cardíaca en reposo de ~75 lpm (ya que RRi ≈ 60,000 / frecuencia).\n\nUn intervalo creíble (IC) del 90% estrecho (ej: 795–805 ms) indica poca incertidumbre.\n\nInsight Fisiológico: Un \\(\\alpha\\) bajo (intervalos cortos) podría indicar estrés crónico o menor condición física. ¿\\(\\alpha\\) alto? Quizás tus participantes son cracks del fitness.\n\n\n\n2. Pánico por Ejercicio (\\(\\beta\\)) y Tasa de Caída (\\(\\lambda\\)): La Fase “Ah, la Cagué”\nGraficamos \\(\\beta\\) (magnitud de la caída) y \\(\\lambda\\) (pendiente de la caída):\n\n# Combinar gráficos  \np1 &lt;- mcmc_areas(muestras_posteriores, pars = \"b_beta_Intercept\", prob = 0.9) +  \n  labs(title = \"Caída Inducida por Ejercicio (β)\",  \n       x = \"Δ Intervalo R-R (ms)\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\np2 &lt;- mcmc_areas(muestras_posteriores, pars = \"b_lambda_Intercept\", prob = 0.9) +  \n  labs(title = \"Tasa de Caída (λ)\",  \n       x = \"Tasa (por min)\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\nggpubr::ggarrange(p1, p2, ncol = 2)  \n\n\n\n\n\n\n\n\n¿Qué Significa?:\n\nUn \\(\\beta\\) centrado en -290 ms significa que los corazones cayeron ~290 ms durante el ejercicio, como pasar de 75 lpm (~800 ms) a 120 lpm (~500 ms).\n\nUn \\(\\lambda\\) de -3 sugiere una caída brusca; tomó ~15 segundos llegar al mínimo. Es como tu corazón gritando “¡¿Esto está pasando?!”.\n\nInsight Fisiológico: Un \\(\\lambda\\) más negativo implica activación simpática rápida (bueno para atletas, preocupante si hay mala recuperación).\n\n\n\n3. Parámetros de Recuperación (\\(c\\), \\(\\phi\\), \\(\\delta\\)): La Redención\nAhora, la fase de recuperación, donde tu corazón perdona (o no) tus decisiones:\n\n# Graficar los tres  \np1 &lt;- mcmc_areas(muestras_posteriores, pars = \"b_c_Intercept\", prob = 0.9) +  \n  labs(title = \"Proporción de Recuperación (c)\",  \n       x = \"Proporción Recuperada\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\np2 &lt;- mcmc_areas(muestras_posteriores, pars = \"b_phi_Intercept\", prob = 0.9) +  \n  labs(title = \"Tasa de Recuperación (φ)\",  \n       x = \"Tasa (por min)\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\np3 &lt;- mcmc_areas(muestras_posteriores, pars = \"b_delta_Intercept\", prob = 0.9) +  \n  labs(title = \"Retraso en Recuperación (δ)\",  \n       x = \"Tiempo de Retraso (min)\", y = \"Densidad\") +  \n  theme(axis.text.y.left = element_blank(),  \n        axis.ticks.y.left = element_blank())  \n\nggpubr::ggarrange(p1, p2, p3, nrow = 3)  \n\n\n\n\n\n\n\n\n¿Qué Significa?:\n\n\\(c = 0.80\\): Los corazones recuperaron 80% de la caída. IC 90% (0.79–0.82) indica alta confianza.\n\n\\(\\phi = -2\\): Recuperación rápida pero no instantánea (~20-25 segundos). Un \\(\\phi\\) más negativo sería reactivación parasimpática más veloz.\n\n\\(\\delta = 3.2\\) min: Los corazones esperaron ~3 minutos post-ejercicio para recuperarse. Retrasos largos podrían indicar fatiga autonómica.\n\nInsight Fisiológico: \\(c\\) bajo, \\(\\phi\\) lento o \\(\\delta\\) largo pueden señalar recuperación deficiente, común en envejecimiento o diabetes.\n\n\n\n4. Simulando Curvas RRi: De Posteriores a Predicciones\nLos posteriores no son solo gráficos bonitos, permiten simular latidos reales. Generemos trayectorias RRi usando muestras:\n\n# Obtener 100 muestras  \nmuestras &lt;- muestras_posteriores[sample.int(100),]  \nnames(muestras) &lt;- names(muestras) |&gt;  \n  gsub(pattern = \"^b_\", replacement = \"\") |&gt;  \n  gsub(pattern = \"_Intercept$\", replacement = \"\")  \n\n# Simular curvas  \nrr_simulado &lt;- purrr::map_dfr(1:100, function(i) {  \n  params &lt;- muestras[i, ]  \n  tibble::tibble(  \n    tiempo = seq(0, 20, by = 0.1),  \n    RRi = params$alpha +  \n          params$beta / (1 + exp(params$lambda * (tiempo - params$tau))) +  \n          (-params$c * params$beta) / (1 + exp(params$phi * (tiempo - params$tau - params$delta))),  \n    sim_id = i  \n  )  \n})  \n\n# Graficar \"espagueti\"  \nggplot(rr_simulado, aes(tiempo, RRi, group = sim_id)) +  \n  geom_line(data = data, aes(group = 1), color = \"gray\") +  \n  geom_line(alpha = 0.1, color = \"purple\") +  \n  labs(title = \"100 Trayectorias RRi Simuladas\",  \n       subtitle = \"Compatibles con datos simulados\",  \n       x = \"Tiempo (min)\", y = \"Intervalo R-R (ms)\")  \n\n\n\n\n\n\n\n\n¿Qué Significa?:\n\nEste “gráfico de espagueti” muestra la incertidumbre en predicciones. Bandas gruesas = alta confianza. Huecos = “No estamos seguros qué pasa aquí”.\n\nLa forma en U es consistente: caída brusca, retraso, luego recuperación. Pero el timing y profundidad varían, reflejando diferencias individuales.\n\nInsight Fisiológico: Mayor dispersión post-ejercicio sugiere corazones con dificultad para recuperarse, candidatos a monitoreo.\n\n\n\n5. Correlaciones entre Parámetros: La Pulseada Autonómica\nFinalmente, exploremos interacciones con un gráfico de correlación por pares:\n\nmcmc_pairs(fit, pars = c(\"b_alpha_Intercept\", \"b_beta_Intercept\",  \n                         \"b_c_Intercept\", \"b_phi_Intercept\"),  \n           diag_fun = \"dens\",  \n           off_diag_fun = \"hex\")  \n\n\n\n\n\n\n\n\n¿Qué Significa?:\n\n\\(\\alpha\\) vs. \\(\\beta\\): ¿Correlación negativa? Corazones relajados (\\(\\alpha\\) alto) podrían entrar más en pánico (\\(\\beta\\) bajo).\n\n\\(c\\) vs. \\(\\phi\\): ¿Correlación positiva? Recuperación rápida (\\(\\phi\\)) suele ir con recuperación completa (\\(c\\)).\n\nInsight Fisiológico: Estas relaciones sugieren acoplamiento autonómico, cómo se coordinan los sistemas simpático y parasimpático.\n\n\n\n¿Por Qué Importa?\nEstas visualizaciones no son solo adornos académicos. Son herramientas para:\n- Medicina Personalizada: Detectar valores extremos en \\(\\alpha\\) o \\(c\\) puede identificar pacientes en riesgo.\n- Optimizar Entrenamiento: Atletas con \\(\\lambda\\) pronunciado podrían soportar mayor intensidad.\n- Investigación en Envejecimiento: La cohorte de adultos mayores del paper tuvo \\(\\phi\\) más lento; comparar con jóvenes puede revelar declive por edad.\nImagina una clínica donde estos gráficos aparezcan durante un test de esfuerzo, guiando intervenciones en tiempo real. O un entrenador ajustando rutinas según el retraso (\\(\\delta\\)) de un corredor. Ese es el poder de casar mates con fisiología.\nLa historia de tu corazón está escrita en curvas logísticas y densidades de probabilidad. Al visualizar los posteriores, no solo procesamos números, sino que desciframos el lenguaje de la vida misma. Y eso, compadre, es mucho más bacán que cualquier tablero de Fitbit.\n\n\n\nDiscusión: El Panorama General\nHablemos claro: el corazón es un caos. No es metrónomo, es un músico de jazz, improvisando, dinámico y a veces más perdido que perro en misa. Por décadas, hemos intentado meter sus ritmos en cajitas etiquetadas “variabilidad cardíaca” o “modelos lineales”, como tratar de embutir un huaso en un ascensor. Este modelo no-lineal no es solo alarde matemático; es una rebelión contra la simplificación bruta. Al abrazar el caos de los intervalos R-R con funciones logísticas, reconocemos que la respuesta cardíaca al ejercicio no es una línea recta, es una montaña rusa. Y eso importa.\nLas implicancias acá son más grandes que la última compra compulsiva en Falabella. Las métricas tradicionales de VFC, aunque útiles, son como resumir Cien años de soledad como “algo pasó en Macondo”. Comprimen toda la historia de tu sistema nervioso en un solo número, perdiendo los giros: cuando el simpático pisa el acelerador en una corrida o el parasimpático hace su lenta vuelta de victoria post-ejercicio. Nuestro modelo captura estos matices, ofreciendo una lente HD a la dinámica cardíaca. Para clínicos, esto podría detectar disfunción autonómica temprana, como diabetes o hipertensión, antes de que escalen. Para atletas, es una hoja de ruta para optimizar recuperación. ¿Para el resto? Prueba que los burpees son, de hecho, invento del diablo.\n\nFoto de Joachim Schnürle en Unsplash\nPero no corramos a abrir champaña. La cohorte del estudio (personas mayores, mayoría mujeres) es como probar una camioneta 4x4 en la Costanera un domingo. Si bien sus corazones son héroes (shoutout a los 70añeros que bailan zumba mejor que millennials), el envejecimiento altera la función autonómica. Corazones jóvenes, con tono vagal fresco y metabolismo eficiente, podrían bailar cueca distinto. Imagina aplicar este modelo a deportistas universitarios: ¿su retraso en recuperación (\\(\\delta\\)) sería menor? ¿Su parámetro de pánico (\\(\\beta\\)) más dramático? No sabemos, y eso es un problema. La ceguera demográfica actual del modelo limita su aplicabilidad. Futuros estudios deben ampliar la red: jóvenes, atletas élite, padres privados de sueño, y sí, ese amigo que corre ultramaratones “para relajarse”. Solo así sabremos si este marco es la multiherramienta de análisis cardíaco o un chiche para abuelitas del bingo.\nLuego está el elefante en la pieza: el ruido del mundo real. El estudio, con datos limpios y controlados, es el equivalente científico de un estudio de TikTok. Pero la vida no es laboratorio. Estrés, café y ese tercero espresso que no debiste tomarte ensucian las señales. Imagina desplegar este modelo al mundo, con relojes inteligentes sufriendo artefactos de movimiento y zonas sin Wi-Fi en el cerro. ¿Aguantará el caos? Las pruebas con datos sintéticos sugieren que sí, pero hasta que no se pruebe en Apple Watches colgando de muñecas sudorosas en CrossFit, seguimos con optimismo cauteloso. Integrar factores ambientales en tiempo real (temperatura, humedad, cortisol) podría transformar este modelo de truco bacán a herramienta clínica pulenta.\nHablando de tecnología, hablemos de wearables. La experiencia Fitbit actual es como tener un copiloto que solo sabe dos frases: “¡Vas bien!” o “¿Y si quizás descansas?”. Nuestro modelo podría mejorarlo a un navegante que entiende los berrinches cardíacos. Imagina tu reloj diciendo: “Tu proporción de recuperación (\\(c\\)) está baja, salta los burpees hoy”, o “Tu \\(\\phi\\) está genial, ¡dale para la press banca!”. No es ciencia ficción; es el próximo paso lógico. Pero para eso, necesitamos integración con APIs de wearables, edge computing para análisis en tiempo real, e interfaces que no parezcan hechas en Excel. Ah, y baterías que no mueran a media rutina. Prioridades, gente.\n\nFoto de Saradasish Pradhan en Unsplash\nAhora, el talón de Aquiles del modelo: interpretabilidad. Las funciones logísticas son elegantes, pero explicárselas a un cardiólogo tomando unos mates es como enseñarle física cuántica a un golden retriever. Parámetros como \\(\\alpha\\), \\(\\beta\\), \\(\\lambda\\) tienen sentido para nosotros los ñoños, pero ¿cómo traducir “tasa de recuperación (\\(\\phi\\)) = -2.6” en consejos útiles? La respuesta: mejores herramientas visuales. Piensa en dashboards interactivos donde médicos ajusten parámetros y vean curvas cardíacas cambiar en tiempo real. O apps que conviertan “mejorar tu \\(c\\)” en una misión tipo Zelda. Cerrando la brecha entre ecuaciones y empatía es el próximo desafío.\nY qué hay de los rincones oscuros del sistema nervioso? El modelo asume una pelea simpático vs parasimpático, pero la biología es más enredada que cable de audífonos. Estudios nuevos sugieren que la “balanza simpatovagal” es más como una relación tóxica: a veces cooperan, a veces se pelean (Storniolo et al. 2021). ¿Puede el modelo capturar eso? Por ahora no. Futuras versiones podrían necesitar ecuaciones diferenciales acopladas o análisis de redes. Además, factores como arritmia sinusal respiratoria (donde la respiración afecta el ritmo) deben controlarse. Incluir esto transformaría el modelo de obra de dos actores a teleserie prime time.\nNo olvidemos la cuerda floja ética. Los datos de salud personalizados son oro para innovación, pero mina antipersonal para privacidad. Si tu reloj sabe los secretos de tu corazón, ¿quién más tiene pase VIP? ¿Isapres? ¿Empleadores? El potencial es innegable, pero sin ética robusta, estamos a un hack de un capítulo de Black Mirror. Transparencia, consentimiento y cifrado no son moda, son el precio de la entrada.\n\nFoto de Maxim Hopman en Unsplash\nEntonces, ¿dónde quedamos? El paper es un avance, pero la maratón recién empieza. El camino tiene preguntas sin responder: (1) ¿Puede predecir eventos cardíacos? (2) ¿Cómo interactúa con otros biomarcadores como presión arterial? (3) ¿Funciona en patologías como fibrilación auricular? Cada paso requiere colaboración: matemáticos de la Chile, médicos de la Católica, ingenieros de la Santa María, y sí, hasta éticos para que no nos pasemos de listos.\nAl final, este trabajo no es de ecuaciones o R2. Es sobre honrar la complejidad cardíaca. Tu corazón no es máquina; es un contador de historias. Cada latido susurra cuentos de estrés, alegría, cansancio y resiliencia. Al escuchar, de verdad escuchar, su narrativa no-lineal, no hacemos ciencia. Aprendemos un nuevo idioma, uno que podría reescribir el futuro de la salud. Y eso, compadre, es un giro argumental que vale la pena seguir.\n\n\n\n\nApéndice\n\nRevisa el paper original donde presentamos el modelo aquí.\n\n\n\nReferences\n\nStorniolo, Jorge L, Beatrice Cairo, Alberto Porta, and Paolo Cavallari. 2021. “Symbolic Analysis of the Heart Rate Variability During the Plateau Phase Following Maximal Sprint Exercise.” Frontiers in Physiology 12: 632883.\n\nCitationBibTeX citation:@misc{castillo-aguilar2025,\n  author = {Castillo-Aguilar, Matías},\n  title = {¿Crees Saber Cómo Modelar El Ritmo Cardíaco?},\n  date = {2025-03-24},\n  url = {https://bayesically-speaking.com/posts/2025-03-24 so-you-think-you-can-model-a-heartbeat/spanish/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2025. “¿Crees Saber Cómo Modelar El\nRitmo Cardíaco?” March 24, 2025. https://bayesically-speaking.com/posts/2025-03-24\nso-you-think-you-can-model-a-heartbeat/spanish/."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html",
    "href": "posts/2023-05-30 welcome/index.html",
    "title": "Welcome to Bayesically Speaking",
    "section": "",
    "text": "Photo from Jon Tyson at Unsplash.\n\n\n\nHello stranger\nAlright folks, welcome to ‘Bayesically Speaking’! Yes, I know, I’m a word nerd. Sue me. But seriously, I’ve been wanting to share my love for all things Bayesian for ages. It’s like, the internet is down right now, so I figured, ‘What the heck, let’s just do it!’ You know how everyone tells you to ‘follow your dreams’? Well, this is my dream: to spread the gospel of Bayes to the masses (or at least to anyone who’s still reading).\nFor years, I’ve been obsessed with stats, especially how they can help us unravel the mysteries of health. It’s like, you have these puzzle pieces, and you’re trying to figure out how they fit together. But sometimes, you look at one piece in isolation, and it seems pretty boring. But then you put two pieces together, and BOOM! Suddenly, you see a hidden pattern, a secret message! That’s the magic of statistics for me.\n\n\nCode\nsim_data &lt;- simulate_simpson(n = 100, \n                             difference = 2, \n                             groups = 4, \n                             r = .7) |&gt; \n  as.data.table()\n\nsim_data[, Group := factor(Group, \n                           levels = c(\"G_1\",\"G_2\",\"G_3\",\"G_4\"),\n                           labels = c(\"Placebo\", \"Low dose\", \"Medium dose\", \"High dose\"))]\n\nggplot(sim_data, aes(V1, V2, col = Group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  geom_smooth(method = \"lm\", aes(group = 1, col = NULL)) +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  labs(x = \"Time exposure\", y = expression(Delta*\"TNF-\"*alpha)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nIn this example, the whole sample correlation is about -0.71 (meaning a negative relationship), but the within group correlation is 0.7 (the exact opposite effect). This phenomenom is better known as Simpson’s Paradox.\n\n\n\n\n\nThe statistics toolbox\nNow, let’s talk about the classic stats toolbox. You know, the t-tests, ANOVAs, and regressions (the old reliables). They’re like those trusty old hammers, simple, easy to use, and they get the job done most of the time. But let’s be honest, they’re not exactly Swiss Army knives. They start to struggle when things get messy. Asymmetrical data? Forget about it. Non-linear relationships? Good luck! And don’t even get me started on unbalanced groups or those pesky outliers.\nEnter the non-parametric heroes! These guys are more flexible, like those fancy adjustable wrenches. But they can be a bit… mysterious. It’s hard to predict how they’ll behave in new situations, which can be a bit nerve-wracking.\nThen there are the ‘black box’ models (neural networks, random forests, and their cousins). These are like those super-powered AI robots. They can handle any data you throw at them, no matter how messy. But sometimes, you have no idea what’s going on inside. It’s like magic, but not in a good way. You just hope it doesn’t turn against you.\n\n\n\nJust between us, I only put this picture because it looked cool. Photo from Dan Cristian Pădureț at Unsplash.\n\n\nSo, we have these fancy new tools, but something’s missing. We need to bring our own brains into the equation! After all, we don’t just blindly stare at data. We bring our own experiences, our own biases (hey, we’re human!), and our own gut feelings to the table.\nThat’s where Bayes comes in, the master of incorporating ‘prior knowledge.’ It’s like saying, ‘Hey, I know a little something about this already, so let’s not start from scratch.’ And instead of just giving us a dry ‘yes’ or ‘no’ answer, Bayes gives us a whole range of possibilities, complete with confidence levels. It’s like, ‘I’m 90% sure this is true, but there’s still a 10% chance I’m completely wrong.’ Much more honest, wouldn’t you say?\nAlright, let’s play a little mind game. Imagine you’re convinced this coin is rigged. You’ve flipped it 15 times and seen heads a whopping 10 times! You’re thinking, “This thing is clearly biased towards heads.” You calculate your odds: 10 heads out of 15 flips, that’s a 66% chance of heads! You feel pretty confident, right?\nNow, for the twist. You decide to put this coin to the test yourself. You flip it 15 times, and guess what? You get 13 tails! Only 2 measly heads. Talk about a reality check! Suddenly, your confidence is shaken. Based on your own experiment, the odds of getting heads plummet to a measly 13%.\nSo, what do you do? Throw your hands up in the air and declare your initial belief wrong? Not so fast! We need to find a way to reconcile these conflicting results. This is where Bayesian thinking comes to the rescue.\n\n\nLet’s Get Bayesian\nTo figure out the true odds of getting heads, we need to use a bit of Bayesian magic. We’ll call the probability of getting heads “P(H)”.\nNow, remember our initial belief? The one based on those first 15 flips? We can represent that belief using something called a Beta distribution.\n\\[\nP(H) \\sim Beta(10, 5)\n\\] This symbol “\\(\\sim\\)” means distributed as\nHere, the Beta distribution parameters are (10, 5) since we had 10 heads and 5 tails in the prior experiment.\nNow, a new experiment with the same 15 tosses gives us 2 heads. To update our prior belief, we can use this information to calculate the posterior probability which can be expressed as follow:\n\n\nThis symbol “\\(\\propto\\)” means proportional to\n\\[\nP(H | Data) \\propto P(Data | H) \\times P(H)\n\\]\nWhich is equivalent as saying:\n\\[\nPosterior \\propto Likelihood \\times Prior\n\\]\nTo find our updated belief about the probability of heads (the “posterior probability”), we need to combine our prior belief with the new evidence. This involves some fancy math – we multiply the likelihood (the probability of getting our observed results) by our prior belief.\nNow, there’s a little trick here. Since our prior belief follows a Beta distribution, and our data fits a binomial distribution, the math works out beautifully. The posterior distribution will also be a Beta distribution! This makes our lives much easier.\n\n\n\n\n\n\nA Quick Note on Normalization\n\n\n\n\n\nWhen we multiply our prior and likelihood, we get a distribution that looks like the final answer, but it’s not quite right. It’s like having a delicious cake batter, but forgetting to bake it! It needs to be “normalized” to make sure it adds up to 1. Usually, this involves some complex math, but luckily, we don’t need to worry about that in this case.\n\n\n\n\\[\nP(H | Data) \\sim Beta(10 + 2, 5 + 13)\n\\]\nAfter incorporating the data from the new experiment, the parameters of the Beta distribution become (12, 18) since we had 2 heads and 13 tails in the new experiment, meaning 12 heads and 18 tails in total.\n\n\n\n\n\n\nAbout conjugacy\n\n\n\n\n\nThis is where things get really interesting. Remember how I mentioned that the posterior distribution is also a Beta distribution? That’s not a coincidence! It’s a special property called “conjugacy”.\nThink of it like this: Imagine you have a set of building blocks. Your prior belief is one type of block, and the new data you collect is another type. When you combine them, you get a block of the exact same type!\nThis conjugacy property is a huge time-saver. Instead of doing a bunch of complicated calculations, we can use a much more simple formula to find our updated belief. It’s like having a shortcut through a maze, much faster and easier!\n\n\n\nTo calculate the posterior probability of getting heads, we can consider the mode (maximum) of the Beta distribution, which is \\((a - 1) / (a + b - 2)\\):\n\\[\n\\begin{aligned}\nP(H | Data) &= (12 - 1) / (12 + 18 - 2) \\\\\n            &= 11 / 28 \\\\\n            &\\approx 0.39\n\\end{aligned}\n\\]\nTherefore, the posterior probability of getting heads is approximately 39% when we consider all the available evidence.\n\n\nCode\n# Prior and Likelihood functions\ndata = function(x, to_log = FALSE) dbeta(x, 2, 13, log = to_log)\nprior = function(x, to_log = FALSE) dbeta(x, 10, 5, log = to_log)\n\n# Posterior\nposterior = function(x) {\n  p_fun = function(i) {\n    # Operation is on log-scale merely for computing performance\n    # and minimize rounding errors giving the small nature of\n    # probability density values at each interval.\n    i_log = data(i, to_log = TRUE) + prior(i, to_log = TRUE)\n    # Then transformed back to get probabilities again\n    exp(i_log)\n  }\n  \n  # Then we integrate using base function `integrate`\n  const = integrate(f = p_fun, \n                    lower = 0L,  upper = 1L, \n                    subdivisions = 1e4L,\n                    rel.tol = .Machine$double.eps)$value\n  p_fun(x) / const\n}\n\n## Plotting phase\n\n### Color palette\ncol_pal &lt;- c(Prior = \"#DEEBF7\", \n             Data = \"#3182BD\", \n             Posterior = \"#9ECAE1\")\n### Main plotting code\nggplot() +\n  #### Main probability density functions\n  stat_function(aes(fill = \"Data\"), fun = data, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Prior\"), fun = prior, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Posterior\"), fun = posterior, geom = \"density\", alpha = 1/2) +\n  #### Minor aesthetics tweaks\n  labs(fill = \"\", y = \"Density\", x = \"Probability of getting heads\") +\n  scale_fill_manual(values = col_pal, aesthetics = \"fill\") +\n  scale_x_continuous(labels = scales::label_percent(), \n                     limits = c(0,1)) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 6.5)) +\n  theme(legend.position = \"top\",\n        legend.spacing.x = unit(3, \"mm\")) +\n  #### Arrows\n  geom_curve(aes(x = .81, y = 4.1, xend = .69232, yend = 3.425), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .9, y = 4.1, label = \"Beta(10,5)\")) +\n  geom_curve(aes(x = .2, y = 5.9, xend = .07693, yend = 5.45), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .29, y = 5.85, label = \"Beta(2,13)\")) +\n  geom_curve(aes(x = .5, y = 5, xend = .3847, yend = 4.4), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .55, y = 5, label = \"≈ 39%\"))\n\n\n\n\n\nGraphical representation of the posterior probability as the combination of both the data and the prior evidence\n\n\n\n\n\n\n\nPractical implications\nThis example is a prime example of how Bayes works its magic. Imagine your beliefs as a stubborn old dog, set in its ways. Then comes along some shiny new data, like a juicy steak. Bayes, the wise dog whisperer, gently nudges the old dog towards the steak, showing it that the world is bigger than its initial assumptions. Pretty cool, huh?\nAnd get this, Bayes isn’t just some fancy math trick. It’s basically how our brains are wired! We’re all walking, talking Bayesian machines, constantly updating our internal models based on new experiences. Remember that time you thought your friend was a total grump? But then they surprised you with a hilarious joke? BAM! Your brain, the Bayesian overlord, adjusted your opinion.\nAs that fancypants statistician, Gelman and Shalizi (2013), once said (probably while sipping a fine whiskey), “Bayesians think they’ve discovered the secret to life, the universe, and everything”. Okay, maybe that’s a slight exaggeration, but you get the idea. Actually, he said “A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics”, but hey! It’s close enough.\nBut let’s not get carried away. These powerful tools are like those fancy kitchen gadgets you see on TV infomercials. They promise to revolutionize your life, but sometimes they just end up collecting dust in the cupboard. We need to know when to use them and when to stick to simpler methods. Otherwise, we might end up overcomplicating things and driving ourselves crazy\n\n\n\nPhoto from NASA at Unsplash.\n\n\n\n\nFrom past to future\nRemember the Dark Ages? I mean, not THAT Dark Ages, but the time before Bayesian stats were cool? Back then, those poor Bayesians were stuck with slide rules and abacuses, trying to wrangle those complex models. Talk about a statistical struggle! But fear not, my friends, for the computer gods have smiled upon us! Now, we mere mortals can unleash the power of Bayes with a few clicks. It’s like having a magic wand that whispers the secrets of the universe.\nWe, the self-proclaimed ‘Statisticians-in-Training’ (or maybe ‘Statisticians-in-Training-While-Sipping-Coffee’), are on a mission to spread the good word. Imagine, folks, the possibilities! We can finally tell our friends, ‘Oh, you think correlation equals causation? Please, let me introduce you to my Bayesian overlord!’ We can predict the next winning lottery number (well, maybe not, but we can at least pretend). And we can finally understand why our cat keeps knocking over that vase, it’s not just mischievous, it’s statistically significant!\nSo, grab your favorite caffeinated beverage and let’s dive headfirst into this statistical wonderland. We might not conquer the world (yet), but we’ll definitely have a lot of fun trying. And hey, who knows, maybe we’ll even discover the secret to a perfectly brewed cup of coffee using Bayesian inference. Now that’s a worthy pursuit, wouldn’t you say?\n\n\n\n\n\nReferences\n\nGelman, Andrew, and Cosma Rohilla Shalizi. 2013. “Philosophy and the Practice of Bayesian Statistics.” British Journal of Mathematical and Statistical Psychology 66 (1): 8–38.\n\nCitationBibTeX citation:@misc{castillo-aguilar2023,\n  author = {Castillo-Aguilar, Matías},\n  title = {Welcome to {Bayesically} {Speaking}},\n  date = {2023-06-10},\n  url = {https://bayesically-speaking.com/posts/2023-05-30 welcome/},\n  doi = {10.59350/35tc8-qyj10},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2023. “Welcome to Bayesically\nSpeaking.” June 10, 2023. https://doi.org/10.59350/35tc8-qyj10."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html",
    "href": "posts/2024-04-14 mcmc part 1/index.html",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "Introduction\nAlright, folks, let’s dive into the wild world of statistics and data science! Picture this: you’re knee-deep in data, trying to make sense of the chaos. But here’s the kicker, sometimes the chaos is just too darn complex. With tons of variables flying around, getting a grip on uncertainty can feel like trying to catch smoke with your bare hands.\nPlease, have in your consideration that the kind of problems that we’re dealing with, it’s not solely related to the number of dimensions, it’s mostly related to trying to estimate something that we can’t see in full beforehand. For instance, consider the following banana distribution (shown below). How could we map this simple two dimensional surface without computing it all at once?\n\n\nCode\ndbanana &lt;- function(x) {\n  a = 2;\n  b = 0.2;\n  \n  y = x / a\n  y = (a * b) * (x^2 + a^2)\n}\n\nx &lt;- seq(-6, 6, length.out = 300)\n\ny = dbanana(x)\n\nz &lt;- MASS::kde2d(x, y, n = 100, lims = c(-10, 10, -2.6, 20))\n\nplot_ly(x = z$x, y = z$y, z = sqrt(z$z)) |&gt; \n  add_surface() |&gt; \n  style(hoverinfo = \"none\")\n\n\n\n\n\n\n\nJust put a darn grid to it\nYou know when you hit a roadblock in your calculations, and you’re like, “Can’t we just crunch the numbers for every single value?” Well, let’s break it down. Picture a grid with \\(N\\) points for \\(D\\) dimensions. Now, brace yourself, ’cause the math needed is like \\(N\\) raised to the power of \\(D\\).\nSo, let’s say you wanna estimate 100 points (to get a decent estimation of the shape) for each of 100 dimensions. That’s like slamming your head against ten to the power of 200 computations… that’s a hell of a lot of computations!\nSure, in la-la land, you could approximate every single number with some degree of approximation. But let’s get real here, even if you had all the time in the world, you’d still be chipping away at those calculations until the sun swallowed the Earth, especially with continuous cases and tons of dimensions that are somewhat correlated (which in reality, tends to be the case).\nThis headache we’re dealing with? It’s what we “affectionately” call (emphasis on double quotes) the curse of dimensionality. It’s like trying to squeeze a square peg into a round hole… it ain’t gonna happen without a supersized hammer!\n\n\nCode\ncurse_dimensionality &lt;- data.frame(dimensions = factor((1:10)^2),\n                                   calculations = 100^((1:10)^2))\n\nggplot(curse_dimensionality, aes(dimensions, calculations)) +\n  geom_col(fill = ggsci::pal_jama()(1)) +\n  scale_y_continuous(transform = \"log10\", n.breaks = 9,\n                     labels = scales::label_log(), expand = c(0,0,.1,0)) +\n  labs(y = \"Computations (log-scale)\", x = \"Dimensions (Variables)\",\n       title = \"Computations needed to compute a grid of 100 points\",\n       subtitle = \"As a function of dimensions/variables involved\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\n\nIllustration of computations needed (in log-scale) for 100 points as a function of dimensions considered.\n\n\n\n\n\n\n\n\n\n\nExplaining the curse of dimensionality further\n\n\n\n\n\nImagine you’re trying to create a grid to map out the probability space for a set of variables. As the number of dimensions increases, the number of grid points needed to adequately represent the space explodes exponentially. This means that even with the most powerful computers, it becomes practically impossible to compute all the probabilities accurately.\n\n\n\n\n\n\nSampling the unknown: Markov Chain Monte Carlo\nNow, if we can’t crack the problem analytically (which, let’s face it, is the case most of the time), we gotta get creative. Lucky for us, there’s a bunch of algorithms that can lend a hand by sampling this high-dimensional parameter space. Enter the Markov Chain Monte Carlo (MCMC) family of algorithms.\nBut hold up… Markov Chain Monte What? Yeah, it’s a mouthful, but bear with me. You’re probably wondering how this fancy-schmancy term is connected to exploring high-dimensional probability spaces. Well, I’ll let you in on the secret sauce behind these concepts and why they’re the go-to tools in top-notch probabilistic software like Stan.\nBut before we get into the nitty-gritty of MCMC, let’s take a detour and talk about Markov Chains, because they’re like the OGs of this whole MCMC gang.\n\nUnderstanding Markov Chains: A rainy example\nConsider the following scenario: if today is rainy, the probability that tomorrow will be rainy again is 60%, but if today is sunny, the probability that tomorrow will be rainy is only 30%. However, the probability of tomorrow being sunny is 40% if today is raining, but 70% if today is sunny as well.\n\n\n\n\n\n\nflowchart LR\n  a((Rainy)) ---&gt;| 40% | b((Sunny))\n  a --&gt;| 60% | a\n  b --&gt;| 70% | b\n  b ---&gt;| 30% | a\n\n\n\n\n\n\n\nAs you can see, the probability of a future step depends on the current step. This logic is central to Bayesian inference, as it allows us to talk about the conditional probability of a future value based on a previous one, like sampling across a continuous variable.\n\n\nConverging to an answer\nNow, let’s imagine letting time run. After a year passes, if we observe how the weather behaves, we’ll notice that the relative frequencies of each state tend to converge to a single number.\nNow, fast forward a year. If we keep an eye on the weather every day, we’ll notice something interesting: the relative frequencies of rainy and sunny days start to settle into a rhythm. This steady state is what we call a stationary distribution. It’s like the true probability of what the weather’s gonna be like in the long run, taking into account all the different scenarios.\n\n\nCode\nsimulate_weather &lt;- function(total_time) {\n  \n  weather &lt;- vector(\"character\", total_time) # Create slots for each day\n  day &lt;- 1 # First day\n  weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1) # Weather for first day\n  \n  while (day &lt; total_time) {\n    day &lt;- day + 1 # Add one more day\n    if (weather[day] == \"Rainy\") {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.6, .4))\n    } else {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.3, .7))\n    }\n  }\n  \n  return(weather)\n}\n\nsim_time &lt;- 365*1\nweather &lt;- simulate_weather(total_time = sim_time)\n\nweather_data &lt;- data.frame(\n  prop = c(cumsum(weather == \"Rainy\") / seq_len(sim_time), cumsum(weather == \"Sunny\") / seq_len(sim_time)),\n  time = c(seq_len(sim_time), seq_len(sim_time)),\n  weather = c(rep(\"Rainy\", times = sim_time), rep(\"Sunny\", times = sim_time))\n)\n\nggplot(weather_data, aes(time, prop, fill = weather)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::label_percent(), n.breaks = 6,\n                     name = \"Proportion of each weather\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Days\", n.breaks = 10, expand = c(0,0)) +\n  scale_fill_brewer(type = \"qual\", palette = 3) +\n  labs(fill = \"Weather\", title = \"Convergence to stationary distribution\",\n       subtitle = \"Based on cumulative proportion of each Sunny or Rainy days\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean proportion of sunny/rainy days across 365 days. Right pass the 100 days, the proportion of rainy/sunny days tends to display a stable trend when we averaged the previous days. This is known as stationary distribution.\n\n\n\n\nThis heuristic allows us to naturally converge to an answer without needing to solve it analytically, which tends to be useful for really complex and high-dimensional problems.\nSure, we could’ve crunched the numbers ourselves to figure out these probabilities. But why bother with all that math when we can let time do its thing and naturally converge to the same answer? Especially when we’re dealing with complex problems that could have given even Einstein himself a headache.\n\n\n\n\n\n\nExplaining the convergence process further\n\n\n\n\n\nThe idea of convergence to a stationary distribution can be likened to taking a random walk through the space of possible outcomes. Over time, the relative frequencies of each outcome stabilize, giving us a reliable estimate of the true probabilities.\n\n\n\nAs we’ve seen, sometimes it becomes impractical to solve analytically or even approximate the posterior distribution using a grid, given the number of calculations needed to even get a decent approximation of the posterior.\nHowever, we’ve also seen that Markov Chains might offer us a way to compute complex conditional probabilities and, if we let them run long enough, they will eventually converge to the stationary distribution, which could resemble the posterior distribution itself. So, all things considered, when does the Monte Carlo part come in?\n\n\n\nThe Need for Monte Carlo Methods\nAlright, let’s break down the magic of Monte Carlo methods in plain English. Picture this: in the wacky world of random events, being able to sample from a distribution is like having a crystal ball to predict the future, pretty nifty, right?\nNow, imagine we’re sampling from a normal probability density, say, with a mean of 80 and a standard deviation of 5. We grab a random sample of 10 folks, calculate their average weight, and repeat this process a thousand times.\nIn the following figure, we overlay the calculated sample mean, from each simulated sample using a histogram, to the population distribution from which we are sampling. As you can see, this sets an interesting opportunity, using this Monte Carlo simulation, we can get an intuition of how likely is, to our sample of 10 individuals, have a mean outside the range of 75 to 85, it’s not impossible, but it’s unlikely.\n\n\nCode\nmean_weights &lt;- matrix(data = rnorm(10 * 1000, 80, 5), nrow = 10, ncol = 1000) |&gt; \n  colMeans()\n\ncols &lt;- ggsci::pal_jama()(2)\n\nggplot() +\n  stat_function(fun = ~dnorm(.x, 80, 5), xlim = c(60, 100), geom = \"area\", fill = cols[2]) +\n  geom_histogram(aes(x = mean_weights, y = after_stat(density)/3.75),\n               fill = cols[1], col = cols[2], binwidth = .4) +\n  scale_y_continuous(expand = c(0,0), name = \"Density\", labels = NULL, breaks = NULL) +\n  scale_x_continuous(expand = c(0,0), name = \"Weight (kg)\", n.breaks = 10) +\n  geom_curve(data = data.frame(\n    x = c(70), xend = c(74.5), y = c(0.061), yend = c(0.05)\n    ), aes(x = x, xend = xend, y = y, yend = yend),\n    curvature = -.2, arrow = arrow(length = unit(0.1, \"inches\"), type = \"closed\")) +\n  geom_curve(data = data.frame(\n    x = c(90), xend = c(82), y = c(0.061), yend = c(0.05)\n    ), aes(x = x, xend = xend, y = y, yend = yend),\n    curvature = .2, arrow = arrow(length = unit(0.1, \"inches\"), type = \"closed\")) +\n  geom_text(aes(x = c(67, 94), y = c(0.0605), label = c(\"Population\\ndistribution\", \"Means of each\\nsimulated sample\")), size = 6) +\n  theme_classic(base_size = 20)\n\n\n\n\n\nSampling distribution of 10 individuals per sample overlayed to the population distribution. Each sample is drawn from a normal distribution of mean of 80 and standard deviation of 5, representing the population distribution of weight.\n\n\n\n\nWith each sample, we’re capturing the randomness of the population’s weight distribution. And hey, it’s not just about weight; we can simulate all sorts of wild scenarios, from multi-variable mayhem to linear model lunacy. This is the heart and soul of Monte Carlo methods: taking random shots in the dark to mimic complex processes.\nBut here’s the kicker: the more samples we take, the clearer the picture becomes. For instance, if we take ten times the amount of samples used, we would get a better intuition about the uncertainty around the expectation for each sample of 10 individuals, which could have important applications in the design of experiments and hypothesis testing.\nAnd that’s where Monte Carlo methods shine. By generating a boatload of samples, we can unravel the mysteries of even the trickiest distributions, no crystal ball required. It’s a game changer for exploring the unknown without needing a PhD in rocket science.\n\n\n\n\n\n\nExplaining the importance of Monte Carlo further\n\n\n\n\n\nMonte Carlo methods provide a powerful tool for approximating complex distributions by sampling from them. By generating a large number of samples, we can gain insight into the shape and properties of the distribution without needing to explicitly calculate all possible outcomes.\n\n\n\n\n\n⁠Basics of MCMC\nAlright, let’s break down the basics of MCMC. Picture this: you’ve got these two heavyweights in the world of statistics, Markov Chains and Monte Carlo methods.\nOn one side, you’ve got Markov Chains. These bad boys help us predict the probability of something happening based on what happened before. It’s like saying, “Hey, if it rained yesterday, what’s the chance it’ll rain again today?”\nThen, there are Monte Carlo methods. These puppies work by randomly sampling from a distribution to get an idea of what the whole shebang looks like. It’s like throwing a bunch of darts at a dartboard in the dark and hoping you hit the bullseye.\nHowever the question remains, how do they team up to tackle real-world problems?\n\nWhat is MCMC actually doing?\nIn essence, MCMC is an algorithm that generates random samples from a proposal distribution. These samples are accepted or rejected based on how much more likely the proposed sample is compared to the previous accepted sample.\nIn this way, the proposed samples are accepted in the same proportion as the actual probability in the target distribution, accepting more samples that are more likely and fewer samples that are less likely.\nThe fascinating nature of this heuristic is that it works to approximate complex distributions without needing to know much about the shape of the final distribution.\nSo, think of it as trekking through this complex landscape, taking random steps (the Monte Carlo part) but guided by the likelihood of each move, given where you currently stand (the Markov Chain part). It’s a meticulous journey, but one that ultimately leads us to a better understanding of these elusive distributions.\nFor instance, consider that we have a distribution (shown below) that we can’t to compute, because it would take too long to integrate the whole function. This will be our target distribution, from which we can only compute the density of one value at a time.\n\n\n\n\n\n\nAbout the target distribution\n\n\n\n\n\nIn practice, we would derive the target distribution from the data and prior information, this enable us to estimate the density in a point-wise manner, without the need to estimate the whole PDF all at once. But for the sake of demonstration we will the use the Gamma probability density function.\nHowever, please consider that you can’t use some family distribution to describe perfectly any probability density, sometimes it can be a mixture of distributions, truncation, censoring. All comes down to the underlying process that generates the data that we are trying to mimic.\n\n\n\n\n\nCode\n# Target distribution that we in practice would derive from\n# the data.\ntarget_dist &lt;- function(i) dgamma(i, shape = 2, scale = 1)\n\nggplot() +\n  stat_function(fun = target_dist,\n                xlim = c(0, 11), geom = \"area\", \n                fill = \"#374E55FF\") +\n  scale_y_continuous(breaks = NULL, name = \"Density\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Some scale\", expand = c(0,0)) +\n  theme_classic(base_size = 20)\n\n\n\n\n\nThis is a Gamma distribution with shape of 2 and scale of 1. We will try to estimate it.\n\n\n\n\nNext thing to do is to specify a proposal distribution, from which we’ll generate proposals for the next step. To this end we’ll be using a Normal density function with \\(\\mu\\) = 0 and \\(\\sigma\\) = 1.\n\n# This is a function that will generate proposals for the next step.\nproprosal &lt;- function() rnorm(1, mean = 0, sd = 1)\n\nAnd set some algorithm parameters that are necessary for our MCMC to run:\n\n## Algorithm parameters ----\n\ntotal_steps &lt;- 1000 # Total number of steps\nstep &lt;- 1 # We start at step 1\nvalue &lt;- 10 # set a initial starting value\n\nFinally, we run our algorithm as explained in previous sections. Try to follow the code to get an intuition of what is doing.\n\n## Algorithm ----\n\nset.seed(1234) # Seed for reproducibility\nwhile(step &lt; total_steps) {\n  # Increase for next step\n  step &lt;- step + 1\n  \n  ## 1. Propose a new value ----\n  \n  # Proposal of the next step is ...\n  value[step] &lt;- \n    # the previous step plus...\n    value[step - 1L] + \n    # a change in a random direction (based on the \n    # proposal distribution)\n    proprosal() \n  \n  ## 2. We see if the new value is more or less likely ----\n  \n  # How likely (in the target distribution)\n  likelihood &lt;- \n    # is the proposed value compared to the previous step\n    target_dist(value[step]) / target_dist(value[step - 1L]) \n  \n  ## 3. Based on its likelihood, we accept or reject it ----\n  \n  # If the proposal value is less likely, we accept it only \n  # to the likelihood of the proposed value\n  if (likelihood &lt; runif(1)) \n    value[step] &lt;- value[step - 1L]\n  \n  # Then we repeat for the next step\n}\n\nFinally, let’s explore how well our algorithm converge to the target distribution.\n\n\nCode\nmcmc &lt;- data.frame(\n  step = seq_len(step),\n  value = value\n)\n\nggplot(mcmc, aes(x = step, y = value)) +\n  geom_line(col = \"#374E55FF\") +\n  ggside::geom_ysidehistogram(aes(x = -after_stat(count)), fill = \"#374E55FF\", binwidth = .3) +\n  ggside::geom_ysidedensity(aes(x = -after_stat(count)*.35), col = \"#374E55FF\") +\n  ggside::scale_ysidex_continuous(expand = c(0,0,0,.1), breaks = NULL) +\n  scale_x_continuous(expand = c(0,0), name = \"Step\") +\n  scale_y_continuous(name = NULL, position = \"right\") +\n  labs(title = \"Trace of MCMC values to target distribution\",\n       subtitle = \"Evolution of values at each step\") +\n  theme_classic(base_size = 20) +\n  ggside::ggside(y.pos = \"left\") +\n  theme(ggside.panel.scale = .4)\n\n\n\n\n\nTraceplot of convergence of MCMC for 1000 steps. With increasing steps we see an increasing resemblance to the target distribution.\n\n\n\n\nAnother thing that we care is to see how well our MCMC is performing. After all, if not, then what would be the point of using it in first place? To check this, we’ll compare the expectation (\\(E(X)\\)) of the target distribution against the posterior derived from our MCMC.\nFor this, we have to consider that the expectation, \\(E(X)\\), of any Gamma distribution is equal to the shape parameter (\\(\\alpha\\)) times by the scale parameter (\\(\\sigma\\)). We could express the aforementioned the following.\n\\[\n\\begin{aligned}\n  E(X) &= \\alpha \\sigma \\\\\n  \\text{with}~X &\\sim \\text{Gamma}(\\alpha, \\sigma)\n\\end{aligned}\n\\]\n\n\nCode\nggplot(mcmc, aes(x = step, y = cumsum(value)/step)) +\n  geom_line(col = \"#374E55FF\") +\n  scale_x_continuous(expand = c(0,.1), name = \"Steps (log-scale)\", \n                     transform = \"log10\", labels = scales::label_log()) +\n  scale_y_continuous(name = NULL, expand = c(0, 1)) +\n  labs(title = \"Convergence to location parameter\",\n       subtitle = \"Cumulative mean across steps\") +\n  geom_hline(aes(yintercept = 2), col = \"darkred\") +\n  geom_hline(aes(yintercept = mean(value)), lty = 2) +\n  annotate(x = 1.5, xend = 1.1, y = 7.5, yend = 9.5, geom = \"curve\", curvature = -.2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = 2, y = 6.8,  label = \"Initial value\", size = 5, geom = \"text\") +\n  annotate(x = (10^2.5), xend = (10^2.6), y = 5, yend = 2.5, geom = \"curve\", curvature = .2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = (10^2.5), y = 5.8,  label = \"Convergence\", size = 5, geom = \"text\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean of the posterior distributions across steps, compared to the empirical mean of the target distribution. Here the dark red line represents the empirical location parameter and the dashed line the one estimated using MCMC.\n\n\n\n\n\n\n\n⁠Popular MCMC Algorithms\nThis general process is central to MCMC, but more specifically to the Metropolis-Hastings algorithm. However, and in order to broaden our understanding, let’s explore additional MCMC algorithms beyond the basic Metropolis-Hastings with some simple examples.\n\nGibbs Sampling: A Buffet Adventure\nImagine you’re at a buffet with stations offering various cuisines (Italian, Chinese, Mexican, you name it). You’re on a mission to create a plate with a bit of everything, but here’s the catch: you can only visit one station at a time. Here’s how you tackle it:\n\nHit up a station and randomly pick a dish.\nMove on to the next station and repeat the process.\nKeep going until you’ve got a plateful of diverse flavors.\n\nGibbs sampling works kind of like this buffet adventure. You take turns sampling from conditional distributions, just like you visit each station for a dish. Each time, you focus on one variable, updating its value while keeping the others constant. It’s like building your plate by sampling from each cuisine until you’ve got the perfect mix.\n\n\nHamiltonian Monte Carlo: Charting Your Hiking Path\nPicture yourself hiking up a rugged mountain with rocky trails and valleys. Your goal? Reach the summit without breaking a sweat (or falling off a cliff). So, you whip out your map and binoculars to plan your route:\n\nStudy the map to plot a path with minimal uphill battles and maximum flat stretches.\nUse the binoculars to scout ahead and avoid obstacles along the way.\nAdjust your route as you go, smoothly navigating the terrain like a seasoned pro.\n\nHamiltonian Monte Carlo (HMC) is a bit like this hiking adventure. It simulates a particle moving through a high-dimensional space, using gradient info to find the smoothest path. Instead of blindly wandering, HMC leverages the curvature of the target distribution to explore efficiently. It’s like hiking with a GPS that guides you around the rough spots and straight to the summit.\n\n\nStrengths, Weaknesses, and Real-World Applications\nNow that you’ve dipped your toes into the MCMC pool, it’s time to talk turkey (well, sampling). Each MCMC method has its perks and quirks, and knowing them is half the battle.\nGibbs sampling is the laid-back surfer dude of the group, simple, chill, and great for models with structured dependencies. But throw in some highly correlated variables, and it starts to wobble like a rookie on a surfboard.\nMeanwhile, HMC is the sleek Ferrari, efficient, powerful, and perfect for tackling complex models head-on. Just don’t forget to fine-tune those parameters, or you might end up spinning out on a sharp curve.\n\n\nKey Differences\n\nSampling Approach\n\nMetropolis-Hastings: Takes random walks to generate samples, with acceptance based on a ratio of target distribution probabilities.\nGibbs Sampling: Updates variables one by one based on conditional distributions, like a tag team wrestling match.\nHamiltonian Monte Carlo: Glides through high-dimensional space using deterministic trajectories guided by Hamiltonian dynamics, like a graceful dancer in a crowded room.\n\n\n\nEfficiency and Exploration\n\nMetropolis-Hastings: Easy to implement but might struggle to explore efficiently, especially in high-dimensional spaces.\nGibbs Sampling: Perfect for structured models but may stumble with highly correlated variables.\nHamiltonian Monte Carlo: Efficiently navigates high-dimensional spaces, leading to faster convergence and smoother mixing.\n\n\n\nAcceptance Criterion\n\nMetropolis-Hastings: Decides whether to accept or reject proposals based on a ratio of target distribution probabilities.\nGibbs Sampling: Skips the acceptance drama and generates samples directly from conditional distributions.\nHamiltonian Monte Carlo: Judges proposals based on the joint energy of position and momentum variables, like a strict dance instructor.\n\n\n\nParameter Tuning and Complexity\n\nMetropolis-Hastings: Requires tweaking the proposal distribution but keeps it simple.\nGibbs Sampling: A breeze to implement, but watch out for those conditional distributions (they can be sneaky).\nHamiltonian Monte Carlo: Needs tuning of parameters like step size and trajectory length, and the implementation might get a bit hairy with momentum variables and gradient computation.\n\n\n\n\nMCMC in action\nIn the following, you can see an interactive animation of different MCMC algorithms (MH, Gibbs and HMC) and how they work to uncover distributions in two dimensions. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo\n\n\n\n\n\n\n\nPractical Tips for the Real World\n\nImplementing MCMC Algorithms in Practice\nAlright, theory’s cool and all, but let’s get down to brass tacks. When you’re rolling up your sleeves to implement MCMC algorithms, it’s like picking the right tool for the job. Simple models? Metropolis-Hastings or Gibbs sampling has your back. But when you’re wrangling with the big boys (those complex models) that’s when you call in Hamiltonian Monte Carlo. It’s like upgrading from a rusty old wrench to a shiny new power tool. And don’t forget about tuning those parameters, it’s like fine-tuning your car for a smooth ride.\nBeyond all the technical jargon, successful Bayesian inference is part gut feeling, part detective work. Picking the right priors is like seasoning a dish, you want just the right flavor without overpowering everything else. And tuning those parameters? It’s like fine-tuning your favorite instrument to make sure the music hits all the right notes.\n\n\nChallenges and What Lies Ahead\nBut hey, nothing worth doing is ever a walk in the park, right? MCMC might be the hero of the Bayesian world, but it’s not without its challenges. Scaling up to big data? It’s like trying to squeeze into those skinny jeans from high school (uncomfortable and a bit awkward). And exploring those complex parameter spaces? It’s like navigating a maze blindfolded.\nBut fear not! There’s always a light at the end of the tunnel. Recent innovations in Bayesian inference, like variational inference (we’ll tackle this cousin of MCMC in a future post) and probabilistic programming languages, are like shiny beacons, guiding us to new horizons.\nThese days, some probabilistic programming languages, like Stan, use a souped-up version of the Hamiltonian Monte Carlo algorithm with hyperparameters tuned on the fly. These tools are like magic wands that turn your ideas into reality, just specify your model, and let the parameter space exploration happen in the background, no sweat.\n\n\nWrapping It Up\nAs we wrap up our journey into the world of MCMC, let’s take a moment to appreciate the wild ride we’ve been on. MCMC might not wear a cape, but it’s the hero behind so much of what we do in Bayesian data analysis. It’s the tool that lets us dive headfirst into the murky waters of uncertainty and come out the other side with clarity and insight.\nIn the next episode of our MCMC series, we’ll dive into the infamous Hamiltonian Monte Carlo and explore the statistical wizardry behind this and other similar algorithms. Until then, happy sampling!\n\n\n\n\n\nCitationBibTeX citation:@misc{castillo-aguilar2024,\n  author = {Castillo-Aguilar, Matías},\n  title = {Markov {Chain} {Monte} {What?}},\n  date = {2024-04-25},\n  url = {https://bayesically-speaking.com/posts/2024-04-14 mcmc part 1/},\n  doi = {10.59350/mxfyk-6av39},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2024. “Markov Chain Monte What?”\nApril 25, 2024. https://doi.org/10.59350/mxfyk-6av39."
  },
  {
    "objectID": "posts/2025-01-06 simulating-action-potential/index.html",
    "href": "posts/2025-01-06 simulating-action-potential/index.html",
    "title": "Decoding the Neuron’s Spark: Building Intuition for Action Potential Dynamics",
    "section": "",
    "text": "Introduction\nEver wondered how your brain manages to tell your hand to grab that slice of pizza before your roommate does? It’s all thanks to an incredibly complex, yet surprisingly efficient communication network, your nervous system. Think of it as the internet of your body, but instead of cat videos and online shopping, it’s buzzing with urgent messages about survival (like pizza acquisition) and other important stuff, like not walking into walls.\nThis network relies on specialized cells called neurons, which are basically your body’s tiny gossips. They constantly chatter with each other, passing along information in the form of rapid electrical signals called action potentials (Drukarch and Wilhelmus 2023). These aren’t your grandma’s static shocks; they’re more like carefully crafted Morse code messages, zipping along neural pathways at impressive speeds. Imagine each neuron as a tiny town crier, shouting the latest news (or sensory input) to the next town over.\nNow, you might be thinking, “Electrical signals? Sounds complicated.” And you’d be right. But fear not! In this post, we’re going to break down the magic behind action potentials using a simplified mathematical model. Think of it as a cheat sheet for understanding how these tiny electrical storms work. We’ll start with the basics and gradually add complexity, like adding extra gossip to the town crier’s message, until we have a decent understanding of the dynamics at play. No PhD in neuroscience required (though a craving for pizza is always helpful). So, buckle up, because we’re about to drive into the electrifying world of neuronal communication!\n Photo from Encyclopaedia Britannica\n\n\nThe Neuron as a Simple Circuit: A First Approximation\nLet’s imagine our neuron not as a bustling town crier, but as a tiny, slightly bored house. It’s got walls (the cell membrane), an inside (intracellular fluid), and an outside (extracellular fluid). These fluids are salty solutions, full of charged particles called ions like sodium, potassium, and chloride (the electrolyte gang you see advertised in sports drinks). Now, because these fluids have different concentrations of these ions, there’s a difference in electrical charge between the inside and the outside of the house. This difference in charge is what we call the membrane potential, measured in millivolts (mV). Think of it like a tiny battery inside the neuron, just waiting to be used.\nNormally, this “battery” sits at a resting voltage of around -70 mV. Why negative? It’s just a convention: we define the outside of the cell as zero, and the inside is negatively charged relative to it. So, our little neuron house is just chilling, with a slight negative charge inside (like a grumpy teenager refusing to get out of bed).\nNow, our neuron’s walls (the membrane) aren’t completely airtight. They have tiny holes, called leak channels, which allow some ions to slowly trickle in and out. It’s like having a slightly leaky faucet, not a major flood, but a constant drip. These leaks are crucial because they maintain that resting membrane potential (Rubin 2021).\n Photo from Neuroscience: Canadian 1st Edition\nAnother important property of the membrane is its ability to store electrical charge, much like a capacitor in an electronic circuit. This is called capacitance. Imagine the membrane as two conductive plates (the inside and outside of the cell) separated by an insulator (the membrane itself). When there’s a difference in charge across the membrane (our membrane potential), it stores some of that charge (Rubin 2021). It’s like a tiny electrical reservoir, ready to release its charge when needed.\nSo, to summarize our lazy neuron house:\n\nMembrane potential: A voltage difference between the inside and outside.\nLeak channels: Tiny holes that allow ions to slowly leak across the membrane, maintaining the resting potential.\nCapacitance: The membrane’s ability to store electrical charge.\n\n\nExpanding our Intuition Using Math: Waking Up the Sleeping Neuron\nRight now, our neuron is basically a zombie. It’s just sitting there, leaking a bit of charge and generally being a bit of a slacker. No exciting messages, no action potentials, it’s like a town crier who’s decided to take an eternal nap. But fear not! Things are about to get much more interesting. We’re about to introduce the real heroes of the story: voltage-gated ion channels.\n Photo from Neuroscience: Canadian 1st Edition\nNow, let’s get a little mathematical here (don’t worry, we’ll keep it fun!). We need a function to describe how the membrane potential (\\(V\\)) changes over time (\\(t\\)) due to all that pesky leakage.\nSo, what kind of function would you use to describe something that decays gradually, like a deflating balloon? You guessed it! We need an exponential function, that classic workhorse of mathematical modeling.\nExponential functions look something like this:\n\n\nCode\ninput &lt;- seq(0, 8, by = 0.01)\n\ndf &lt;- data.table(input = input,\n           `e^x` = exp(input),\n           `-e^x` = -exp(input),\n           `e^{-x}` = exp(-input),\n           `-e^{-x}` = -exp(-input))\n\ndf &lt;- melt(df, id.vars = \"input\")\n\nggplot(df, aes(input, value)) +\n  facet_wrap(~ variable, labeller = \"label_parsed\", scales = \"free_y\") +\n  geom_hline(yintercept = 0, linewidth = 1/2, \n             linetype = 2, col = \"gray50\") +\n  geom_line(linewidth = 1, \n                col = \"orange\") +\n  labs(x = \"Input\", y = \"Output\",\n       title = \"Exponential Function Dynamics\")\n\n\n\n\n\nExponential Function and Common Variations. Here we see how changing a small detail in the original function (in this case a negative sign) can yield dramatically different behavior.\n\n\n\n\nAt first glance, we can see that the bottom variations of the exponential function (the ones with negative exponents) seem to behave more like a neuron should. They oscillate nicely, staying within a reasonable range. This function with the negative exponent, \\(e^{-x}\\), seems like a good starting point to describe how the membrane potential gradually returns to its resting state. It’s like watching a deflating balloon (slow and steady).\n\n\nCustomizing It Further: Tweaking the Decay Rate\nNow, let’s get a little more creative! We can tweak this function to control how quickly the membrane potential returns to rest. Imagine it’s like adjusting the valve on a tire, you can control how fast the air leaks out.\nWe can do this by adding a “rate parameter” to our exponent. Instead of just \\(e^{-x}\\), we can use \\(e^{-x \\cdot \\lambda}\\). This \\(\\lambda\\) value acts like a speed control for the decay. By changing \\(\\lambda\\), we can adjust how quickly the membrane potential returns to its resting value. It’s like having a dial to control how fast the balloon deflates.\nLet’s visualize this change and see how it affects the decay rate!\n\n\nCode\ninput &lt;- seq(0, 10, by = 0.01)\n\ndf &lt;- lapply(X = seq(0, 1.4, by = 0.2), function(x) {\n  data.table(Input = input, \n             lambda = format(x, digits = 1, nsmall = 1),\n             Output = exp(-input * x))\n}) |&gt; rbindlist()\n\nggplot(df, aes(Input, Output, color = ordered(lambda))) +\n  geom_hline(yintercept = 0, linewidth = 1/2, \n             linetype = 2, col = \"gray50\") +\n  geom_line(linewidth = 1) +\n  labs(color = expression(lambda),\n       y = expression(e^{-x*lambda}),\n       title = expression(\"Rate Parameter (\"*lambda*\") on Output Dynamics\"))\n\n\n\n\n\nEffect of Rate Parameter on Function Dynamics. Here we see that by altering the rate parameter \\(\\lambda\\) we can toy around with “how fast” our exponential function is returning to baseline.\n\n\n\n\nGreat! Now we need to give this function a little personality. Right now, it’s a bit generic. It always starts at 1 and decays towards 0. But what if we want to start at a different value? Or maybe we want it to decay towards a different value than 0?\nNo problem! We can easily customize this function.\nFirst, let’s give it a starting point. We can add a multiplicative factor (let’s call it \\(b\\)) to the exponential function. This is like giving our function a “starting height”. Now, our function looks like this:\n\\[\nb \\cdot e^{-x \\cdot \\lambda}\n\\]\nNext, let’s give it a target value. We can add a linear factor (let’s call it \\(a\\)) to shift the entire curve up or down. This is like setting a new “ground level” for our function.\nWith these adjustments, our function now looks like this:\n\\[\na + b \\cdot e^{-x \\cdot \\lambda}\n\\]\nThis gives us much more flexibility! For example, if we set \\(a\\) to 0 and \\(b\\) to 1, we get our original function. It’s like having a basic template and then customizing it to fit our specific needs.\n In fact, she did use it at the end. Photo from Frank Mariani in Cartoonist for Hire\nBut wait a minute! We’ve been doing all this math, and we haven’t even talked about neurons yet! What was the point of all this mental gymnastics? Well, it turns out that this seemingly random function has a very real biological meaning.\nLet’s take a look at that equation again, but this time with some real neuron terms:\n\\[\nV(t) = E_L + (V_0 - E_L) \\cdot e^{-g_L \\cdot t}\n\\]\nNow, this equation starts to make sense. It tells us how the membrane potential (\\(V\\)) changes over time (\\(t\\)). It’s like watching a movie of the membrane potential.\n\n\\(V(t)\\): This is the membrane potential at any given moment, the voltage across the cell membrane at that exact instant.\n\\(E_L\\): This is the “leak reversal potential”, the magical voltage where the leak channels are perfectly balanced. It’s like the “sweet spot” for the membrane, usually close to the resting membrane potential (around -70 mV). This is the value our function will try to get over time.\n\\(V_0\\): This is the starting point, the initial membrane potential at time \\(t = 0\\). It’s like the starting position in a race.\n\\(g_L\\): This is the “leak conductance”, a measure of how easily ions can leak through the membrane. Think of it as how wide the leaky faucet is open. A higher \\(g_L\\) means ions leak faster, and the membrane potential changes more quickly.\n\\(e^{-g_L \\cdot t}\\): This is the exponential decay term, the engine that drives the change. It ensures that the membrane potential gradually approaches \\(E_L\\) over time, like a car slowly coming to a stop.\n\nNow, let’s get a little more mathematical. If we want to express how the membrane potential changes over time, we need to find the “rate of change” of the membrane potential. In math terms, this is called the derivative.\n Average Bayesically-Speaking reader going through the content on a typical blog post after I say “We’ll start with the basics”. Photo from Giphy\nIf we take the derivative of our previous equation (don’t worry, you don’t need to know the calculus!), we get something like this:\n\\[\n\\frac{d V}{d t} = -g_L \\cdot (V_0 - E_L)\n\\]\nThis equation tells us how fast the membrane potential is changing at any given moment.\nLet’s break this down:\n\n\\(\\frac{d V}{d t}\\): This fancy term represents the “rate of change” of the membrane potential. It tells us how quickly the voltage is rising or falling.\n\\(g_L\\): This is our old friend, the leak conductance. A higher \\(g_L\\) means the membrane is “more leaky”, and the membrane potential will change more rapidly.\n\\((V_0 - E_L)\\): This term represents the “driving force” for the leak current. The bigger the difference between the current potential and the leak reversal potential, the stronger the “urge” for the membrane potential to return to equilibrium. It’s like a ball rolling down a steep hill, the steeper the hill, the faster it rolls.\n\nIf the next plot, we’ll see the solution to this equation, an exponential decay curve. It’s like watching a ball rolling down a hill, it starts fast but gradually slows down as it approaches the bottom.\n\n\nCode\n# Parameters\ntime &lt;- seq(0, 50, 0.1) # Time vector\n\n# Function to calculate membrane potential over time\nmembrane_potential &lt;- function(time, V0, g_L, E_L) {\n  V &lt;- (V0 - E_L) * exp(-g_L * time) + E_L\n  return(V)\n}\n\n# Starting membrane potentials\ndf &lt;- lapply(c(-50,-60,-65,-68,-72,-75,-80,-90), function(x) {\n           data.table(v = membrane_potential(time, V0 = x, g_L = 0.1, E_L = -70),\n                      start_v = x,\n                      time = time)\n         }) |&gt; \n  rbindlist()\n\n# Create the plot using ggplot2\nggplot(df, aes(x = time, y = v, color = ordered(start_v))) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = -70, linetype = \"dashed\", color = \"black\", alpha = 0.7) + # Add E_L line\n  labs(\n    x = \"Time (Arbitrary Units)\",\n    y = \"Membrane Potential (mV)\",\n    title = \"Membrane Potential Decay Due to Leak Current\",\n    color = \"Starting Voltage (mV)\"\n  )\n\n\n\n\n\nMembrane Potential and Leak Current. Here we can see that the membrane tends to return to its resting potential. This behavior is accomplished by the tweaks we made earlier to our exponential function.\n\n\n\n\nFor example, if we inject a small current that briefly changes the membrane potential to -60 mV, the membrane potential will decay back to -70 mV, following the equation.\n\n\nToo Simple?\nThis simple model is important because it shows us how the membrane potential would behave passively, in the absence of any active processes. It explains how the resting membrane potential is maintained. However, it doesn’t explain the rapid, dramatic changes we see during an action potential. Our sleepy neuron is still just leaking and slowly returning to its resting state. It’s like watching paint dry (not exactly thrilling). We need something more to explain the neuron’s “shouting” behavior. That “something more” is the introduction of voltage-gated ion channels, which we’ll explore in the next section.\nOkay, let’s add some simplified equations to represent the voltage-gated channels and incorporate them into our membrane potential equation.\n\n\n\nAdding Voltage-Gated Channels: The Key to Excitation\n Photo from Human Bio Media\nOkay, let’s get down to the nitty-gritty. How does this whole “voltage” thing actually work? Well, picture this: the change in voltage over time (basically how fast the voltage is shifting), is all about the flow of ions. You’ve got ions lazily leaking through the membrane (let’s call that the “lazy ion flow”), and then you’ve got the more exciting players: sodium, potassium, and even some other ions that like to join the party.\nSo, if we want to get fancy, we could say that the change in voltage over time, which we can write as \\(\\frac{dV}{dt}\\) for all you math whizzes out there, is basically a sum of all these ion flows. We’ve got the lazy ion flow (\\(I_L\\)), the sodium party crashers (\\(I_{Na}\\)), the potassium crew (\\(I_K\\)), and any other surprise guests (\\(I_{\\text{other ions}}\\)).\nAnd if we want to put it all together in a fancy equation, we could say that:\n\\[\n\\frac{dV}{dt} = I_L + I_{Na} + I_K + \\dots + I_{\\text{other ions}}\n\\]\nSo, now we’ve got this “leaky current” (\\(I_L\\)), which we can basically describe using that fancy equation \\(-g_L \\cdot (V_0 - E_L)\\). But what about those exciting voltage-gated channels? How do we explain their contribution to the party? To figure that out, we need to dive deeper into the wild world of voltage-gated channel dynamics.\nRemember our boring old passive membrane? It just kind of slumped back to its resting potential, like a deflated balloon. Well, our active neurons are a whole different story. They’ve got these amazing channels that open and close depending on the voltage, making the whole system much more exciting.\nThese channels, especially the sodium (\\(Na^+\\)) and potassium (\\(K^+\\)) channels, are the real stars of the show. They’re the ones that cause those dramatic swings in voltage during an action potential.\nSodium channels are like little gates. They have an “activation gate” (let’s call it gate \\(m\\)) that swings open around -55 mV. This is like the starting pistol for the race! Sodium ions flood into the cell, and the voltage skyrockets. But here’s the catch: these channels also have an “inactivation gate” (let’s call it gate \\(h\\)) that slams shut around +30 mV. It’s like the emergency brakes! This prevents the cell from exploding with too much sodium. And don’t worry, the inactivation gate eventually reopens when the cell calms down and returns to its resting potential (around -70 mV).\n Sodium channel in an open state, illustrating its activation and inactivation gates. Photo from Human Bio Media\nSo, if we want to express the sodium current (\\(I_{Na}\\)) in all its glory, we could say that:\n\\[\nI_{Na} = -g_{Na} \\cdot m \\cdot h \\cdot (V_0 - E_{Na})\n\\]\nWhere:\n\n\\(g_{Na}\\): Sodium conductance (how easily sodium ions flow when channels are open).\n\\(m\\): Sodium activation gate (1 = open, 0 = closed).\n\\(h\\): Sodium inactivation gate (1 = open, 0 = closed). This gate introduces a delay and helps terminate the sodium current.\n\\(E_{Na}\\): Sodium reversal potential.\n\nIn contrast, potassium channels are a bit simpler. They only have one gate (let’s call it gate \\(n\\)), which acts like a slow-opening door. This gate starts to swing open around +30mV, allowing potassium ions to flood out of the cell. This outflow of positive charge helps bring the voltage back down to resting potential (a phase we call “repolarization”). But here’s the twist: this gate is a bit slow to close. It doesn’t fully shut until the voltage dips below -80 mV, which is even more negative than the resting potential. This creates a slight undershoot before the cell finally settles back down.\n Potassium channel in an open state, with its only voltage-dependent gate. Photo from Human Bio Media\nJust like we did with the sodium channels, we can express the potassium current (\\(I_K\\)) as the following:\n\\[\nI_{K} = -g_{K} \\cdot n \\cdot (V_0 - E_{K})\n\\]\nWhere:\n\n\\(g_K\\): Potassium conductance.\n\\(n\\): Potassium activation gate (1 = open, 0 = closed).\n\\(E_K\\): Potassium reversal potential.\n\nSo now, our updated equation now looks like this:\n\\[\n\\begin{aligned}\n\\frac{dV}{dt} = -&g_L \\cdot (V - E_L) + \\\\\n-&g_{Na} \\cdot m \\cdot h \\cdot (V - E_{Na}) + \\\\\n-&g_K \\cdot n \\cdot (V - E_K)\n\\end{aligned}\n\\] To maintain clarity and focus on the core dynamics, we’ll use simplified “on/off” gating variables rather than the full Hodgkin-Huxley equations (which we can explore another time).\n\n\n\n\n\n\nSimplified Gating: A Key Simplification\n\n\n\n\n\nIn the real world, these gating variables (m, h, and n) are described by some seriously complex equations in the full Hodgkin-Huxley model (see Yao et al. 2023). It’s like trying to understand the inner workings of a Swiss watch!\nBut for our simplified model, we’re going to take a shortcut. We’ll assume these gates simply snap open and shut based on the voltage. It’s like we’re replacing that intricate Swiss watch with a simple on/off switch. This simplification helps us grasp the fundamental principles of action potential generation without getting bogged down in a sea of equations.\n\n\n\nWith these additions, our model finally starts to show some exciting behavior! If the membrane potential reaches a certain “trigger point” (called the threshold), the sodium channels swing open wide (\\(m\\) = 1). It’s like a floodgate opening! Sodium ions rush into the cell, causing a rapid spike in voltage, that’s our action potential!\nBut then, the sodium inactivation gate slams shut (\\(h\\) = 0), and the potassium channels finally open their doors (\\(n\\) = 1). Potassium ions rush out of the cell, bringing the voltage back down to normal, that’s the “repolarization” phase.\n Voltage-gated channels for sodium (Na+) and potassium (K+). ECF, extracellular fluid; ICF, intracellular fluid. Photo from Human Bio Media\nNow, let’s put this model into action! We’ll use something called the Euler method, which is basically a fancy way of numerically solving our equation. This will allow us to see how the membrane potential changes over time in response to a stimulus. It’s like running a simulation to see how our “neuron” behaves in the real world.\n\n\n\n\n\n\nAbout the Hodgkin-Huxley Model\n\n\n\n\n\nThe Hodgkin-Huxley equations (see Hodgkin and Huxley 1939), developed in the mid-20th century, revolutionized our understanding of how neurons fire. These complex equations, initially derived from meticulous experiments on the giant squid axon, accurately model the intricate interplay of ion channels that generate the action potential.\nOkay but, why it matters today? Well, for many reasons like:\n\nFoundation for modern neuroscience: The Hodgkin-Huxley model remains a cornerstone of computational neuroscience, providing a framework for understanding the electrical behavior of neurons.\nDrug development: Researchers use these equations to study the effects of drugs and toxins on neuronal activity, aiding in the development of new medications for neurological disorders.\nArtificial intelligence: Inspired by the Hodgkin-Huxley model, researchers are developing biologically realistic artificial neurons for use in neuromorphic computing, a promising area of artificial intelligence.\n\n\n\n\n\n\nSimulating an Action Potential: Bringing the Model to Life\nAlright, let’s get this show on the road! Now that we have our simplified mathematical model of the neuron, it’s time to bring it to life. We’ll use R, a powerful programming language, to simulate our model and watch how the membrane potential changes over time.\nWe’ll use a technique called the Euler method, which is basically like taking tiny little steps to solve our equation. It’s a bit like trying to reach a destination by taking a series of small hops instead of jumping straight there.\n Average Bayesically-Speaking reader when is asked about their knowledge on voltage-gated channels. Photo from Wojak-Studio\n\nSetting the Stage: Defining the Parameters\nFirst, we need to define the parameters of our model. These parameters represent the properties of our neuron, such as how “leaky” the membrane is and the properties of the sodium and potassium channels. We also need to define the time step (dt), which determines how often we “take a picture” of the membrane potential during our simulation.\n\n# Parameters\nparams &lt;- list(\n  g_L = 1, E_L = -70,   ## Leak Channels\n  g_Na = 20, E_Na = 30, ## Sodium Channels\n  g_K = 2, E_K = -90,   ## Potassium Channels\n  dt = 0.001\n)\n\nHere:\n\ng_L: Leak conductance.\nE_L: Leak reversal potential (resting potential).\ng_Na: Sodium conductance.\nE_Na: Sodium reversal potential.\ng_K: Potassium conductance.\nE_K: Potassium reversal potential.\ndt: Time step (0.001 ms).\n\n\n\nThe Sodium Activation Gate: The On/Off Switch\nThe sodium activation gate (m) is like the “on” switch for sodium channels. When the membrane potential (V_prev) reaches a critical point, around -55 mV, this gate swings open instantly, allowing sodium ions to flood in. It’s like someone yelling “Go!” at a starting line.\nIn our simplified model, we’re making a big assumption: this gate is an all-or-nothing switch. Either it’s completely open (m becomes 1) or completely closed (m is 0). This is a simplification, but it helps us capture the essence of the rapid rise of the action potential.\n\nupdate_m &lt;- function(V_prev) {\n  ifelse(V_prev &gt; -55, 1, 0)\n}\n\nThis function takes the previous membrane potential (V_prev) as input and tells us whether the sodium activation gate is open or closed.\n\n\nThe Sodium Inactivation Gate: The Emergency Brake\nThe sodium inactivation gate (h) is like the emergency brake for the sodium channels. It starts open (h = 1) at rest, but when the membrane depolarizes and sodium channels open wide (m = 1), this gate slowly starts to close (h transitions towards 0). This is crucial to prevent a runaway influx of sodium.\nHowever, the inactivation gate only starts to close when the membrane potential is sufficiently positive (around +20 mV). It’s like the brake only engages when the car is going fast enough.\nConversely, when the membrane repolarizes and the sodium channels close (m = 0), the inactivation gate slowly reopens (h transitions towards 1). But this recovery only happens when the membrane potential is sufficiently negative (around -70 mV). It’s like the brake only releases when the car has slowed down significantly.\nThis conditional behavior is implemented in the following function:\n\nupdate_h &lt;- function(m_current, V_prev, h_prev) {\n  ifelse(\n    test = m_current == 1 & V_prev &gt;= 20, \n    yes = 0, \n    no = ifelse(\n      test = m_current == 0 & V_prev &lt;= -70, \n      yes = 1, \n      no = h_prev\n    )\n  )\n}\n\nThis function takes the current value of m (m_current), the previous membrane potential (V_prev), and the previous value of h (h_prev) as inputs and returns the updated value of h.\n\n\nThe Potassium Activation Gate: The Slow and Steady Door\nThe potassium activation gate (n) is like a slow-opening door. It starts to open when the membrane potential reaches a sufficiently positive value (around +20 mV). But unlike the sodium gate, it’s a bit sluggish.\nThere’s also a bit of “hysteresis” here. This means that the potassium gate only closes when the membrane potential becomes extremely negative (around -80 mV) and it was already open in the previous time step. It’s like the door is reluctant to close once it’s been opened. This hysteresis ensures that the membrane repolarizes completely before the potassium channels fully close.\n\nupdate_n &lt;- function(V_prev, n_prev) {\n  ifelse(\n    test = V_prev &gt;= 20, \n    yes = 1, \n    no = ifelse(\n      test = V_prev &lt;= -80 & n_prev == 1, \n      yes = 0, \n      no = n_prev\n    )\n  )\n}\n\nThis function takes the previous membrane potential (V_prev) and the previous value of n (n_prev) as input and returns the updated value of n.\n\n\nCalculating the Ion Flows: The Grand Finale\nThis function calculates the flow of ions across the membrane. We have three main players:\n\nThe Leaky Current: This is the “background” current, where ions slowly leak through the membrane.\nThe Sodium Current: This is where the action happens! It’s influenced by the sodium activation gate (m) and the inactivation gate (h).\nThe Potassium Current: This current is controlled by the potassium activation gate (n).\n\nTo calculate each current, we use a simple formula: \\(I = g \\cdot (V - E)\\). It’s like calculating the flow of water through a pipe, where \\(g\\) represents the size of the pipe, \\(V\\) is the pressure difference, and \\(E\\) is a kind of “equilibrium point.”\n\ncalculate_currents &lt;- \n  function(V_prev, m, h, n, params) {\n    \n  l_current &lt;- -params$g_L * (V_prev - params$E_L)\n  na_current &lt;- -params$g_Na * m * h * (V_prev - params$E_Na)\n  k_current &lt;- -params$g_K * n * (V_prev - params$E_K)\n  \n  output &lt;- list(l_current = l_current, \n                 na_current = na_current, \n                 k_current = k_current)\n  \n  return(output)\n}\n\nThis function takes the previous membrane potential (V_prev), the gating variables (m, h, n), and the parameters (params) as input and returns a list containing the calculated currents.\n\n\nSimulating Neurons in Action: Bringing it All Together!\nThis is the grand finale! This function brings all the pieces together to simulate how our neuron actually behaves. It takes the model parameters, a time vector (which tells us how long to run the simulation), and a stimulus current (like an external input to the neuron) as input.\nThen, it iterates through each time step, updating the gating variables (those pesky “m,” “h,” and “n” gates) and calculating how the membrane potential changes using the Euler method. It’s like watching a movie frame-by-frame, seeing how the neuron responds to the different inputs.\n\nsimulate_ap &lt;- function(params, time, stimulus_current) {\n  n &lt;- length(time)\n  V &lt;- rep(params$E_L, n) # Initialize Membrane potential\n  m &lt;- numeric(n) # Initialize Sodium Activation\n  h &lt;- rep(1, n) # Initialize Sodium Inactivation\n  n_inf &lt;- numeric(n) # Initialize Potassium Activation\n  l_current &lt;- numeric(n) # Initialize Leak Current\n  na_current &lt;- numeric(n) # Initialize Sodium Current\n  k_current &lt;- numeric(n) # Initialize Potassium Current\n  \n  for (i in 2:n) {\n    ## Updates the state of each channel's gates\n    m[i] &lt;- update_m(V[i - 1])\n    h[i] &lt;- update_h(m[i], V[i - 1], h[i - 1])\n    n_inf[i] &lt;- update_n(V[i - 1], n_inf[i - 1])\n    \n    ## Compute the current \n    currents &lt;- calculate_currents(\n      V_prev = V[i - 1], \n      m = m[i], \n      h = h[i], \n      n = n_inf[i], \n      params = params\n    )\n    \n    l_current[i] &lt;- currents$l_current\n    na_current[i] &lt;- currents$na_current\n    k_current[i] &lt;- currents$k_current\n    \n    ## Sum each current contribution to the overall voltage change\n    dVdt &lt;- \n      l_current[i] + \n      na_current[i] + \n      k_current[i] + \n      stimulus_current[i]\n    \n    ## Update the Voltage based on current net flow\n    V[i] &lt;- \n      V[i - 1] + dVdt * params$dt\n  }\n  \n  output &lt;- data.table(Time = time, Voltage = V)\n  return(output)\n}\n\nThis function gets the party started! It initializes the membrane potential and all those gating variables (“m,” “h,” and “n”) to their starting values. Then, it enters a loop, stepping through time and updating these variables at each step. It’s like watching a movie play out frame-by-frame, observing how the membrane potential changes in response to the shifting currents.\nNow, let’s run this simulation and see what happens! We’ll visualize the results to get a better understanding of how the neuron behaves.\n\n\nCode\n# Simulation time\ntime &lt;- seq(0, 8, by = params$dt)\n\n# Stimulus\nstimulus_current &lt;- numeric(length(time))\nstimulus_current[time &gt; 2 & time &lt; 2.5] &lt;- 50\n\n# Run the simulation\ndf &lt;- simulate_ap(params, time, stimulus_current)\n\n# Plotting\nggplot(df, aes(Time, Voltage)) +\n  geom_hline(yintercept = -70, linewidth = 1/2, linetype = 2, col = \"gray50\") +\n  geom_line(linewidth = 1, color = \"orange\") +\n  labs(x = \"Time (ms)\", y = \"Membrane Potential (mV)\", title = \"Simplified Action Potential Model\")\n\n\n\n\n\nSimplified Action Potential Model. Here we can see that after taking into account each contribution to the overall voltage dynamics over time (based on net current flow), we can see something more alike to a real action potential curve.\n\n\n\n\n\n\n\n\nTweaking the Knobs: Sensitivity Analysis\nNow that we have our neuron simulation up and running, let’s see what happens when we start tinkering with the settings. This is called sensitivity analysis. We’ll systematically change one parameter at a time and see how it affects the neuron’s behavior. It’s like tweaking the knobs on a radio to see how it changes the sound.\nThis analysis is crucial for understanding how our model works and figuring out which parameters have the biggest impact on the action potential. It’s like trying to figure out which ingredients are most important for baking the perfect cake.\nTo make this process easier, we’ll create a special function called sensitivity_analysis. This function takes the model parameters, the time vector, the stimulus current, the name of the parameter we want to change (param_name), and a list of values we want to test for that parameter (values) as input. It then runs the simulation for each value in the list, stores the results, and combines them all into a single, organized data frame.\n\nsensitivity_analysis &lt;- \n  function(params, time, stimulus_current, param_name, values) {\n  \n    results &lt;- vector(mode = \"list\")\n    \n    for (value in values) {\n      # Modify the parameter value\n      modified_params &lt;- params\n      modified_params[[param_name]] &lt;- value \n      \n      # Run the simulation\n      df &lt;- simulate_ap(modified_params, time, stimulus_current) \n      \n      # Add a column to track the parameter value\n      df$ParameterValue &lt;- value\n      \n      # Store the results\n      results[[as.character(value)]] &lt;- df\n    }\n    \n    output &lt;- rbindlist(results)\n    return(output) # Combine all data.tables into one\n}\n\nInside the function, we do the following:\n\nCreate an empty list: We create an empty list called results to store the output of each simulation. Think of it as an empty box to store all our simulation results.\nLoop through the values: We loop through each value in the list of values we want to test. It’s like trying different settings on a machine one by one.\nMake a copy: We create a copy of our original parameters called modified_params. This is crucial to avoid messing up the original settings.\nTweak the parameter: We modify the specific parameter (param_name) in our modified_params copy to the current value from our list.\nRun the simulation: We run our simulate_ap function with these modified parameters to see how the neuron behaves.\nAdd a label: We add a new column called ParameterValue to the results to keep track of which parameter value we used for that simulation. It’s like labeling each experiment.\nStore the results: We store the results of this simulation in our results list, using the parameter value as a label for easy reference.\nCombine the results: Finally, we use a special function called rbindlist to neatly combine all the individual data frames in our results list into one big data frame. This gives us a complete overview of how the neuron behaves with different parameter settings.\n\n\nThe Plotting Function: Seeing the Results\nNow, let’s visualize our results! We’ll create a special function called plot_sensitivity to do this. This function takes the combined results data frame and the name of the parameter we were analyzing as input. It then generates a plot that shows how the membrane potential changes over time for each of the parameter values we tested. It’s like creating a visual story of how the neuron behaves under different conditions.\n\nplot_sensitivity &lt;- function(results, param_name) {\n  ggplot(results, aes(Time, Voltage, color = factor(ParameterValue))) +\n    geom_hline(yintercept = -70, linewidth = 1/2, linetype = 2, col = \"gray50\") +\n    geom_line(linewidth = 1) +\n    labs(x = \"Time (ms)\", y = \"Membrane Potential (mV)\", \n         title = scales::label_parse()(paste(\"Sensitivity~Analysis~of~\", param_name))) +\n    scale_color_ordinal(name = scales::label_parse()(param_name))\n}\n\nThis function creates a line plot using ggplot2, where the x-axis represents time, the y-axis represents the membrane potential, and different colors represent different values of the varied parameter. The factor() function is used to treat the ParameterValue as a categorical variable, ensuring that each value gets a distinct color in the plot.\n Your face after I told you that we’re going to see the basics and it turned out not so basic after all. Photo from Pinteres\n\n\nSetting the Stage for Simulations\nBefore we dive into the nitty-gritty of sensitivity analysis, we need to set the stage for our simulations.\n\nTime: We define the simulation time, which tells us how long to run the experiment. We’ll run the simulation from 0 to 8 milliseconds (ms).\nTime Step: We need to decide how often we “take a picture” of the membrane potential. This is determined by the dt parameter in our params list.\nStimulus: We’ll create a brief stimulus current, like a little jolt of electricity, that’s applied between 0.5 and 1.0 milliseconds.\n\n\n# Simulation time and stimulus\ntime &lt;- seq(0, 6, by = params$dt)\nstimulus_current &lt;- numeric(length = length(time))\nstimulus_current[time &gt; 0.5 & time &lt; 1.0] &lt;- 50\n\n\n\nAssessing the Effect of Sodium Conductance (\\(g_{Na}\\)): Turning Up the Heat\nNow, let’s see what happens when we tweak the sodium conductance (\\(g_{Na}\\)). This is like adjusting the heat on a stove.\n\n\nCode\n# Run sensitivity analysis for g_Na\ngna_values &lt;- c(9.5, 10, 12, 15, 20)\n\ngna_results &lt;- sensitivity_analysis(params, time, stimulus_current, \"g_Na\", gna_values)\n\nplot_sensitivity(gna_results, \"g[Na]\")\n\n\n\n\n\nEffect of Sodium Conductance in Potential Dynamics. Here we can see the overall effect of changing the sodium conductance on membrane potential dynamics.\n\n\n\n\nBy running our sensitivity_analysis function and then plotting the results, we can clearly see how changing \\(g_{Na}\\) affects the action potential. It’s like watching how turning up the heat affects how quickly water boils. As expected, increasing \\(g_{Na}\\) makes the depolarization much faster and more dramatic.\n\n\nAssessing the Effect of Potassium Conductance (\\(g_{K}\\)): Controlling the Cool-Down\nNext, let’s see how changing the potassium conductance (\\(g_{K}\\)) affects things. This is like adjusting the thermostat, it controls the cool-down period.\n\n\nCode\n# Run sensitivity analysis for g_K\ngk_values &lt;- c(1, 2, 5, 10)\n\ngk_results &lt;- sensitivity_analysis(params, time, stimulus_current, \"g_K\", gk_values)\n\nplot_sensitivity(gk_results, \"g[K]\")\n\n\n\n\n\nEffect of Potassium Conductance in Potential Dynamics. Here we can see the overall effect of changing the potassium conductance on membrane potential dynamics.\n\n\n\n\nAgain, using our handy functions, we can easily see how \\(g_{K}\\) impacts the action potential. Increasing \\(g_{K}\\) makes the repolarization phase much faster and more pronounced. It’s like adding more ice to a hot drink, it cools down much quicker. This structured approach allows us to efficiently explore how different parameters affect our model and gain a deeper understanding of its dynamics.\n\n\n\nSo, You Think You Know How a Neuron Fires? Think Again!\nOkay, let’s be honest, this “action potential” business is a bit overhyped. It’s basically just a fancy term for a sudden influx of sodium ions followed by a swift efflux of potassium. Really, who needs all those fancy textbooks and intimidating equations? (Kidding! We need them).\nOur little model here? Yeah, it’s basically a “neuron for beginners”. We stripped it down to the essentials, ignoring the nitty-gritty details like how ion channels actually open and close and the subtle ways different potassium channels interact. Who needs that level of detail, right?1\n1 In fact we do! But that much level of detail is reserved for another blog postNow, let’s get real for a second. Real neurons? They’re not this simple. Oh no. They have a whole zoo of ion channels, each with its own quirks and preferences for voltage (Drukarch and Wilhelmus 2023). Sodium channels? Potassium channels? Please, it’s more like a neurochemical orchestra! And don’t even get me started on the calcium-dependent potassium channels…\nBut hey, our little “dumbed-down” neuron serves a purpose. It’s like, the “Cliff’s Notes” of neurophysiology. You get the main idea, you understand the basic principles of membrane excitability. And that, my friends, is enough to impress at your next cocktail party.\nNow, let’s talk about drugs. Oh boy, drugs. You know, those pesky little molecules that interfere with neuronal signaling? Well, guess what? They love to target these ion channels. Lidocaine? Blocks sodium channels, basically shutting down the neuron. Potassium channel blockers? Well, let’s just say they can make neurons go wild. Our model? It can predict these effects, to a certain extent. Of course, it’s not exactly a perfect predictor of pharmacological outcomes. But hey, it’s a start.\nAnd finally, let’s talk about the big picture. Communication. You know, how your brain actually does stuff? Well, it all starts with these action potentials. They’re like, the brain’s version of Morse code. But instead of dots and dashes, it’s a symphony of electrical impulses, traveling along axons and triggering the release of neurotransmitters. And our little model? It gives us a glimpse into this mysterious world of neuronal chatter.\n Motor Neurons (Multipolar) with many Processes (mostly Dendrites). Photo from Neurology Advisor\n\n\nHasta la Vista Baby\nSo, there you have it. Our “simplified” action potential model. Not exactly a perfect mirror of reality, but hey, it’s a good start. Now go forth and explore the wonders of neuroscience! Dive deeper into the hows and whys of ion channel behavior, uncover the secrets of synaptic transmission, and maybe even contribute to the next big breakthrough in neuroscience! The human brain is an incredibly complex machine, and understanding its workings is a journey of endless fascination.\nJust remember, this is just the tip of the iceberg. Real neurons are far more sophisticated than our little model. They’re a tornado of activity, a symphony of ions, and a constant dance of electrical signals. But hey, at least now you have a basic understanding of how these amazing cells work. Disclaimer: This model is for entertainment purposes only. Please do not attempt to perform any neurological procedures based on the information presented here (or maybe try it, but don’t mention my name).\nNow go forth and amaze your friends with your newfound knowledge of neurophysiology. Just don’t get carried away and start diagnosing everyone with “abnormal neuronal activity”. And most importantly, remember to have fun! Neuroscience is a fascinating field, and there’s always something new to learn.\n\n\n\n\n\n\nReferences\n\nDrukarch, Benjamin, and Micha MM Wilhelmus. 2023. “Thinking about the Action Potential: The Nerve Signal as a Window to the Physical Principles Guiding Neuronal Excitability.” Frontiers in Cellular Neuroscience 17: 1232020.\n\n\nHodgkin, Alan L, and Andrew F Huxley. 1939. “Action Potentials Recorded from Inside a Nerve Fibre.” Nature 144 (3651): 710–11.\n\n\nRubin, Devon I. 2021. “Basics of Neurophysiology.” Clinical Neurophysiology, 9.\n\n\nYao, Wei, Yingchen Li, Zhihao Ou, Mingzhu Sun, Qiongxu Ma, and Guanghong Ding. 2023. “Dynamic Analysis of Neural Signal Based on Hodgkin–Huxley Model.” Mathematical Methods in the Applied Sciences 46 (4): 4676–87.\n\nCitationBibTeX citation:@misc{castillo-aguilar2025,\n  author = {Castillo-Aguilar, Matías},\n  title = {Decoding the {Neuron’s} {Spark:} {Building} {Intuition} for\n    {Action} {Potential} {Dynamics}},\n  date = {2025-01-06},\n  url = {https://bayesically-speaking.com/posts/2025-01-06 simulating-action-potential/},\n  doi = {10.59350/e3kv7-20r70},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2025. “Decoding the Neuron’s Spark:\nBuilding Intuition for Action Potential Dynamics.” January 6,\n2025. https://doi.org/10.59350/e3kv7-20r70."
  },
  {
    "objectID": "posts/2025-03-28 from-heartbeats-to-dynamical-systems/index.html",
    "href": "posts/2025-03-28 from-heartbeats-to-dynamical-systems/index.html",
    "title": "From Heartbeats to Dynamical Systems",
    "section": "",
    "text": "Let’s talk about your heart. No, not the one that skips a beat when your crush texts back, that heart is a drama queen we’ll save for therapy. I’m talking about the actual meat-pump in your chest, the one that’s currently working overtime to process the existential dread of Monday mornings. For something so vital, it’s surprisingly bad at subtlety. When you exercise, it’s either “chill mode” (Netflix and nap) or “I’m about to audition for Speed 3: Cardiac Edition.” But here’s the kicker: scientists have spent decades trying to decode its mood swings, and let’s just say the results are… mixed. Enter my latest obsession: a math model so extra, it makes your gym’s heart rate monitor look like a Tamagotchi.\nPicture this: you’re on a treadmill, sweating like a popsicle in July, and your heart rate zigzags like a caffeinated EKG. Traditional methods to track this chaos? They’re like using a ruler to measure a squiggle. Heart rate variability (HRV) metrics, those bland averages your Fitbit spits out, are the equivalent of summarizing War and Peace as “some guys had feelings.” Sure, they tell you something, but they miss the juicy plot twists. Your heart isn’t a metronome; it’s a freestyle jazz musician with a caffeine problem. And when you throw exercise into the mix? Cue the autotuned chaos.\nNow, imagine if we could actually model this nonsense. Not with boring straight lines or sleepy exponentials, but with something that embraces the heart’s flair for melodrama. That’s where this new non-linear model struts in, wearing sunglasses indoors. It’s basically the Sherlock Holmes of heartbeats, solving the mystery of why your R-R intervals (the time between heartbeats, for the uninitiated) look like a toddler’s crayon drawing during a spin class. Using logistic functions, fancy curves that even your high school math teacher would side-eye, it maps out how your heart crashes during exercise and limps back to normal afterward. Think of it as a GPS for your cardiovascular system: “Recalculating… turn left at adrenaline alley.”\nBut why should you care? Well, unless you’re a cyborg, your autonomic nervous system (ANS) is the puppet master behind this circus. When you exercise, it’s a tug-of-war between “fight-or-flight” (sympathetic nervous system) and “nap time” (parasympathetic). Traditional models treat this like a polite debate. Spoiler: it’s not. It’s a WWE smackdown, complete with folding chairs. Linear models? They’re the guy at the party who insists the music isn’t that loud. This new model? It’s the one crowd-surfing while yelling, “I CAN EXPLAIN THE PLATEAU PHASE!”\nHere’s the plot twist: the model wasn’t born in a lab coat. It started as a desperate attempt to make sense of data from 272 elderly folks marching in place like it’s 1999. (Side note: If you ever want to feel better about your fitness, watch a senior citizen out-step you in a “light” exercise test.) The real magic? The model doesn’t just fit the data, it throws a interpretive dance party. By smooshing together two logistic functions (one for the crash, one for the comeback), it captures the heart’s U-shaped rollercoaster: freefall during exercise, slow climb back to sanity. Bonus points: the parameters have names like \\(\\alpha\\) and \\(\\beta\\), which sounds like a sorority for math nerds.\nOf course, no model is perfect. This one’s got quirks. For starters, it’s really into baseline RRI (\\(\\alpha\\)), the “resting heart rate” of the equation. According to Sobol sensitivity analysis, \\(\\alpha\\) is basically the Beyoncé of parameters, hogging 60% of the variance spotlight. Then there’s “c,” the recovery proportion, which is like the friend who says they’ll cover your Uber but only chips in 84%. The other parameters? They’re the background dancers. \\(\\lambda\\) and \\(\\phi\\) control the speed of crash and recovery, but apparently, nobody cares about speed when you’ve got drama queens like \\(\\alpha\\) stealing the show.\nNow, let’s address the elephant in the room: this model was tested on grandmas and grandpas. That’s right, your heart’s midlife crisis during Zumba? Not represented here. The elderly cohort is both a strength and a weakness. On one hand, they’re the ultimate stress test (imagine coding a model while your participants debate fiber intake). On the other, it’s like tuning a race car using data from a golf cart parade. But hey, if it works for them, it’ll probably work for you, unless you’re an Olympic sprinter, in which case, why are you reading a blog post? Go win a medal.\nThe real kicker? This model could change how we monitor hearts in real time. Imagine wearables that don’t just count steps but scream, “HEY, YOUR PARASYMPATHETIC SYSTEM IS SLACKING!” during meetings. Or rehab programs tailored like Spotify playlists, “Autonomic Acoustic” for recovery, “Sympathetic Screamo” for HIIT. The future is weird, folks.\nSo buckle up. We’re diving into the mathy, sweaty, gloriously chaotic world of heart rate modeling. Whether you’re a fitness junkie, a data nerd, or just here for the sarcasm, this model’s got something for you. Spoiler: It involves differential equations, but I promise to keep the calculus to a minimum. After all, your heart’s already doing enough heavy lifting."
  },
  {
    "objectID": "posts/2025-03-28 from-heartbeats-to-dynamical-systems/index.html#the-diet-coke-version-of-heart-math-same-chaos-fewer-calories",
    "href": "posts/2025-03-28 from-heartbeats-to-dynamical-systems/index.html#the-diet-coke-version-of-heart-math-same-chaos-fewer-calories",
    "title": "From Heartbeats to Dynamical Systems",
    "section": "The Diet Coke Version of Heart Math (Same Chaos, Fewer Calories)",
    "text": "The Diet Coke Version of Heart Math (Same Chaos, Fewer Calories)\nAlright, let’s take a breath. We’ve just spent a small eternity dissecting how your heart’s existential crisis during exercise can be modeled with nonlinear differential equations, math’s version of a Shakespearean tragedy. It’s got drama, depth, and enough quadratic terms to make your laptop sweat. But here’s the thing: sometimes you don’t need a five-act play about cardiac dynamics. Sometimes you just want a TikTok skit.\nEnter the simplified linear model. Think of it as the tutorial version of the original equation. It’s faster, lighter, and almost as accurate, like swapping a vintage wine for a LaCroix. But why bother simplifying something that’s already brilliant? Let’s talk about that.\n\nWhy Simplify?\nThe original dynamical system we built is a masterpiece. It’s got logistic decay, sigmoidal switches, and enough Greek letters to confuse a sorority. But here’s the problem: nonlinear equations are computational divas. They demand attention, processing power, and patience, three things you don’t have when you’re trying to simulate heartbeats in real time on a smartwatch powered by a potato battery.\nImagine you’re a fitness app developer. Your users want instant feedback on their heart rate recovery after a workout. The full model? It’s like asking them to wait while you solve a Rubik’s cube blindfolded. They’ll uninstall your app and go pet a dog instead.\nOr say you’re a researcher teaching undergrads about autonomic regulation. Throwing nonlinear differential equations at them is like explaining quantum physics using interpretive dance. They’ll nod politely while mentally planning their dropout party.\nThat’s where the simplified model shines. It’s the IKEA furniture of cardiac math: easy to assemble, good enough for most purposes, and unlikely to collapse unless you’re doing something weird. Let’s break down how we hacked the drama out of the equations.\n\n\nStep 1: Throw Out the Quadratic Drama (It’s Not You, It’s Math)\nThe original equations were nonlinear, a fancy way of saying they’re clingy and complicated. Take the exercise phase equation:\n\\[  \n\\frac{dx}{dt} = -\\lambda x \\left(1 - \\frac{x}{\\beta}\\right)  \n\\]\nThat \\(x^2\\) term? That’s the equation’s way of saying, “I’m not like other girls.” But if we assume your heart’s meltdown is controlled (i.e., \\(x^2\\) is close to its rock bottom \\(\\beta\\)), we can Taylor-expand this like it’s a meme.\nHere’s the math magic:\n\nDefine \\(\\Delta x = \\beta - x\\) (how far \\(x^2\\) is from its emo phase).\n\nPretend \\(\\Delta x\\) is small (like your attention span during a Zoom meeting).\n\nRewrite the equation in terms of \\(\\Delta x\\):\n\n\\[  \n\\frac{dx}{dt} \\approx -\\lambda (\\beta - x)  \n\\]\nTranslation: The panic fades linearly, like your enthusiasm for kale smoothies.\nSame trick for the recovery phase:\n\\[  \n\\frac{dy}{dt} = -\\phi y \\left(1 + \\frac{y}{c\\beta}\\right)  \n\\]\nDefine \\(\\Delta y = -c \\beta - y\\) (how far \\(y\\) is from redemption), assume \\(\\Delta y\\) is tiny, and boom:\n\\[  \n\\frac{dy}{dt} \\approx -\\phi (-c\\beta - y)  \n\\]\nNow the redemption arc is linear, like your progress bar during a software update.\n\n\nStep 2: Add Time-Out Switches (Because Even Hearts Need Boundaries)\nHearts don’t panic 24/7 (unless you’re a Wall Street trader). To control when the drama unfolds, we use sigmoidal activation functions, smooth on/off switches that say, “Not now, sweetie.”\n\nExercise switch:\n\n\\[  \n\\psi_{\\text{exercise}}(t) = \\frac{1}{1 + e^{-\\lambda(t - \\tau)}}  \n\\]\n\nRecovery switch:\n\n\\[  \n\\psi_{\\text{recovery}}(t) = \\frac{1}{1 + e^{-\\phi(t - (\\tau + \\delta))}}  \n\\]\nThese switches act like parental controls for your heart rate. Before τ (exercise o’clock), ψ_exercise ≈ 0: “No cardio before coffee.” After τ + δ (nap o’clock), ψ_recovery ≈ 1: “OK, you can stop dying now.”\nSlap these into the linear equations:\n\\[  \n\\frac{dx}{dt} = -\\lambda (\\beta - x) \\cdot \\psi_{\\text{exercise}}(t)  \n\\]\n\\[  \n\\frac{dy}{dt} = -\\phi (-c\\beta - y) \\cdot \\psi_{\\text{recovery}}(t)  \n\\]\nNow the meltdown and redemption only happen when scheduled. No surprises, like a Netflix show that actually respects your bedtime.\n\n\nStep 3: Why Bother? (Trade-Offs for the Attention-Span Deprived)\nPros of the Diet Model:\n\nSpeed: Simulates faster than you can say “existential crisis.” Perfect for real-time apps where users tap their feet waiting for results.\n\nStability: No chaotic surprises. It’s like replacing a rollercoaster with a escalator.\n\nTransparency: Parameters slap you in the face with meaning. λ? Panic speed. φ? Recovery laziness. No PhD required.\n\nCons:\n\nBlandness: Loses the spicy logistic saturation. It’s the difference between a rom-com and a spreadsheet.\n\nAccuracy: Only works near equilibrium. Stray too far, and it’s like using a ruler to measure a tornado.\n\n\n\nWhen to Use Which Model (Choose Your Own Adventure)\nFull Dynamical System:\n\nFor Researchers: If you’re publishing a paper titled “Nonlinear Autonomic Oscillations in Elderly Step-Test Cohorts: A Bayesian Perspective,” this is your jam.\n\nDeep Analysis: Want to explore how λ and φ interact during a HIIT workout? Go nuts.\n\nFlexibility: Captures the full cardiac soap opera, breakdowns, comebacks, and plot twists.\n\nSimplified Linear Model:\n\nFor Developers: Building a fitness app? Use this. Your servers won’t cry.\n\nEducation: Teaching undergrads? They’ll thank you for not melting their brains.\n\nQuick Simulations: Need to model 10,000 heartbeats before lunch? This’ll do.\n\n\n\nThe Big Picture: Why Simplification Isn’t Sacrilege\nLet’s be real: the original model is a Ferrari. It’s sleek, powerful, and turns heads at academic conferences. But most of us don’t need a Ferrari to drive to Walmart. Sometimes a Toyota Corolla (read: linear model) gets the job done without the maintenance fees.\nSimplification isn’t about dumbing things down, it’s about pragmatism. The world runs on approximations. Weather apps? They use simplified models. GPS? Simplified models. Your mom’s meatloaf recipe? Definitely simplified.\nThe key is knowing when to simplify. If you’re designing a pacemaker? Stick with the full model. If you’re coding a heart rate widget for a smartwatch? The linear version is your friend.\n\n\nThe Heart’s Many Faces\nAt the end of the day, your heart is a drama queen with range. Sometimes it’s method-acting through a spin class, other times it’s phoning it in during a Zoom meeting. The models we’ve built, both the full nonlinear saga and its streamlined sibling, are just tools to decode the performance.\nSo next time you’re on a treadmill, remember: your heart isn’t just beating. It’s starring in a math rock opera, complete with logistic plot twists and linear encores. And whether you’re team Ferrari or team Corolla, there’s a model for that."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Bayesically Speaking\n\n\nPriors & Coffee\n\n\n\n\n\n\n\nWhat is this?\nIf you are curious about how to use data and probability to understand and visualize real world problems, you have come to the right place.\n About  Related work\n\n\n\n\n\n\nLatest posts\n See all posts\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSo, You Think You Can Model a Heartbeat?\n\n\n32 min\n\n\nYour heart isn’t a metronome, it’s a chaotic jazz drummer. Dive with me into the math madness behind why your heartbeat skyrockets during burpees and how a non-linear model…\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoing NUTS: A Step-by-Step Guide to Adaptive Hamiltonian Sampling in R\n\n\n41 min\n\n\nYou may have wonder what’s the engine behind many Bayesian analysis in modern research? Well, in this post we’ll not only cover the fundamentals, but we’ll also build from…\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Neuron’s Spark: Building Intuition for Action Potential Dynamics\n\n\n35 min\n\n\nAction potentials are the language of neurons. In this post we build a simplified model to explain how these electrical signals are generated. We’ll explore the role of ion…\n\n\n\nJan 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations\n\n\n40 min\n\n\nIn this post, we’ll take you step by step through building and playing with a model of lymphocyte dynamics. So, grab your keyboard, and let’s whip up a recipe for…\n\n\n\nDec 23, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNeed asistance with your analyses?\nWhether you need assistance performing complex statistical analysis for your project or simply need help wrapping your head around some statistics concepts, our team is ready to assist you.\n Check our services  Contact us"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Portfolio",
    "section": "",
    "text": "Portfolio"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Portfolio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChilean Austral Molecular Integrative Neurophysiology by its acronym in Spanish.↩︎"
  }
]