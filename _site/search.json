[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte What?\n\n\n\n\n\n\nbasics\n\n\nmcmc\n\n\n\nIn this post we will delve into the main idea behind Markov Chain Monte Carlo (MCMC for short) and why it is useful within the bayesian inference framework. \n\n\n\n\n\nApr 25, 2024\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Bayesically Speaking\n\n\n\n\n\n\nnews\n\n\n\nHi everyone! This is the first post of Bayesically Speaking, so get your seatbelt on and get ready to join me on this ride! \n\n\n\n\n\nJun 10, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "about.html#what-is-all-the-fuzz-about",
    "href": "about.html#what-is-all-the-fuzz-about",
    "title": "About",
    "section": "What is all the fuzz about?",
    "text": "What is all the fuzz about?\nHello and welcome to Bayesically Speaking, the blog where I, Matías, share my passion for statistics, bayesian methods and coffee. If you are curious about how to use data and probability to understand and solve real world problems, you have come to the right place. Here you will find practical examples, tutorials, tips and tricks on how to apply statistical thinking and inference to various domains and scenarios."
  },
  {
    "objectID": "about.html#who-is-this-guy",
    "href": "about.html#who-is-this-guy",
    "title": "About",
    "section": "Who is this guy?",
    "text": "Who is this guy?\nWho am I and why should you care? Well, I am a researcher who loves numbers and coffee (not necessarily in that order). I have a special interest in bayesian methods, which are a powerful and flexible way of doing statistics that allows you to incorporate prior knowledge and uncertainty into your analysis. Bayesian methods are not magic, though. They have their limitations and challenges, just like any other approach. That’s why I don’t shy away from using other tools when they are appropriate, such as frequentist methods or p-values. My goal is not to start a war between different schools of thought, but to show you how to use the best tool for the job."
  },
  {
    "objectID": "about.html#a-secret-weapon",
    "href": "about.html#a-secret-weapon",
    "title": "About",
    "section": "A secret weapon",
    "text": "A secret weapon\nIn this blog, you will also learn how to create beautiful and informative graphics and plots using R, which is a free and open source software for data analysis and visualization. R is my favorite tool for doing statistics, because it has a huge community of users and developers who create amazing packages and resources for all kinds of purposes. R can also do much more than just statistics, such as web scraping, text mining, machine learning and more. Even this website was build using R!"
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html",
    "href": "posts/2024-04-14 mcmc part 1/index.html",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "Alright, folks, let’s dive into the wild world of statistics and data science! Picture this: you’re knee-deep in data, trying to make sense of the chaos. But here’s the kicker, sometimes the chaos is just too darn complex. With tons of variables flying around, getting a grip on uncertainty can feel like trying to catch smoke with your bare hands.\nPlease, have in your consideration that the kind of problems that we’re dealing with, it’s not solely related to the number of dimensions, it’s mostly related to trying to estimate something that we can’t see in full beforehand. For instance, consider the following banana distribution (shown below). How could we map this simple two dimensional surface without computing it all at once?\n\n\nCode\ndbanana &lt;- function(x) {\n  a = 2;\n  b = 0.2;\n  \n  y = x / a\n  y = (a * b) * (x^2 + a^2)\n}\n\nx &lt;- seq(-6, 6, length.out = 300)\n\ny = dbanana(x)\n\nz &lt;- MASS::kde2d(x, y, n = 100, lims = c(-10, 10, -2.6, 20))\n\nplot_ly(x = z$x, y = z$y, z = sqrt(z$z)) |&gt; \n  add_surface() |&gt; \n  style(hoverinfo = \"none\")\n\n\n\n\n\n\n\n\nYou know when you hit a roadblock in your calculations, and you’re like, “Can’t we just crunch the numbers for every single value?” Well, let’s break it down. Picture a grid with \\(N\\) points for \\(D\\) dimensions. Now, brace yourself, ’cause the math needed is like \\(N\\) raised to the power of \\(D\\).\nSo, let’s say you wanna estimate 100 points (to get a decent estimation of the shape) for each of 100 dimensions. That’s like slamming your head against ten to the power of 200 computations… that’s a hell of a lot of computations!\nSure, in la-la land, you could approximate every single number with some degree of approximation. But let’s get real here, even if you had all the time in the world, you’d still be chipping away at those calculations until the sun swallowed the Earth, especially with continuous cases and tons of dimensions that are somewhat correlated (which in reality, tends to be the case).\nThis headache we’re dealing with? It’s what we “affectionately” call — emphasis on double quotes — the curse of dimensionality. It’s like trying to squeeze a square peg into a round hole… it ain’t gonna happen without a supersized hammer!\n\n\nCode\ncurse_dimensionality &lt;- data.frame(dimensions = factor((1:10)^2),\n                                   calculations = 100^((1:10)^2))\n\nggplot(curse_dimensionality, aes(dimensions, calculations)) +\n  geom_col(fill = ggsci::pal_jama()(1)) +\n  scale_y_continuous(transform = \"log10\", n.breaks = 9,\n                     labels = scales::label_log(), expand = c(0,0,.1,0)) +\n  labs(y = \"Computations (log-scale)\", x = \"Dimensions (Variables)\",\n       title = \"Computations needed to compute a grid of 100 points\",\n       subtitle = \"As a funtion of dimensions/variables involved\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nIllustration of computations needed (in log-scale) for 100 points as a function of dimensions considered.\n\n\n\n\n\n\n\n\n\n\nExplaining the curse of dimensionality further\n\n\n\n\n\nImagine you’re trying to create a grid to map out the probability space for a set of variables. As the number of dimensions increases, the number of grid points needed to adequately represent the space explodes exponentially. This means that even with the most powerful computers, it becomes practically impossible to compute all the probabilities accurately.\n\n\n\n\n\n\nNow, if we can’t crack the problem analytically (which, let’s face it, is the case most of the time), we gotta get creative. Lucky for us, there’s a bunch of algorithms that can lend a hand by sampling this high-dimensional parameter space. Enter the Markov Chain Monte Carlo (MCMC) family of algorithms.\nBut hold up—Markov Chain Monte What? Yeah, it’s a mouthful, but bear with me. You’re probably wondering how this fancy-schmancy term is connected to exploring high-dimensional probability spaces. Well, I’ll let you in on the secret sauce behind these concepts and why they’re the go-to tools in top-notch probabilistic software like Stan.\nBut before we get into the nitty-gritty of MCMC, let’s take a detour and talk about Markov Chains, because they’re like the OGs of this whole MCMC gang."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#just-put-a-darn-grid-to-it",
    "href": "posts/2024-04-14 mcmc part 1/index.html#just-put-a-darn-grid-to-it",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "You know when you hit a roadblock in your calculations, and you’re like, “Can’t we just crunch the numbers for every single value?” Well, let’s break it down. Picture a grid with \\(N\\) points for \\(D\\) dimensions. Now, brace yourself, ’cause the math needed is like \\(N\\) raised to the power of \\(D\\).\nSo, let’s say you wanna estimate 100 points (to get a decent estimation of the shape) for each of 100 dimensions. That’s like slamming your head against ten to the power of 200 computations… that’s a hell of a lot of computations!\nSure, in la-la land, you could approximate every single number with some degree of approximation. But let’s get real here, even if you had all the time in the world, you’d still be chipping away at those calculations until the sun swallowed the Earth, especially with continuous cases and tons of dimensions that are somewhat correlated (which in reality, tends to be the case).\nThis headache we’re dealing with? It’s what we “affectionately” call — emphasis on double quotes — the curse of dimensionality. It’s like trying to squeeze a square peg into a round hole… it ain’t gonna happen without a supersized hammer!\n\n\nCode\ncurse_dimensionality &lt;- data.frame(dimensions = factor((1:10)^2),\n                                   calculations = 100^((1:10)^2))\n\nggplot(curse_dimensionality, aes(dimensions, calculations)) +\n  geom_col(fill = ggsci::pal_jama()(1)) +\n  scale_y_continuous(transform = \"log10\", n.breaks = 9,\n                     labels = scales::label_log(), expand = c(0,0,.1,0)) +\n  labs(y = \"Computations (log-scale)\", x = \"Dimensions (Variables)\",\n       title = \"Computations needed to compute a grid of 100 points\",\n       subtitle = \"As a funtion of dimensions/variables involved\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nIllustration of computations needed (in log-scale) for 100 points as a function of dimensions considered.\n\n\n\n\n\n\n\n\n\n\nExplaining the curse of dimensionality further\n\n\n\n\n\nImagine you’re trying to create a grid to map out the probability space for a set of variables. As the number of dimensions increases, the number of grid points needed to adequately represent the space explodes exponentially. This means that even with the most powerful computers, it becomes practically impossible to compute all the probabilities accurately."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#sampling-the-unknown-markov-chain-monte-carlo",
    "href": "posts/2024-04-14 mcmc part 1/index.html#sampling-the-unknown-markov-chain-monte-carlo",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "Now, if we can’t crack the problem analytically (which, let’s face it, is the case most of the time), we gotta get creative. Lucky for us, there’s a bunch of algorithms that can lend a hand by sampling this high-dimensional parameter space. Enter the Markov Chain Monte Carlo (MCMC) family of algorithms.\nBut hold up—Markov Chain Monte What? Yeah, it’s a mouthful, but bear with me. You’re probably wondering how this fancy-schmancy term is connected to exploring high-dimensional probability spaces. Well, I’ll let you in on the secret sauce behind these concepts and why they’re the go-to tools in top-notch probabilistic software like Stan.\nBut before we get into the nitty-gritty of MCMC, let’s take a detour and talk about Markov Chains, because they’re like the OGs of this whole MCMC gang."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#converging-to-an-answer",
    "href": "posts/2024-04-14 mcmc part 1/index.html#converging-to-an-answer",
    "title": "Markov Chain Monte What?",
    "section": "Converging to an answer",
    "text": "Converging to an answer\nNow, let’s imagine letting time run. After a year passes, if we observe how the weather behaves, we’ll notice that the relative frequencies of each state tend to converge to a single number.\nNow, fast forward a year. If we keep an eye on the weather every day, we’ll notice something interesting: the relative frequencies of rainy and sunny days start to settle into a rhythm. This steady state is what we call a stationary distribution. It’s like the true probability of what the weather’s gonna be like in the long run, taking into account all the different scenarios.\n\n\nCode\nsimulate_weather &lt;- function(total_time) {\n  \n  weather &lt;- vector(\"character\", total_time) # Create slots for each day\n  day &lt;- 1 # First day\n  weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1) # Weather for first day\n  \n  while (day &lt; total_time) {\n    day &lt;- day + 1 # Add one more day\n    if (weather[day] == \"Rainy\") {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.6, .4))\n    } else {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.3, .7))\n    }\n  }\n  \n  return(weather)\n}\n\nsim_time &lt;- 365*1\nweather &lt;- simulate_weather(total_time = sim_time)\n\nweather_data &lt;- data.frame(\n  prop = c(cumsum(weather == \"Rainy\") / seq_len(sim_time), cumsum(weather == \"Sunny\") / seq_len(sim_time)),\n  time = c(seq_len(sim_time), seq_len(sim_time)),\n  weather = c(rep(\"Rainy\", times = sim_time), rep(\"Sunny\", times = sim_time))\n)\n\nggplot(weather_data, aes(time, prop, fill = weather)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::label_percent(), n.breaks = 6,\n                     name = \"Proportion of each weather\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Days\", n.breaks = 10, expand = c(0,0)) +\n  scale_fill_brewer(type = \"qual\", palette = 3) +\n  labs(fill = \"Weather\", title = \"Convergence to stationary distribution\",\n       subtitle = \"Based on cumulative proportion of each Sunny or Rainy days\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean proportion of sunny/rainy days across 365 days. Right pass the 100 days, the proportion of rainy/sunny days tends to display a stable trend when we averaged the previous days. This is known as stationary distribution.\n\n\n\n\nThis heuristic allows us to naturally converge to an answer without needing to solve it analytically, which tends to be useful for really complex and high-dimensional problems.\nSure, we could’ve crunched the numbers ourselves to figure out these probabilities. But why bother with all that math when we can let time do its thing and naturally converge to the same answer? Especially when we’re dealing with complex problems that could have given even Einstein himself a headache.\n\n\n\n\n\n\nExplaining the convergence process further\n\n\n\n\n\nThe idea of convergence to a stationary distribution can be likened to taking a random walk through the space of possible outcomes. Over time, the relative frequencies of each outcome stabilize, giving us a reliable estimate of the true probabilities.\n\n\n\nAs we’ve seen, sometimes it becomes impractical to solve analytically or even approximate the posterior distribution using a grid, given the number of calculations needed to even get a decent approximation of the posterior.\nHowever, we’ve also seen that Markov Chains might offer us a way to compute complex conditional probabilities and, if we let them run long enough, they will eventually converge to the stationary distribution, which could resemble the posterior distribution itself. So, all things considered, when does the Monte Carlo part come in?"
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#what-is-mcmc-actually-doing",
    "href": "posts/2024-04-14 mcmc part 1/index.html#what-is-mcmc-actually-doing",
    "title": "Markov Chain Monte What?",
    "section": "What is MCMC actually doing?",
    "text": "What is MCMC actually doing?\nIn essence, MCMC is an algorithm that generates random samples from a proposal distribution. These samples are accepted or rejected based on how much more likely the proposed sample is compared to the previous accepted sample.\nIn this way, the proposed samples are accepted in the same proportion as the actual probability in the target distribution, accepting more samples that are more likely and fewer samples that are less likely.\nThe fascinating nature of this heuristic is that it works to approximate complex distributions without needing to know much about the shape of the final distribution.\nSo, think of it as trekking through this complex landscape, taking random steps (the Monte Carlo part) but guided by the likelihood of each move, given where you currently stand (the Markov Chain part). It’s a meticulous journey, but one that ultimately leads us to a better understanding of these elusive distributions.\nFor instance, consider that we have a distribution (shown below) that we can’t to compute, because it would take too long to integrate the whole function. This will be our target distribution, from which we can only compute the density of one value at a time.\n\n\n\n\n\n\nAbout the target distribution\n\n\n\n\n\nIn practice, we would derive the target distribution from the data and prior information, this enable us to estimate the density in a point-wise manner, without the need to estimate the whole PDF all at once. But for the sake of demonstration we will the use the Gamma probability density function.\nHowever, please consider that you can’t use some family distribution to describe perfectly any probability density, sometimes it can be a mixture of distributions, truncation, censoring. All comes down to the underlying process that generates the data that we are trying to mimic.\n\n\n\n\n\nCode\n# Target distribution that we in practice would derive from\n# the data.\ntarget_dist &lt;- function(i) dgamma(i, shape = 2, scale = 1)\n\nggplot() +\n  stat_function(fun = target_dist,\n                xlim = c(0, 11), geom = \"area\", \n                fill = \"#374E55FF\") +\n  scale_y_continuous(breaks = NULL, name = \"Density\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Some scale\", expand = c(0,0)) +\n  theme_classic(base_size = 20)\n\n\n\n\n\nThis is a Gamma distribution with shape of 2 and scale of 1. We will try to estimate it.\n\n\n\n\nNext thing to do is to specify a proposal distribution, from which we’ll generate proposals for the next step. To this end we’ll be using a Normal density function with \\(\\mu\\) = 0 and \\(\\sigma\\) = 1.\n\n# This is a function that will generate proposals for the next step.\nproprosal &lt;- function() rnorm(1, mean = 0, sd = 1)\n\nAnd set some algorithm parameters that are necessary for our MCMC to run:\n\n## Algorithm parameters ----\n\ntotal_steps &lt;- 1000 # Total number of steps\nstep &lt;- 1 # We start at step 1\nvalue &lt;- 10 # set a initial starting value\n\nFinally, we run our algorithm as explained in previous sections. Try to follow the code to get an intuition of what is doing.\n\n## Algorithm ----\n\nset.seed(1234) # Seed for reproducibility\nwhile(step &lt; total_steps) {\n  # Increase for next step\n  step &lt;- step + 1\n  \n  ## 1. Propose a new value ----\n  \n  # Proposal of the next step is ...\n  value[step] &lt;- \n    # the previous step plus...\n    value[step - 1L] + \n    # a change in a random direction (based on the \n    # proposal distribution)\n    proprosal() \n  \n  ## 2. We see if the new value is more or less likely ----\n  \n  # How likely (in the target distribution)\n  likelihood &lt;- \n    # is the proposed value compared to the previous step\n    target_dist(value[step]) / target_dist(value[step - 1L]) \n  \n  ## 3. Based on its likelihood, we accept or reject it ----\n  \n  # If the proposal value is less likely, we accept it only \n  # to the likelihood of the proposed value\n  if (likelihood &lt; runif(1)) \n    value[step] &lt;- value[step - 1L]\n  \n  # Then we repeat for the next step\n}\n\nFinally, let’s explore how well our algorithm converge to the target distribution.\n\n\nCode\nmcmc &lt;- data.frame(\n  step = seq_len(step),\n  value = value\n)\n\nggplot(mcmc, aes(x = step, y = value)) +\n  geom_line(col = \"#374E55FF\") +\n  ggside::geom_ysidehistogram(aes(x = -after_stat(count)), fill = \"#374E55FF\", binwidth = .3) +\n  ggside::geom_ysidedensity(aes(x = -after_stat(count)*.35), col = \"#374E55FF\") +\n  ggside::scale_ysidex_continuous(expand = c(0,0,0,.1), breaks = NULL) +\n  scale_x_continuous(expand = c(0,0), name = \"Step\") +\n  scale_y_continuous(name = NULL, position = \"right\") +\n  labs(title = \"Trace of MCMC values to target distribution\",\n       subtitle = \"Evolution of values at each step\") +\n  theme_classic(base_size = 20) +\n  ggside::ggside(y.pos = \"left\") +\n  theme(ggside.panel.scale = .4)\n\n\n\n\n\nTraceplot of convergence of MCMC for 1000 steps. With increasing steps we see an increasing resemblance to the target distribution.\n\n\n\n\nAnother thing that we care is to see how well our MCMC is performing. After all, if not, then what would be the point of using it in first place? To check this, we’ll compare the expectation (\\(E(X)\\)) of the target distribution[^1] against the posterior derived from our MCMC.\nFor this, we have to consider that the expectation, \\(E(X)\\), of any Gamma distribution is equal to the shape parameter (\\(\\alpha\\)) times by the scale parameter (\\(\\sigma\\)). We could express the aforementioned the following.\n\n\\(\\begin{aligned}\n  E(X) &= \\alpha \\sigma \\\\\n  \\text{with}~X &\\sim \\text{Gamma}(\\alpha, \\sigma)\n\\end{aligned}\\)\n\n\n\nCode\nggplot(mcmc, aes(x = step, y = cumsum(value)/step)) +\n  geom_line(col = \"#374E55FF\") +\n  scale_x_continuous(expand = c(0,.1), name = \"Steps (log-scale)\", \n                     transform = \"log10\", labels = scales::label_log()) +\n  scale_y_continuous(name = NULL, expand = c(0, 1)) +\n  labs(title = \"Convergence to location parameter\",\n       subtitle = \"Cumulative mean across steps\") +\n  geom_hline(aes(yintercept = 2), col = \"darkred\") +\n  geom_hline(aes(yintercept = mean(value)), lty = 2) +\n  annotate(x = 1.5, xend = 1.1, y = 7.5, yend = 9.5, geom = \"curve\", curvature = -.2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = 2, y = 6.8,  label = \"Initial value\", size = 5, geom = \"text\") +\n  annotate(x = (10^2.5), xend = (10^2.6), y = 5, yend = 2.5, geom = \"curve\", curvature = .2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = (10^2.5), y = 5.8,  label = \"Convergence\", size = 5, geom = \"text\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean of the posterior distributions across steps, compared to the empirical mean of the target distribution. Here the dark red line represents the empirical location parameter and the dashed line the one estimated using MCMC."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#gibbs-sampling-a-buffet-adventure",
    "href": "posts/2024-04-14 mcmc part 1/index.html#gibbs-sampling-a-buffet-adventure",
    "title": "Markov Chain Monte What?",
    "section": "Gibbs Sampling: A Buffet Adventure",
    "text": "Gibbs Sampling: A Buffet Adventure\nImagine you’re at a buffet with stations offering various cuisines — Italian, Chinese, Mexican — you name it. You’re on a mission to create a plate with a bit of everything, but here’s the catch: you can only visit one station at a time. Here’s how you tackle it:\n\nHit up a station and randomly pick a dish.\nMove on to the next station and repeat the process.\nKeep going until you’ve got a plateful of diverse flavors.\n\nGibbs sampling works kind of like this buffet adventure. You take turns sampling from conditional distributions, just like you visit each station for a dish. Each time, you focus on one variable, updating its value while keeping the others constant. It’s like building your plate by sampling from each cuisine until you’ve got the perfect mix."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#hamiltonian-monte-carlo-charting-your-hiking-path",
    "href": "posts/2024-04-14 mcmc part 1/index.html#hamiltonian-monte-carlo-charting-your-hiking-path",
    "title": "Markov Chain Monte What?",
    "section": "Hamiltonian Monte Carlo: Charting Your Hiking Path",
    "text": "Hamiltonian Monte Carlo: Charting Your Hiking Path\nPicture yourself hiking up a rugged mountain with rocky trails and valleys. Your goal? Reach the summit without breaking a sweat—or falling off a cliff. So, you whip out your map and binoculars to plan your route:\n\nStudy the map to plot a path with minimal uphill battles and maximum flat stretches.\nUse the binoculars to scout ahead and avoid obstacles along the way.\nAdjust your route as you go, smoothly navigating the terrain like a seasoned pro.\n\nHamiltonian Monte Carlo (HMC) is a bit like this hiking adventure. It simulates a particle moving through a high-dimensional space, using gradient info to find the smoothest path. Instead of blindly wandering, HMC leverages the curvature of the target distribution to explore efficiently. It’s like hiking with a GPS that guides you around the rough spots and straight to the summit."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#strengths-weaknesses-and-real-world-applications",
    "href": "posts/2024-04-14 mcmc part 1/index.html#strengths-weaknesses-and-real-world-applications",
    "title": "Markov Chain Monte What?",
    "section": "Strengths, Weaknesses, and Real-World Applications",
    "text": "Strengths, Weaknesses, and Real-World Applications\nNow that you’ve dipped your toes into the MCMC pool, it’s time to talk turkey—well, sampling. Each MCMC method has its perks and quirks, and knowing them is half the battle.\nGibbs sampling is the laid-back surfer dude of the group—simple, chill, and great for models with structured dependencies. But throw in some highly correlated variables, and it starts to wobble like a rookie on a surfboard.\nMeanwhile, HMC is the sleek Ferrari—efficient, powerful, and perfect for tackling complex models head-on. Just don’t forget to fine-tune those parameters, or you might end up spinning out on a sharp curve."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#key-differences",
    "href": "posts/2024-04-14 mcmc part 1/index.html#key-differences",
    "title": "Markov Chain Monte What?",
    "section": "Key Differences",
    "text": "Key Differences\n\nSampling Approach\n\nMetropolis-Hastings: Takes random walks to generate samples, with acceptance based on a ratio of target distribution probabilities.\nGibbs Sampling: Updates variables one by one based on conditional distributions, like a tag team wrestling match.\nHamiltonian Monte Carlo: Glides through high-dimensional space using deterministic trajectories guided by Hamiltonian dynamics, like a graceful dancer in a crowded room.\n\n\n\nEfficiency and Exploration\n\nMetropolis-Hastings: Easy to implement but might struggle to explore efficiently, especially in high-dimensional spaces.\nGibbs Sampling: Perfect for structured models but may stumble with highly correlated variables.\nHamiltonian Monte Carlo: Efficiently navigates high-dimensional spaces, leading to faster convergence and smoother mixing.\n\n\n\nAcceptance Criterion\n\nMetropolis-Hastings: Decides whether to accept or reject proposals based on a ratio of target distribution probabilities.\nGibbs Sampling: Skips the acceptance drama and generates samples directly from conditional distributions.\nHamiltonian Monte Carlo: Judges proposals based on the joint energy of position and momentum variables, like a strict dance instructor.\n\n\n\nParameter Tuning and Complexity\n\nMetropolis-Hastings: Requires tweaking the proposal distribution but keeps it simple.\nGibbs Sampling: A breeze to implement, but watch out for those conditional distributions—they can be sneaky.\nHamiltonian Monte Carlo: Needs tuning of parameters like step size and trajectory length, and the implementation might get a bit hairy with momentum variables and gradient computation."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#mcmc-in-action",
    "href": "posts/2024-04-14 mcmc part 1/index.html#mcmc-in-action",
    "title": "Markov Chain Monte What?",
    "section": "MCMC in action",
    "text": "MCMC in action\nIn the following, you can see an interactive animation of different MCMC algorithms (MH, Gibbs and HMC) and how they work to uncover distributions in two dimensions. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo"
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#implementing-mcmc-algorithms-in-practice",
    "href": "posts/2024-04-14 mcmc part 1/index.html#implementing-mcmc-algorithms-in-practice",
    "title": "Markov Chain Monte What?",
    "section": "Implementing MCMC Algorithms in Practice",
    "text": "Implementing MCMC Algorithms in Practice\nAlright, theory’s cool and all, but let’s get down to brass tacks. When you’re rolling up your sleeves to implement MCMC algorithms, it’s like picking the right tool for the job. Simple models? Metropolis-Hastings or Gibbs sampling has your back. But when you’re wrangling with the big boys—those complex models—that’s when you call in Hamiltonian Monte Carlo. It’s like upgrading from a rusty old wrench to a shiny new power tool. And don’t forget about tuning those parameters — it’s like fine-tuning your car for a smooth ride.\nBeyond all the technical jargon, successful Bayesian inference is part gut feeling, part detective work. Picking the right priors is like seasoning a dish — you want just the right flavor without overpowering everything else. And tuning those parameters? It’s like fine-tuning your favorite instrument to make sure the music hits all the right notes."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html",
    "href": "posts/2023-05-30 welcome/index.html",
    "title": "Welcome to Bayesically Speaking",
    "section": "",
    "text": "Photo from Jon Tyson at Unsplash."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#the-statistics-toolbox",
    "href": "posts/2023-05-30 welcome/index.html#the-statistics-toolbox",
    "title": "Welcome to Bayesically Speaking",
    "section": "The statistics toolbox",
    "text": "The statistics toolbox\nWithin the statistics toolbox, we have commonly used tests like t-tests, ANOVA, correlations, and regression. These methods have their advantages, as they are relatively easy to use and understand. However, like any other tool, they also have their limitations. For instance, they struggle with scenarios involving variables with asymmetric distributions, non-linear relationships, unbalanced groups, heterogeneous variance, extreme values, or repeated measurements with loss of follow-up.\nTo address these limitations, non-parametric alternatives have been developed. These approaches offer flexibility but make it challenging to extrapolate inferences to new data due to the lack of distributional parameters. Other models, such as neural networks or random forest models, provide assistance when analyzing data with special properties. However, they often sacrifice simplicity and interpretability for increased flexibility and are commonly referred to as “black box” models.\n\n\n\nJust between us, I only put this picture because it looked cool. Photo from Dan Cristian Pădureț at Unsplash.\n\n\nDespite the availability of these alternative methods, there is still a pressing need to incorporate previous knowledge and align with the way human understanding is constructed. As humans, our perception of the world is shaped by experiences and prior beliefs. This is where Bayesian statistics come into play.\nBayesian statistics offer several advantages over classical statistics (also known as “frequentist”). Firstly, they provide a coherent framework for incorporating prior information into our analysis, enabling us to update our beliefs systematically. Additionally, Bayesian statistics allow us to quantify uncertainty through probability distributions, offering a more intuitive and interpretable way to express our findings and the degree of certainty."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#computing-the-posterior",
    "href": "posts/2023-05-30 welcome/index.html#computing-the-posterior",
    "title": "Welcome to Bayesically Speaking",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nTo estimate the posterior probability of getting heads after tossing a coin, we can use the Bayesian framework. Let’s denote the probability of getting heads in a coin toss as \\(P(H)\\).\nAccording to the information provided, we have the prior probability of \\(P(H)\\) estimated from an independent experiment as 10 heads out of 15 tosses. This can be written as a Beta distribution:\n\n\nThis symbol “\\(\\sim\\)” means distributed as\n\\[\nP(H) \\sim Beta(10, 5)\n\\]\nHere, the Beta distribution parameters are (10, 5) since we had 10 heads and 5 tails in the prior experiment.\nNow, a new experiment with the same 15 tosses gives us 2 heads. To update our prior belief, we can use this information to calculate the posterior probability which can be expressed as follow:\n\n\nThis symbol “\\(\\propto\\)” means proportional to\n\\[\nP(H | Data) \\propto P(Data | H) \\times P(H)\n\\]\nWhich is equivalent as saying:\n\\[\nPosterior \\propto Likelihood \\times Prior\n\\]\nTo calculate the posterior probability, we need to normalize the product of the likelihood and prior, which involves integrating over all possible values of H. However, in this case, we can use a shortcut because the prior distribution is conjugate to the binomial distribution, so the posterior distribution will also follow a Beta distribution:\n\n\n\n\n\n\nAbout normalization\n\n\n\n\n\nThe product of both the prior and the likelihood maintains the same shape as the final posterior probability distribution, indicated by the “proportional to” (\\(\\propto\\)) in the previous equation. However, this raw product does not sum up to 1, making it an improper probability density function. To rectify this, the raw product needs to be normalized using integration or simulation in most cases.\n\n\n\n\\[\nP(H | Data) \\sim Beta(10 + 2, 5 + 13)\n\\]\nAfter incorporating the data from the new experiment, the parameters of the Beta distribution become (12, 18) since we had 2 heads and 13 tails in the new experiment, meaning 12 heads and 18 tails in total.\n\n\n\n\n\n\nAbout conjugacy\n\n\n\n\n\nWhen we choose a Beta distribution as our prior belief and gather new data from a coin toss, an intriguing property emerges: the posterior distribution also follows a Beta distribution. This property, known as conjugacy, offers a valuable advantage by simplifying calculations. It acts as a mathematical shortcut that saves time and effort, making the analysis more efficient and streamlined.\n\n\n\nTo calculate the posterior probability of getting heads, we can consider the mode (maximum) of the Beta distribution, which is \\((a - 1) / (a + b - 2)\\):\n\n\\(\\begin{aligned}\nP(H | Data)    &= (12 - 1) / (12 + 18 - 2) \\\\\n               &= 11 / 28 \\\\\n               &\\approx 0.39\n\\end{aligned}\\)\n\nTherefore, the posterior probability of getting heads is approximately 39% when we consider all the available evidence.\n\n\nCode\n# Prior and Likelihood functions\ndata = function(x, to_log = FALSE) dbeta(x, 2, 13, log = to_log)\nprior = function(x, to_log = FALSE) dbeta(x, 10, 5, log = to_log)\n\n# Posterior\nposterior = function(x) {\n  p_fun = function(i) {\n    # Operation is on log-scale merely for computing performance\n    # and minimize rounding errors giving the small nature of\n    # probability density values at each interval.\n    i_log = data(i, to_log = TRUE) + prior(i, to_log = TRUE)\n    # Then transformed back to get probabilities again\n    exp(i_log)\n  }\n  \n  # Then we integrate using base function `integrate`\n  const = integrate(f = p_fun, \n                    lower = 0L,  upper = 1L, \n                    subdivisions = 1e3L,\n                    rel.tol = .Machine$double.eps)$value\n  p_fun(x) / const\n}\n\n## Plotting phase\n\n### Color palette\ncol_pal &lt;- c(Prior = \"#DEEBF7\", \n             Data = \"#3182BD\", \n             Posterior = \"#9ECAE1\")\n### Main plotting code\nggplot() +\n  #### Main probability density functions\n  stat_function(aes(fill = \"Data\"), fun = data, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Prior\"), fun = prior, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Posterior\"), fun = posterior, geom = \"density\", alpha = 1/2) +\n  #### Minor aesthetics tweaks\n  labs(fill = \"\", y = \"Density\", x = \"Probability of getting heads\") +\n  scale_fill_manual(values = col_pal, aesthetics = \"fill\") +\n  scale_x_continuous(labels = scales::label_percent(), \n                     limits = c(0,1)) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 6.5)) +\n  see::theme_modern() +\n  theme(legend.position = \"top\",\n        legend.spacing.x = unit(3, \"mm\")) +\n  #### Arrows\n  geom_curve(aes(x = .81, y = 4.1, xend = .69232, yend = 3.425), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .9, y = 4.1, label = \"Beta(10,5)\")) +\n  geom_curve(aes(x = .2, y = 5.9, xend = .07693, yend = 5.45), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .29, y = 5.85, label = \"Beta(2,13)\")) +\n  geom_curve(aes(x = .5, y = 5, xend = .3847, yend = 4.4), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .55, y = 5, label = \"≈ 39%\"))\n\n\n\n\n\nGraphical representation of the posterior probability as the combination of both the data and the prior evidence"
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#from-past-to-future",
    "href": "posts/2023-05-30 welcome/index.html#from-past-to-future",
    "title": "Welcome to Bayesically Speaking",
    "section": "From past to future",
    "text": "From past to future\nImagine a time not too long ago when Bayesian statistics were not as prevalent as they are today. The computational challenges posed significant hurdles, limiting our ability to fully embrace their potential. But thanks to the rapid advancement of computing power and simulation techniques, the statistical landscape has undergone a revolution. We now find ourselves in an exciting era where complex Bayesian analysis is accessible to all. It’s like having a superpower in the palm of our hands—an empowering time where our statistical prowess can thrive and conquer new frontiers.\nAs passionate self-learners on this thrilling statistical journey, even without a formal statistician’s hat, we can’t help but feel an overwhelming excitement to share the vast potential of these tools for unraveling real-world phenomena. Delving into the world of statistics, especially through the lens of Bayesian inference, opens up a universe of captivating possibilities. By melding prior knowledge with fresh evidence and embracing the enigmatic realm of uncertainty, we can uncover profound insights into health, well-being, and the wondrous phenomena that shape our lives.\nSo, fellow adventurers, let’s ignite our curiosity, embrace our thirst for knowledge, and embark on this exhilarating voyage together. With statistics as our compass, we will navigate the complexities of our reality, expanding our understanding and seizing the extraordinary opportunities that await us.\nGet ready to experience a world that’s more vivid, more nuanced, and more awe-inspiring than ever before. Together, let’s dive into the captivating realm of statistics, fueled by enthusiasm and a passion for discovery."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Bayesically Speaking\n\n\nPriors & Coffee\n\n\n\n\n\n\n\nWhat is this?\nIf you are curious about how to use data and probability to understand and visualize real world problems, you have come to the right place.\n\n\n About\n\n\n Related work\n\n\n\n\n\n\n\n\nLatest posts\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte What?\n\n\n23 min\n\n\nIn this post we will delve into the main idea behind Markov Chain Monte Carlo (MCMC for short) and why it is useful within the bayesian inference framework.\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Bayesically Speaking\n\n\n12 min\n\n\nHi everyone! This is the first post of Bayesically Speaking, so get your seatbelt on and get ready to join me on this ride!\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Portfolio",
    "section": "",
    "text": "Portfolio"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Portfolio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChilean Austral Molecular Integrative Neurophysiology by its acronym in Spanish.↩︎"
  }
]