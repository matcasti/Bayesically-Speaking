[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Blog\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nModeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations\n\n\n\n\n\n\nnon-linear\n\n\nagent-based-simulation\n\n\nsimulation\n\n\ncell-dynamics\n\n\n\nIn this post, we’ll take you step by step through building and playing with a model of lymphocyte dynamics. So, grab your keyboard, and let’s whip up a recipe for understanding how T cells become the immune system’s MVPs. \n\n\n\n\n\nDec 23, 2024\n\n\n40 min\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: A Case for Synaptic Plasticity\n\n\n\n\n\n\nnon-linear\n\n\n\nIn this second edition we’ll be discussing the role of long term potentiation and depression, a form of neuroplasticity, and their dynamics using non-linear models. Here, we’ll dig deep into the statistical intricacies of the overlapping dynamics of synaptic neuroplasticity. \n\n\n\n\n\nDec 14, 2024\n\n\n35 min\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: Pharmacokinetics and Indomethacin\n\n\n\n\n\n\nnon-linear\n\n\n\nHere we dive in the process of making a non-linear model to predict the decay of plasma levels of an anti-inflammatory drug, and compare frequentist and bayesian methods. \n\n\n\n\n\nAug 20, 2024\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Good, The Bad, and Hamiltonian Monte Carlo\n\n\n\n\n\n\nmcmc\n\n\n\nFollowing the footsteps of the previous post, here we delve ourselves into the mud of hamiltonian mechanics and how its dynamics can help us to explore parameter space more efficiently. \n\n\n\n\n\nMay 15, 2024\n\n\n30 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte What?\n\n\n\n\n\n\nintro\n\n\nmcmc\n\n\n\nIn this post we will delve into the main idea behind Markov Chain Monte Carlo (MCMC for short) and why it is useful within the bayesian inference framework. \n\n\n\n\n\nApr 25, 2024\n\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Bayesically Speaking\n\n\n\n\n\n\nnews\n\n\n\nHi everyone! This is the first post of Bayesically Speaking, so get your seatbelt on and get ready to join me on this ride! \n\n\n\n\n\nJun 10, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About"
  },
  {
    "objectID": "about.html#what-is-all-the-fuzz-about",
    "href": "about.html#what-is-all-the-fuzz-about",
    "title": "About",
    "section": "What is all the fuzz about?",
    "text": "What is all the fuzz about?\nHello and welcome to Bayesically Speaking, the blog where I, Matías, share my passion for statistics, bayesian methods and coffee. If you are curious about how to use data and probability to understand and solve real world problems, you have come to the right place. Here you will find practical examples, tutorials, tips and tricks on how to apply statistical thinking and inference to various domains and scenarios."
  },
  {
    "objectID": "about.html#who-is-this-guy",
    "href": "about.html#who-is-this-guy",
    "title": "About",
    "section": "Who is this guy?",
    "text": "Who is this guy?\nWho am I and why should you care? Well, I am a researcher who loves numbers and coffee (not necessarily in that order). I have a special interest in bayesian methods, which are a powerful and flexible way of doing statistics that allows you to incorporate prior knowledge and uncertainty into your analysis. Bayesian methods are not magic, though. They have their limitations and challenges, just like any other approach. That’s why I don’t shy away from using other tools when they are appropriate, such as frequentist methods or p-values. My goal is not to start a war between different schools of thought, but to show you how to use the best tool for the job."
  },
  {
    "objectID": "about.html#a-secret-weapon",
    "href": "about.html#a-secret-weapon",
    "title": "About",
    "section": "A secret weapon",
    "text": "A secret weapon\nIn this blog, you will also learn how to create beautiful and informative graphics and plots using R, which is a free and open source software for data analysis and visualization. R is my favorite tool for doing statistics, because it has a huge community of users and developers who create amazing packages and resources for all kinds of purposes. R can also do much more than just statistics, such as web scraping, text mining, machine learning and more. Even this website was build using R!"
  },
  {
    "objectID": "posts/2024-12-10 a-case-for-synaptic-plasticity/index.html",
    "href": "posts/2024-12-10 a-case-for-synaptic-plasticity/index.html",
    "title": "Non-linear models: A Case for Synaptic Plasticity",
    "section": "",
    "text": "Introduction\nIf you missed the first edition of this series of non-linear models posts, go see it now!\nThe human brain is a show-off, really. It’s constantly rewiring itself, adapting to new challenges, and making you forget where you put your keys (all at the same time). This remarkable ability, called neuronal plasticity, is the brain’s way of saying, “Don’t worry, I can learn, unlearn, and even relearn, if necessary.” Imagine a guitarist practicing a tricky solo. With each attempt (and many failed ones), the neurons involved in mastering that melody become like overzealous gym buddies: stronger, faster, and annoyingly efficient. That’s plasticity in action.\nBut capturing this in a model? Oh, that’s another story. Modeling neuronal plasticity is like trying to explain a teenager’s mood swings (messy, non-linear, and full of unexpected spikes). The process involves sharp changes in synaptic strength, calcium signaling thresholds that act like overly jealous gatekeepers, and dynamics so complex they make rocket science look like child’s play. Traditional models just shrug and wave the white flag.\nLet’s welcome the non-linear models, the superheroes of mathematical modeling. In this post, we’ll take a closer look at how these models, specifically logistic growth equations, can tame the chaos of neuronal plasticity. We’ll demystify the biology, turn it into manageable math, and then buckle up to simulate it all in R. Yes, R… the land of syntax errors and endless parentheses, but don’t worry, I’ll guide you.\nBy the end of this journey, you’ll not only appreciate the elegance of neuronal plasticity but also see how non-linear modeling can turn brain mysteries into manageable equations. So whether you’re a neuroscience nerd, or someone who just likes the idea of bossing around neurons via code, this post has something for you. Let’s dive in, and don’t forget to pack your sense of humor. You’re going to need it!\n\n\n\nUnderstanding Neuronal Plasticity\nNeuronal plasticity is the brain’s way of being a multitasking genius or (depending on your perspective), a workaholic control freak. At its heart, it’s the neurons’ ability to adapt their structure and function, responding to experiences, activity, or the occasional existential crisis. This adaptability powers the brain’s greatest hits: learning, memory, and even bouncing back from injuries. This concept, stating that neuronal connections can be remodeled by experience, is also known as Hebbian Theory, and its behind many studied mechanisms by which plastic changes occurs at the synapse-level (Scott and Frank 2023). Think of it as the neural equivalent of turning your spare bedroom into a home gym, if neurons are adaptable, why shouldn’t you be?\n\nThe Role of Synaptic Strength\nPlasticity does its magic at the synapse, that microscopic handshake where one neuron whispers (or yells) to another. The strength of this connection, aptly named synaptic strength, determines how effectively neurons gossip. Strengthening these connections, a process grandiosely called Long-Term Potentiation (LTP), makes neural communication as smooth as butter on warm toast (Malenka 2003). On the flip side, Long-Term Depression (LTD) weakens connections, pruning the neural network like a gardener trimming overzealous hedges. Goodbye, redundant pathways; hello, optimization.\n Long-Term Potentiation (LTP) and Long-Term Depression (LTD) occurs as a function of synapse communication. This is what makes some brain pathways stronger or weaker over time, contributing to both the acquiring and loosing cognitive functions and motor skills. Source: Biology 2e. OpenStax.\n\n\nCalcium: The Master Regulator\nIf synaptic strength is the party, calcium ions (\\(Ca^{2+}\\)) are the overzealous DJ deciding whether to pump up the volume or call it a night. Calcium levels dictate whether the synapse becomes stronger or weaker, essentially flipping the plasticity switches:\n\nHigh calcium levels? Cue the LTP rave, connections strengthen, neurons fire happily ever after.\nLow calcium levels? Time for the LTD chill-out session, synapses quiet down and pathways are pruned.\n\nThe drama lies in the thresholds. Calcium must hit the sweet spot: too high, and it’s all systems go for strengthening; too low, and the neuron gets out its metaphorical scissors. Anything in between is neural purgatory (synaptic strength stays stable), which is another way of saying, “Meh, let’s just keep things as they are.”\n\n\nDynamic and Non-Linear Nature\nHere’s where things get tricky: the relationship between calcium levels and synaptic strength isn’t linear, because why would the brain ever take the easy route? A tiny nudge in calcium concentration can tip the balance dramatically, depending on whether those pesky thresholds are crossed. Feedback loops ensure that synaptic adjustments don’t spiral out of control, think of them as neural quality control, keeping things proportional and adaptive (Lisman, Yasuda, and Raghavachari 2012; Yasuda, Hayashi, and Hell 2022).\n\n\nCode\n# Define calcium levels and thresholds\ncalcium_levels &lt;- seq(0, 2, by = 0.01) # Simulated calcium levels (arbitrary units)\nltp_threshold &lt;- 1.2  # Threshold for LTP activation\nltd_threshold &lt;- 0.8  # Threshold for LTD activation\n\n# Define synaptic strength change based on calcium\nsynaptic_strength_change &lt;- function(calcium, ltp_thresh, ltd_thresh) {\n  # Non-linear responses for LTP and LTD\n  ltp_response &lt;- ifelse(calcium &gt; ltp_thresh, (calcium - ltp_thresh)^2, 0)\n  ltd_response &lt;- ifelse(calcium &lt; ltd_thresh, -(ltd_thresh - calcium)^2, 0)\n  ltp_response + ltd_response\n}\n\n# Compute synaptic strength changes\nstrength_changes &lt;- synaptic_strength_change(calcium_levels, ltp_threshold, ltd_threshold)\n\n# Create a data frame for plotting\ndata &lt;- data.frame(\n  Calcium = calcium_levels,\n  StrengthChange = strength_changes\n)\n\n# Plot\nggplot(data, aes(x = Calcium, y = StrengthChange)) +\n  geom_line(color = \"orange\", size = 1) +\n  geom_vline(xintercept = ltp_threshold, linetype = \"dashed\", color = \"darkgreen\", size = 1/2) +\n  geom_vline(xintercept = ltd_threshold, linetype = \"dashed\", color = \"darkred\", size = 1/2) +\n  annotate(\"text\", x = ltp_threshold + 0.01, y = 0.5, label = \"LTP Threshold\", color = \"darkgreen\", hjust = 0, size = 6) +\n  annotate(\"text\", x = ltd_threshold - 0.01, y = -0.5, label = \"LTD Threshold\", color = \"darkred\", hjust = 1, size = 6) +\n  labs(\n    title = \"Non-Linear Relationship Between Calcium and Synaptic Strength\",\n    x = \"Calcium Levels (arbitrary units)\",\n    y = \"Change in Synaptic Strength\"\n  )\n\n\n\n\n\nHere, the relationship between calcium levels and the change in synaptic strength is simplified to illustrate the relationship between calcium dynamics and synaptic plasticity.\n\n\n\n\nThis delicate dance between calcium signaling, thresholds, and synaptic strength is the essence of neuronal plasticity’s beauty. It’s also the reason why modeling this process is as tricky as explaining quantum physics to a toddler. But don’t worry; the non-linear models we’ll tackle next will help demystify this wild ride.\n\n\n\nThe Challenge of Modeling Neuronal Plasticity\nModeling neuronal plasticity is like trying to choreograph a dance for cats: dynamic, unpredictable, and prone to sudden chaos. The brain doesn’t play by the simple rules of linear systems. Instead, it thrives on thresholds, feedback loops, and behaviors that emerge as if neurons decided to collectively say, “Let’s make this interesting.”\n\nThe Complexity of Biological Systems\n Depiction of the step-by-step communication between neurons at the synapse-level through calcium signaling at the presynaptic neuron, responsible for neurotransmitter releasing and neuron depolarization. Source: Biology 2e. OpenStax.\nNeuronal plasticity isn’t just complex, it’s a web of biochemical drama. Take calcium concentration dynamics, these levels don’t just sit still like a well-behaved variable. They oscillate wildly, influenced by synaptic activity and timing, like calcium’s trying out for a rhythm section. These oscillations trigger intracellular signaling cascades that either crank up the volume or dial it down, depending on the situation. And then there’s synaptic change over time, a gradual process, often following a sigmoid or exponential curve. In other words, synaptic strength doesn’t just flick a switch; it warms up, stretches, and then decides if it’s going to do some serious lifting.\nLinear models? They try their best but are basically out of their depth here. Sure, they can hint at early synaptic changes, but when feedback mechanisms, thresholds, and saturation effects enter the chat, linear models might as well pack up and go home.\n\n\nThreshold-Dependent Behavior\nCalcium thresholds are where the real drama unfolds. Picture this: when \\(Ca^{2+}\\) crosses the high threshold (\\(C_{\\text{LTP}}\\)), synaptic strength goes into overdrive, climbing exponentially until it maxes out like a hiker who’s finally reached the summit. But if \\(Ca^{2+}\\) drops below the low threshold (\\(C_{\\text{LTD}}\\)), it’s pruning time, and synaptic strength takes a sharp nosedive to the baseline. These thresholds create non-linearity, where tiny calcium fluctuations can flip the synaptic switch from party mode to shutdown in a heartbeat.\n\n\nFeedback and Regulation\nIf thresholds are the stage, feedback mechanisms are the stagehands, constantly adjusting the scene. Positive feedback amplifies responses, with LTP inviting more calcium to the party and reinforcing synaptic strengthening. Negative feedback, on the other hand, keeps LTD from going full demolition crew by dialing down activity and avoiding unnecessary pruning. Together, these loops keep the system both chaotic and oddly balanced, a testament to biological ingenuity (or masochism).\n\n\nVariability and Noise\nAnd let’s not forget the noise. Neurons are the original rebels, no two behave identically. Calcium concentrations fluctuate, receptors show wildly different sensitivities, and neurotransmitter release has a habit of being unpredictable. Stochastic effects at the molecular level add another layer of randomness, making modeling this system akin to predicting the stock market in a thunderstorm.\n\n\nWhy Non-Linear Models?\nNon-linear models, like logistic growth equations, come to the rescue with their ability to handle the brain’s antics. They’re perfect for capturing threshold-dependent dynamics, saturation effects, and time-dependent changes, all while leaving room for noise and feedback. It’s as if these models were built specifically for taming the wild world of neuronal plasticity.\nIn the next section, we’ll get ready the magic of non-linear modeling, focusing on logistic growth equations. Spoiler: these equations are like cheat codes for understanding synaptic strength changes. Let’s see them in action!\n\n\n\nIntroducing the Non-Linear Model\nTo simulate the dynamics of neuronal plasticity, we need a model that doesn’t crumble under the weight of complexity. Let me introduce you the logistic growth model, our mathematical hero. This model elegantly captures the key ingredients of synaptic changes:\n\nThreshold behavior: Synaptic strength only changes when stimulation crosses a “Do Not Disturb” threshold.\n\nSaturation: Synaptic strength doesn’t keep growing forever (because even neurons know their limits).\n\nTime-dependence: Synaptic changes are gradual, like making sourdough bread. Patience is key.\n\nLogistic equations are pretty simple at its core, they all follow the same structure.\n\\[\nf(x) = \\frac{1}{1 + e^{-x}}\n\\]\nThis kind of equations will (in general) yield the following behavior.\n\n\nCode\nlogistic &lt;- function(x) { 1 / (1 + exp(-x)) }\n\nggplot() +\n  geom_hline(yintercept = c(0, 1), linetype = 2, size = 1/2, col = \"gray50\") +\n  stat_function(fun = logistic, xlim = c(-10, 10), linewidth = 1, col = \"orange\") +\n  annotate(geom = \"text\", label = \"f(x) == frac(1,1 + e^{-x})\", x = -3, y = 0.5, parse = TRUE, size = 6) +\n  labs(title = \"Asymptotic Behavior of Logistic Functions\",\n       y = expression(italic(f)(x)),\n       x = expression(italic(x)))\n\n\n\n\n\n\n\n\n\nThis logistic growth model is widely used in biology to describe systems where growth starts slow, accelerates, and then hits a plateau (also called, asymptotes). In our case, it’s the perfect candidate for modeling synaptic strength during LTP and LTD, all while looking deceptively simple.\n\nLooking into the model\nIf we tweak the forementioned logistic equation, we get something like this:\n\\[\nS(t) = S_{\\text{min}} + \\frac{S_{\\text{max}} - S_{\\text{min}}}{1 + e^{-\\lambda (t - t_0)}}\n\\]\nLet’s break it down without making your eyes glaze over:\n\n\\(S(t)\\): Synaptic strength at time \\(t\\). This is what we’re modeling.\n\n\\(S_{\\text{min}}\\): The baseline synaptic strength, think of it as the starting line.\n\n\\(S_{\\text{max}}\\): The upper limit of synaptic strength after all the excitement of LTP.\n\n\\(\\lambda\\): The rate of change, or how quickly the synapse is building muscle.\n\n\\(t_0\\): The starting point of stimulation, shifting the curve along the time axis.\n\n\n\n\n\n\n\nPedagogic convenience\n\n\n\n\n\nUsing synaptic strength is simpler and more abstract, making it easier to fit and interpret in many contexts, especially when data on calcium dynamics is unavailable.\nExplicitly modeling calcium dynamics and thresholds can provide deeper biological insights but at the cost of added complexity.\n\n\n\nHere’s why this logistic structure is very convenient:\n\nSynaptic strength starts close to \\(S_{\\text{min}}\\) when time is still warming up (\\(t \\ll t_0\\)).\n\nIt ramps up as \\(t\\) approaches \\(t_0\\), because neurons love to make a dramatic entrance.\n\nGrowth slows as \\(S(t)\\) nears \\(S_{\\text{max}}\\), reflecting the biological saturation point (like most physiological processes).\n\n\n\nModeling LTP: The Big Boost\nIn Long-Term Potentiation (LTP), the synapse gets a power-up, causing a rapid and lasting increase in strength. Using the logistic growth model:\n\n\\(S_{\\text{min}}\\) is the baseline before stimulation.\n\n\\(S_{\\text{max}}\\) is the exciting new level of strength after LTP has worked its magic.\n\nTweaking \\(\\lambda\\) and \\(t_0\\) lets us control how quickly and when the synapse gets its boost, perfect for simulating the effects of different stimuli.\n\n\nModeling LTD: The Great Decline\nIn Long-Term Depression (LTD), the synapse doesn’t throw a party, it hits the brakes instead. For this, we flip the logistic equation on its head, changing the sign on the logistic part of the function:\n\\[\nS(t) = S_{\\text{max}} - \\frac{S_{\\text{max}} - S_{\\text{min}}}{1 + e^{-\\lambda (t - t_0)}}\n\\]\nHere:\n\n\\(S_{\\text{max}}\\) is where the synaptic strength starts before LTD brings it down a notch.\n\n\\(S_{\\text{min}}\\) is the new baseline after the synapse gets reorganized.\n\nIt’s like a reverse LTP (same logistics), but with a focus on pruning rather than boosting.\n\n\nCombining LTP and LTD: The Balancing Act\nNeurons aren’t simple creatures. They often experience LTP and LTD simultaneously, like trying to watch a movie while your neighbor is mowing the lawn. To capture this, we combine two logistic equations into one glorious model:\n\\[\nS(t) = S_{\\text{baseline}} + \\frac{\\Delta S_{\\text{LTP}}}{1 + e^{-\\lambda_{\\text{LTP}} (t - t_{\\text{LTP}})}} - \\frac{\\Delta S_{\\text{LTD}}}{1 + e^{-\\lambda_{\\text{LTD}} (t - t_{\\text{LTD}})}}\n\\]\nI know what you’re thinking… it seems like its too much. However, for sanity sakes lets break it down:\n\n\\(S_{\\text{baseline}}\\): The synaptic strength before anything exciting happens.\n\\(\\Delta S_{\\text{LTP}} = S_{\\text{max}} - S_{\\text{baseline}}\\): The increase due to LTP.\n\\(\\Delta S_{\\text{LTD}} = S_{\\text{baseline}} - S_{\\text{min}}\\): The decrease due to LTD.\n\\(\\lambda_{\\text{LTP}}\\) and \\(\\lambda_{\\text{LTD}}\\): Rates of change for LTP and LTD (i.e., the “how quickly” part).\n\\(t_{\\text{LTP}}\\) and \\(t_{\\text{LTD}}\\): Onset times for LTP and LTD (i.e., the “when” part).\n\nThis combined model lets us handle real-world complexity, where strengthening and weakening signals overlap like a neural tug-of-war.\n\n\nWhy Two Logistic Functions?\nYou might wonder why we bother with two logistic functions instead of just one that switches direction halfway. The answer: biology loves chaos.\n\nLTP and LTD aren’t mutually exclusive. They can overlap, fight, or ignore each other altogether.\n\nTheir onset times (\\(t_{\\text{LTP}}\\) and \\(t_{\\text{LTD}}\\)) and rates (\\(\\lambda_{\\text{LTP}}\\), \\(\\lambda_{\\text{LTD}}\\)) often vary, making a single equation woefully inadequate.\n\nOf course, we could have just used a single logistic function. However, in most real-world scenarios, beyond the pedagogic convenience, simple models are just the initial phase of any model building process.\nDespite any complexity we could add to the model, we must always remember that any model is always “an approximation to reality” at best. Or, as the famous George Box would say:\n\n“Essentially all models are wrong, but some are useful”\n\n\n\n\n\n\n\nLTP and LTD at the Same Time?\n\n\n\n\n\nIn some fascinating scenarios, neurons manage to pull off the ultimate balancing act: inducing both LTP and LTD simultaneously. This can happen when synaptic inputs are spatially separated, allowing different synapses on the same neuron to independently engage in potentiation or depression based on local calcium dynamics (Scott and Frank 2023). It’s like a multi-tasking maniac performing two distinct solos at once. Similarly, in spike-timing-dependent plasticity (discussed up ahead), complex spike-timing patterns can produce overlaps in the conditions for LTP and LTD, with some synapses “winning” in one direction and others in the opposite.\nSometimes, the brain’s calcium dynamics get a little wild, oscillating between the ranges required for LTP and LTD due to the convergence of excitatory and inhibitory inputs. Throw in a dash of neuromodulation (like dopamine or acetylcholine tweaking thresholds) and some synapses might lean toward strengthening while others favor weakening. This interplay can also occur when synapses start in different states: stronger synapses may undergo LTD to maintain homeostasis, while weaker ones are busy potentiating (Scott and Frank 2023). Interestingly, experimental protocols designed to study synaptic plasticity in mood disorders often force these conditions, with pharmacological interventions inducing concurrent LTP and LTD in some brain areas (Krystal, Kavalali, and Monteggia 2024).\n\n\n\nTwo logistic functions give us the flexibility to capture these dynamics without assuming biology plays nice.\nIn the next section, we’ll fire up R and use these equations to simulate synaptic strength changes. Spoiler: it’s going to be visually satisfying, so stick around!\n\n\n\nSimulating Synaptic Plasticity in R\nNow that we’ve got our fancy non-linear model ready to roll, it’s time to turn theory into practice (or, more specifically, into code). Let’s use R to simulate synaptic changes over time. Think of this as our chance to play neuroscientific alchemist, blending math, biology, and programming into a beautiful plot. Spoiler alert: things will go up, things will go down, and everything will make sense… eventually.\n\nStep 1: Defining the Model\nFirst, we need to teach R how to think like a synapse. We’ll define our logistic functions for LTP and LTD, then combine them to model the overall synaptic strength. It’s like giving R its own brain, except this one follows your rules and doesn’t forget where it put the car keys.\nThe logistic function describes how synaptic strength changes with time. For LTP, strength rises from a baseline to a maximum. For LTD, the opposite happens: it’s the digital equivalent of neurons ghosting each other.\n\n\nCode\n# Define the logistic function\nlogistic &lt;- function(t, t_onset, lambda, delta_S) {\n  delta_S / (1 + exp(-lambda * (t - t_onset)))\n}\n\n# Define the combined model\nsynaptic_strength &lt;- function(t, S_baseline, t_LTP, lambda_LTP, delta_LTP,\n                               t_LTD, lambda_LTD, delta_LTD) {\n  \n  ltp &lt;- logistic(t, t_LTP, lambda_LTP, delta_LTP)\n  ltd &lt;- -logistic(t, t_LTD, lambda_LTD, delta_LTD)\n  strength &lt;- S_baseline + ltp + ltd\n  \n  data.table::data.table(\n    Time = t, \n    LTP = ltp,\n    LTD = ltd,\n    `LTP + LTD` = strength\n  )\n}\n\n\nNotice how LTP and LTD are modeled as separate entities but combine their forces (or opposing forces) in the overall strength equation. It’s teamwork, or in this case, team tug-of-war.\n\n\nStep 2: Setting the Stage\nNext, we’ll define some parameters. Think of this as setting up your neural experiment in a lab, but with fewer pipettes and a lot more debugging.\nWe’ll start with baseline synaptic strength set to 1 (for our convenience), simulate LTP kicking in at time 5, and introduce LTD at time 10. Each process has its own speed (\\(\\lambda\\)) and magnitude (\\(\\Delta S\\)).\n\n\nCode\n# Parameters for the simulation\nS_baseline &lt;- 1\nt_LTP &lt;- 5\nlambda_LTP &lt;- 0.5\ndelta_LTP &lt;- 0.5\nt_LTD &lt;- 10\nlambda_LTD &lt;- 0.3\ndelta_LTD &lt;- 0.3\n\n# Time range for simulation\ntime &lt;- seq(0, 20, by = 0.1)\n\n# Compute synaptic strength over time\nS &lt;- synaptic_strength(time, S_baseline, t_LTP, lambda_LTP, delta_LTP,\n                       t_LTD, lambda_LTD, delta_LTD)\n\n## Format the resulting simulation into a convenient plotting format for later\nS &lt;- data.table::melt(S, id.vars = \"Time\")\n\n\nBy now, we’ve turned R into a synaptic storyteller, weaving tales of LTP, LTD, and the epic battle for dominance over synaptic strength.\n\n\nStep 3: Plotting the Results\nIt’s time for the pièce de résistance: visualization! Using ggplot2, we’ll craft a plot that shows how synaptic strength evolves. Imagine this as the biologist’s equivalent of putting your art project on the fridge.\n\n\nCode\n# Plot the synaptic strength\nggplot(S, aes(x = Time, y = value)) +\n  facet_wrap(~ variable, scales = \"free_y\", nrow = 1) +\n  geom_line(aes(color = variable), size = 1, show.legend = FALSE) +\n  scale_x_continuous(expand = c(0,0,0,0.01)) +\n  scale_y_continuous(n.breaks = 6) +\n  labs(title = \"Synaptic Strength Over Time\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\") \n\n\n\n\n\n\n\n\n\nThe resulting plot reveals that the resulting synaptic strength is the result of the simultaneously gradual rise in synaptic strength thanks to LTP, followed by a dip as LTD gets its turn (plus the constant that determines the baseline strength). It’s like a see-saw, but the laws of biochemistry decide who gets to go up or down.\n\n\n\n\n\n\nWhat do the units of synaptic strength means?\n\n\n\n\n\nThe synaptic strength (\\(S\\)) in the model is dimensionless. It represents a normalized value that captures the relative magnitude of potentiation or depression of a synapse, typically scaled between \\(S_{\\text{min}}\\) and \\(S_{\\text{max}}\\) (e.g., 0.5 to 1.5 in the upcoming extended model). This normalization simplifies the model by abstracting away specific biophysical units, such as conductance (\\(\\mu S\\)) or post-synaptic current (\\(pA\\)), to focus on the dynamics of synaptic changes.\nIf needed, the model could be adapted to use specific units, like synaptic conductance, by rescaling \\(S\\) and its parameters to reflect actual physiological measurements. For instance, calcium oscillation amplitudes and the associated logistic functions would need to be parameterized according to empirical data from experiments measuring synaptic responses.\n\n\n\n\n\nStep 4: Playing with Parameters\nThis is where things get really fun. By tweaking parameters, you can explore different scenarios, like what happens if neurons get hyped with LTP or slack off on LTD. For example, cranking up the LTP magnitude to 1.5 (from 0.5) while leaving LTD at 0.3 results in a net strengthening of the synapse.\n\n\nCode\n# Example: Modify LTP magnitude\ndelta_LTP &lt;- seq(0.5, 1.5, by = 0.1)\n\n# Recompute synaptic strength\ndata_modified &lt;- lapply(delta_LTP, function(x) {\n  strength &lt;- synaptic_strength(time, S_baseline, t_LTP, lambda_LTP, x,\n                                t_LTD, lambda_LTD, delta_LTD)\n  strength$delta_ltp &lt;- x\n  strength\n}) |&gt; \n  rbindlist()\n\n# Plot the modified synaptic strength\nggplot(data_modified, aes(x = Time, y = `LTP + LTD`, col = ordered(delta_ltp))) +\n  geom_line(size = 1) +\n  labs(title = \"Modified Synaptic Strength (Stronger LTP)\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\",\n       col = expression(Delta*\"S\"[LTP]))\n\n\n\n\n\n\n\n\n\nThis flexibility lets you conduct virtual experiments, saving you from lab-induced caffeine dependency while still yielding valuable insights.\nIn the next section, we’ll explore the implications of these simulations and explore how tweaking parameters can reveal deeper truths about neuronal plasticity. Stay tuned because in science, even the tiniest tweak can lead to a plot twist.\n\n\n\nExtending the Model to Include LTP and LTD Dynamics\nOur model so far is like a sturdy bicycle: simple, reliable, and great for short trips. But neuroscience is a highway with twists, turns, and occasional potholes, so it’s time to upgrade to a more dynamic model, think of this as strapping a rocket engine to that bike. Synaptic plasticity isn’t just about changes in strength; timing, saturation, and other nuances come into play. Let’s extend our model to capture some of these complexities.\n\nIncorporating Temporal Dependencies\nTiming is everything in neuroscience. In the world of synapses, when presynaptic and postsynaptic neurons fire relative to each other determines whether you get LTP or LTD. It’s a bit like a dance: if one partner steps too late, the magic is lost, and the audience (synaptic strength) is unimpressed.\nThis timing phenomenon, called spike-timing-dependent plasticity (STDP), hinges on \\(\\Delta t\\) (the interval between pre- and postsynaptic spikes). LTP loves when presynaptic spikes lead the way, while LTD thrives when postsynaptic spikes take the lead (Scott and Frank 2023).\nTo capture this, we introduce exponential decay functions for the magnitudes of LTP and LTD:\n\\[\n\\begin{aligned}\n\\Delta S_{\\text{LTP}} &= A_{\\text{LTP}} \\cdot e^{-\\frac{\\Delta t}{\\tau_{\\text{LTP}}}} \\\\\n\\Delta S_{\\text{LTD}} &= A_{\\text{LTD}} \\cdot e^{-\\frac{\\Delta t}{\\tau_{\\text{LTD}}}}\n\\end{aligned}\n\\]\nHere, \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\) are the maximum amplitudes of change in synaptic strength, equivalent to \\(\\Delta S\\) (the size of the “dance move”), and \\(\\tau_{\\text{LTP}}\\), \\(\\tau_{\\text{LTD}}\\) are time constants that govern how quickly the effect of time between spikes (\\(\\Delta t\\)) fades. If you’ve ever tried to tell a joke but botched the timing, you already understand how crucial these constants are.\nNow, let’s see how each of these parameters affects the synaptic choreography. If \\(A_{\\text{LTP}}\\) (the amplitude of the LTP move) is cranked up, the presynaptic dancer delivers an eye-catching performance, resulting in a stronger synapse. A higher amplitude here means even a moderately timed spike can still significantly strengthen the synapse. However, if \\(A_{\\text{LTD}}\\) takes center stage, the postsynaptic dancer has the spotlight, weakening the synapse more dramatically. Reducing either amplitude dulls their impact; it’s like both dancers are phoning it in, leaving the synapse barely moved, regardless of the timing.\nNow, consider \\(\\tau_{\\text{LTP}}\\) and \\(\\tau_{\\text{LTD}}\\), the time constants that control how forgiving the synapse is about timing. A large \\(\\tau_{\\text{LTP}}\\) means the presynaptic dancer gets more leeway; even if the moves are slightly late, the synapse is still impressed, leading to a broader window for LTP. In contrast, a smaller \\(\\tau_{\\text{LTP}}\\) signals a pickier synapse, rewarding only perfectly timed presynaptic spikes. Similarly, a long \\(\\tau_{\\text{LTD}}\\) gives the postsynaptic neuron plenty of room to weaken the synapse, tolerating more variability in timing. With a shorter \\(\\tau_{\\text{LTD}}\\), the postsynaptic dancer must hit the perfect beat to have any meaningful impact.\nThe plot below illustrates the STDP model, showing how the magnitude of synaptic changes depends on the timing (\\(\\Delta t\\)). Positive \\(\\Delta t\\) corresponds to presynaptic spikes leading, favoring LTP, while negative \\(\\Delta t\\) corresponds to postsynaptic spikes leading, favoring LTD.\n\n\nCode\n# Parameters for STDP\nA_LTP &lt;- 0.5  # Maximum potentiation amplitude\ntau_LTP &lt;- 20  # LTP time constant\nA_LTD &lt;- -0.5  # Maximum depression amplitude (negative)\ntau_LTD &lt;- 20  # LTD time constant\n\n# Function to compute LTP and LTD effects\nSTDP_effect &lt;- function(delta_t) {\n  ifelse(delta_t &gt;= 0,\n         A_LTP * exp(-delta_t / tau_LTP),  # LTP for positive delta_t\n         A_LTD * exp(delta_t / tau_LTD))  # LTD for negative delta_t\n}\n\n# Simulate data\ndelta_t &lt;- seq(-50, 50, by = 1)\nsynaptic_changes &lt;- sapply(delta_t, STDP_effect)\n\n# Create a data frame for plotting\ndata &lt;- data.frame(delta_t, synaptic_changes)\n\n# Plot the STDP curve\nggplot(data, aes(x = delta_t, y = synaptic_changes)) +\n  geom_line(color = \"orange\", size = 1) +\n  labs(title = \"Spike-Timing-Dependent Plasticity (STDP) Curve\",\n       x = \"Time Difference (Δt, ms)\",\n       y = \"Synaptic Change (ΔS)\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  annotate(\"text\", x = 30, y = 0.5, label = \"LTP Zone\", size = 6) +\n  annotate(\"text\", x = -30, y = -0.3, label = \"LTD Zone\", size = 6)\n\n\n\n\n\n\n\n\n\nThis plot captures the essence of STDP. It extends the model by introducing \\(\\Delta t\\) as a critical parameter, linking the timing of neural activity directly to the direction and magnitude of synaptic changes. It bridges abstract mathematical functions with tangible biological timing phenomena, making the model more representative of real neural processes.\n\n\n\n\n\n\nAddressing Saturation Effects\n\n\n\n\n\nSynaptic strength isn’t infinite, it has physiological constraints. However, in the current model, this limitation is already handled effectively by the logistic framework and STDP parameters. The logistic function naturally ensures that synaptic strength asymptotes at biologically meaningful upper and lower bounds, dictated by parameters like \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\). These parameters cap the magnitude of synaptic changes, avoiding unbounded potentiation or depression.\nIntroducing additional saturation mechanisms, such as hard boundaries on \\(S(t)\\), would be redundant in this context. The plateau behavior of the logistic function inherently aligns with physiological observations, maintaining synaptic changes within plausible limits without requiring explicit constraints. This reflects the elegance of using non-linear models: the boundaries are built into the system’s behavior, reducing the need for arbitrary external adjustments.\n\n\n\n\n\nSimulating the Extended Model\nLet’s see these concepts in action using R. By adding the timing-dependent amplitudes and enforcing strength limits, we upgrade our model to reflect these new dynamics.\n\n\nCode\n# Parameters for STDP-based dynamics\nA_LTP &lt;- 0.5\ntau_LTP &lt;- 20\nA_LTD &lt;- 0.3\ntau_LTD &lt;- 15\n\n# STDP-based amplitude functions\namplitude_LTP &lt;- function(delta_t, A_LTP, tau_LTP) {\n  A_LTP * exp(-abs(delta_t) / tau_LTP)\n}\n\namplitude_LTD &lt;- function(delta_t, A_LTD, tau_LTD) {\n  A_LTD * exp(-abs(delta_t) / tau_LTD)\n}\n\n# Extended model with saturation effects\nextended_synaptic_strength &lt;- function(t, S_baseline, \n                                       t_LTP, lambda_LTP, A_LTP, tau_LTP, \n                                       t_LTD, lambda_LTD, A_LTD, tau_LTD, \n                                       delta_t) {\n  S_baseline +\n       amplitude_LTP(delta_t, A_LTP, tau_LTP) / (1 + exp(-lambda_LTP * (t - t_LTP))) -\n       amplitude_LTD(delta_t, A_LTD, tau_LTD) / (1 + exp(-lambda_LTD * (t - t_LTD)))\n}\n\ndelta_t &lt;- seq(3, 30, length.out = 10)  # Example time difference between spikes\n\n# Simulate the extended model\nS_extended &lt;- lapply(delta_t, function(x) {\n  data.table(delta_t = x, \n             time = time,\n             strength = extended_synaptic_strength(time, S_baseline, \n                                                   t_LTP, lambda_LTP, A_LTP, tau_LTP, \n                                                   t_LTD, lambda_LTD, A_LTD, tau_LTD, \n                                                   delta_t = x) )\n}) |&gt; \n  rbindlist()\n\n\nWith this setup, our model now considers both the timing of spikes and the physiological constraints on synaptic strength.\n\n\nVisualizing the Extended Model\nLet’s plot the extended model to visualize these added dynamics.\n\n\nCode\n# Plot the extended synaptic strength\nggplot(S_extended, aes(x = time, y = strength, col = ordered(delta_t))) +\n  geom_line(aes(group = delta_t), size = 1) +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(title = \"Extended Synaptic Strength with Dynamics\",\n       x = \"Time (arbitrary units)\",\n       y = \"Synaptic Strength\",\n       col = expression(Delta*italic(t)~\"Between Spikes\"))\n\n\n\n\n\n\n\n\n\nThe plot reveals two key features:\n\nSTDP Influence: LTP and LTD magnitudes decay and flow based on spike timing (\\(\\Delta t\\)), showing how temporal precision can shape neural connections.\nSaturation Effects: Synaptic strength remains bounded thanks to parameters like \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\), showcasing the physiological checks and balances that prevent runaway excitation or extreme depression.\n\n\n\nFurther Extensions\nWhat’s next? We could incorporate frequency-dependent plasticity to explore how high-frequency stimulation turbocharges LTP or how low-frequency inputs foster LTD. Or we could unleash this model on a network of neurons to study how multiple synapses interact. The possibilities are endless!\nBy extending the model, we’re not just adding bells and whistles. We’re crafting a tool that captures the messy, dynamic beauty of real-world synaptic plasticity.\n\n\n\nExploring Parameter Effects\nOur extended model is like a well-equipped chemistry set for the brain, where each parameter represents a crucial ingredient in the recipe of synaptic plasticity. By varying these parameters, we can simulate how neurons behave under different conditions, from hyperactive learning states to subdued inhibition. This section focus on how key parameters influence the model’s behavior, explores their biological significance, and demonstrates how to experiment with them in R to uncover fascinating insights.\n\nSensitivity Analysis\nSensitivity analysis is a systematic way of tweaking the model’s parameters to observe their impact on synaptic strength. It’s akin to turning the dials on a stereo system to find the perfect sound balance, adjusting bass, treble, and volume to craft the desired output. In our case, the knobs control properties like calcium dynamics, synaptic decay rates, and baseline strength. These adjustments not only refine the model but also provide valuable biological insights into how real-life neurons might behave in various scenarios, from learning and memory formation to neurological disorders.\nHere’s an example of how you can experiment with the parameters in R to visualize their effects:\n\n\nCode\n# Define a range of parameter values\nlambda_LTP_vals &lt;- seq(0.5, 2, by = 0.5)  # Example: Steepness of LTP\nlambda_LTD_vals &lt;- seq(0.5, 2, by = 0.5)  # Example: Steepness of LTD\n\n# Simulate model for different parameters\nresults &lt;- expand.grid(\n  time = time, S_baseline = S_baseline, \n  t_LTP = t_LTP, lambda_LTP = lambda_LTP_vals, A_LTP = A_LTP, tau_LTP = tau_LTP,\n  t_LTD = t_LTD, lambda_LTD = lambda_LTD_vals, A_LTD = A_LTD, tau_LTD = tau_LTD,\n  delta_t = 0\n)\nresults$strength &lt;- mapply(extended_synaptic_strength, \n                           results$time, results$S_baseline,\n                           results$t_LTP, results$lambda_LTP, results$A_LTP, results$tau_LTP,\n                           results$t_LTD, results$lambda_LTD, results$A_LTD, results$tau_LTD,\n                           results$delta_t)\n\n# Plot sensitivity to lambda parameters\nggplot(results, aes(x = time, y = strength)) +\n  facet_wrap(~ ordered(lambda_LTD, levels = lambda_LTD_vals, \n                       labels = paste0(\"LTD Growth Rate = \", lambda_LTD_vals)), nrow = 2) +\n  geom_line(aes(color = ordered(lambda_LTP)), linewidth = 1) +\n  labs(title = \"Sensitivity Analysis: Impact of LTP and LTP Rate\",\n       x = \"Time (arbitrary units)\", y = \"Synaptic Strength\",\n       color = \"LTP Decay Rate\")\n\n\n\n\n\n\n\n\n\n\n\nFrom Model Parameters to Biological Applications\nThe calcium oscillation amplitude, \\(A_{\\text{LTP}}\\) and \\(A_{\\text{LTD}}\\), dictates the maximum strength of synaptic potentiation and depression. Imagine these amplitudes as the size of a neuron’s emotional response, whether it leaps for joy (LTP) or sighs deeply in despair (LTD). Increasing \\(A_{\\text{LTP}}\\) simulates a brain on overdrive, as in heightened learning states, while lowering it mirrors synaptic inhibition or even neurodegeneration.\nThe decay constants, \\(\\tau_{\\text{LTP}}\\) and \\(\\tau_{\\text{LTD}}\\), add a temporal layer to this story. These parameters determine how quickly the effects of potentiation and depression fade over time, similar to how fast a good mood or a bad one dissipates. Short decay constants are like fleeting bursts of inspiration, ideal for circuits needing rapid adaptation, such as the visual system. In contrast, long decay constants suit processes like memory consolidation in the hippocampus, where stability over time is crucial.\nGrowth rates, denoted by \\(\\lambda_{\\text{LTP}}\\) and \\(\\lambda_{\\text{LTD}}\\), control how steeply synaptic strength transitions in response to stimuli. Think of these as the sensitivity of a neuron’s accelerator pedal. A steep curve means the synapse reacts instantly to changes, akin to a sprinter’s quick start. Shallow growth rates, on the other hand, are like a marathon runner pacing themselves, better reflecting gradual changes in long-term neural adaptation.\nFinally, the baseline synaptic strength, \\(S_{\\text{baseline}}\\), sets the initial tone of the model. This parameter represents the synapse’s “default mood” before any plasticity occurs. Adjusting \\(S_{\\text{baseline}}\\) allows us to model different starting conditions, from a highly potentiated synapse involved in an established skill to a depressed one, such as in the aging brain or during neurodegeneration.\n\n\nFurther Applications\nExploring these parameters is not just a theoretical exercise; it has practical applications in neuroscience research and beyond. Researchers can use sensitivity analysis to design experiments or predict neuronal responses to stimuli. Educators can use these simulations as teaching tools to demonstrate the complexity of synaptic plasticity. Clinicians might find such models useful for exploring the effects of treatments targeting synaptic mechanisms, providing insights into potential therapies for conditions like Alzheimer’s disease or epilepsy.\nUltimately, parameter exploration transforms our model into a versatile tool for understanding the dynamic interplay of synaptic mechanisms. It serves as a gateway to uncovering the mysteries of learning, memory, and neurological disorders while offering practical insights into experimental and therapeutic approaches.\n\n\nCode\n# Define a range of parameter values\nn &lt;- 30\nset.seed(1234)\nvals &lt;- data.table(\n  synapse_id = seq_len(n),\n  S_baseline = runif(n, 0.8, 1.2),\n  t_LTP = runif(n, 0, 20),\n  t_LTD = runif(n, 0, 20),\n  lambda_LTP = runif(n, 1.0, 1.5),\n  lambda_LTD = runif(n, 0.5, 1.0),\n  A_LTP = runif(n, 1.0, 1.5),\n  A_LTD = runif(n, 0.5, 1.0),\n  tau_LTP = runif(n, 15, 20),\n  tau_LTD = runif(n, 10, 25),\n  delta_t = runif(n, 0, 5)\n)\n\nvals &lt;- vals[vals[, list(t = time), synapse_id], on = \"synapse_id\"]\n\nvals[, synapse_strength := extended_synaptic_strength(\n  t, S_baseline, t_LTP, lambda_LTP, A_LTP, tau_LTP, \n  t_LTD, lambda_LTD, A_LTD, tau_LTD, delta_t\n) + rnorm(length(t), 0, 0.05), synapse_id]\n\n# Plot sensitivity to lambda parameters\nggplot(vals, aes(x = t, y = synapse_strength)) +\n  geom_line(aes(group = synapse_id), color = \"gray85\", linewidth = 1/3) +\n  geom_hline(yintercept = 1, col = \"gray20\", linewidth = 1/2, linetype = 2) +\n  stat_summary(geom = \"line\", fun = mean, linewidth = 1, color = \"orange\") +\n  scale_x_continuous(expand = c(0,0,0,0)) +\n  labs(title = \"Simulating Many Synapses: Stochastic Processes\",\n       x = \"Time (arbitrary units)\", y = \"Synaptic Strength\") +\n  annotate(\"text\", x = 17, y = 1.1, label = \"LTP\", size = 6, color = \"darkgreen\") +\n  annotate(\"text\", x = 17, y = 0.9, label = \"LTD\", size = 6, color = \"darkred\")\n\n\n\n\n\nOrange line denotes the mean synaptic strength over time. Horizontal dashed line denotes the zero-change value associated with no variations in synaptic strength. LTP, long-term potentiation; LTD, long-term depression.\n\n\n\n\n\n\n\nFinal Remarks\nEmbarking on this journey through non-linear modeling of neuronal plasticity, we’ve uncovered the intricate dynamics of how synaptic strength shifts over time under the competing forces of LTP and LTD. By leveraging coupled logistic functions, we’ve illuminated the non-linear and time-sensitive nature of synaptic changes, emphasizing the crucial role of calcium oscillations as the molecular metronomes orchestrating these processes.\nThroughout this exploration, several pivotal lessons can be drawn. Logistic functions proved to be elegant tools for capturing the threshold-dependent behavior of synaptic transitions, moving beyond gradual shifts to reveal the tipping points inherent in plasticity. Moreover, we’ve seen how tweaking parameters like calcium amplitude or decay rates can profoundly alter synaptic behavior, echoing the biological reality where minor molecular adjustments yield vastly different functional outcomes. Finally, the extended model demonstrated its robustness by integrating dynamic temporal dependencies and saturation effects, offering a nuanced depiction of synaptic adaptation to stimuli.\nThis is but the tip of the iceberg. The framework we’ve developed invites exploration into countless other physiological processes where non-linear dynamics reign supreme. Consider extending these concepts to other forms of plasticity, such as structural changes in dendrites or the fine-tuning of synaptic homeostasis. Similar models can be applied to cardiac autonomic modulation1, where the balance of sympathetic and parasympathetic activity echoes the interplay of LTP and LTD. Or consider the feedback loops governing hormonal or metabolic regulation, fertile grounds for non-linear interactions.\n1 A big surprise in this regard is coming up for the next post!!For the intrepid explorer of neuronal plasticity, there’s much to be done. Experiment with the model’s parameters, simulate pathological scenarios, or challenge the boundaries of its assumptions. With each iteration, you’ll not only refine your understanding of synaptic plasticity but also contribute to the broader pursuit of understanding life’s complexities through mathematics and computation.\nNeuroscience (or science in general) thrives at the nexus of theory, experimentation, and computation. Non-linear models are indispensable in bridging these fields, revealing the brain’s staggering capacity for adaptation and resilience. Whether you’re a scientist, student, or curious mind, there’s always room to stay inquisitive, tinker with ideas, and push the boundaries of what we know.\nSo, grab your equations, fire up R, and let curiosity lead the way. Biology and mathematics are waiting to converge in ways that can transform how we understand the very essence of life. Stay curious, and never stop exploring!\n\n\n\n\n\n\nReferences\n\nKrystal, John H, Ege T Kavalali, and Lisa M Monteggia. 2024. “Ketamine and Rapid Antidepressant Action: New Treatments and Novel Synaptic Signaling Mechanisms.” Neuropsychopharmacology 49 (1): 41–50.\n\n\nLisman, John, Ryohei Yasuda, and Sridhar Raghavachari. 2012. “Mechanisms of CaMKII Action in Long-Term Potentiation.” Nature Reviews Neuroscience 13 (3): 169–82.\n\n\nMalenka, Robert C. 2003. “The Long-Term Potential of LTP.” Nature Reviews Neuroscience 4 (11): 923–26.\n\n\nScott, Daniel N, and Michael J Frank. 2023. “Adaptive Control of Synaptic Plasticity Integrates Micro-and Macroscopic Network Function.” Neuropsychopharmacology 48 (1): 121–44.\n\n\nYasuda, Ryohei, Yasunori Hayashi, and Johannes W Hell. 2022. “CaMKII: A Central Molecular Organizer of Synaptic Plasticity, Learning and Memory.” Nature Reviews Neuroscience 23 (11): 666–82.\n\nCitationBibTeX citation:@misc{castillo-aguilar2024,\n  author = {Castillo-Aguilar, Matías},\n  title = {Non-Linear Models: {A} {Case} for {Synaptic} {Plasticity}},\n  date = {2024-12-14},\n  url = {https://bayesically-speaking.com/posts/2024-12-10 a-case-for-synaptic-plasticity/},\n  doi = {10.59350/mgpv2-d5e33},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCastillo-Aguilar, Matías. 2024. “Non-Linear Models: A Case for\nSynaptic Plasticity.” December 14, 2024. https://doi.org/10.59350/mgpv2-d5e33."
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html",
    "href": "posts/2024-05-15 mcmc part 2/index.html",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hey there, fellow science enthusiasts and stats geeks! Welcome back to the wild world of Markov Chain Monte Carlo (MCMC) algorithms. This is part two of my series on the powerhouse behind Bayesian Inference. If you missed the first post, no worries! Just hop on over here and catch up before we dive deeper into the MCMC madness. Today, we’re exploring the notorious Hamiltonian Monte Carlo (HMC), a special kind of MCMC algorithm that taps into the dynamics of Hamiltonian mechanics.\n\n\nHold up, did you say Hamiltonian mechanics? What in the world do mechanics and physics have to do with Bayesian stats? I get it, it sounds like a mashup of your wildest nightmares. But trust me, this algorithm sometimes feels like running a physics simulation in a statistical playground. Remember our chat from the last post? In Bayesian stats, we’re all about estimating the shape of a parameter space, aka the posterior distribution.\n\nFun fact: There’s this whole field called statistical mechanics where scientists mix stats and physics to solve cool problems, mostly related to thermodynamics and quantum mechanics.\n\n\n\n\nPicture this: You drop a tiny particle down a cliff, and it rolls naturally along the landscape’s curves and slopes. Easy, right? Now, swap out the real-world terrain for a funky high-dimensional probability function. That same little particle? It’s cruising through this wild statistical landscape like a boss, all thanks to the rules of Hamiltonian mechanics.\n\n\n\n\n\n\n\n\n\n\nAbout the animation\n\n\n\n\n\nThe previous animation illustrate the Hamiltonian dynamics of a particle traveling a two-dimensional parameter space. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo"
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#stats-meets-physics",
    "href": "posts/2024-05-15 mcmc part 2/index.html#stats-meets-physics",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Hold up, did you say Hamiltonian mechanics? What in the world do mechanics and physics have to do with Bayesian stats? I get it, it sounds like a mashup of your wildest nightmares. But trust me, this algorithm sometimes feels like running a physics simulation in a statistical playground. Remember our chat from the last post? In Bayesian stats, we’re all about estimating the shape of a parameter space, aka the posterior distribution.\n\nFun fact: There’s this whole field called statistical mechanics where scientists mix stats and physics to solve cool problems, mostly related to thermodynamics and quantum mechanics."
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#a-particle-rolling-through-stats-land",
    "href": "posts/2024-05-15 mcmc part 2/index.html#a-particle-rolling-through-stats-land",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Picture this: You drop a tiny particle down a cliff, and it rolls naturally along the landscape’s curves and slopes. Easy, right? Now, swap out the real-world terrain for a funky high-dimensional probability function. That same little particle? It’s cruising through this wild statistical landscape like a boss, all thanks to the rules of Hamiltonian mechanics.\n\n\n\n\n\n\n\n\n\n\nAbout the animation\n\n\n\n\n\nThe previous animation illustrate the Hamiltonian dynamics of a particle traveling a two-dimensional parameter space. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo"
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#wrapping-our-heads-around-the-math",
    "href": "posts/2024-05-15 mcmc part 2/index.html#wrapping-our-heads-around-the-math",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "Wrapping Our Heads Around the Math",
    "text": "Wrapping Our Heads Around the Math\nOkay, I know Hamiltonian dynamics can be a real brain-buster, trust me, it took me a hot minute to wrap my head around it. But hey, I’ve got an analogy that might just make it click. Let’s revisit our swing scenario: remember our picture of a kid on a swing, right? The swing’s angle from the vertical (\\(q\\)) tells us where the kid is, and momentum (\\(p\\)) is how fast the swing’s moving.\nNow, let’s break down those equations:\n\\[\n\\frac{{dq}}{{dt}} = \\frac{{\\partial H}}{{\\partial p}}\n\\]\nThis one’s like peeking into the future to see how the angle (\\(q\\)) changes over time. And guess what? It’s all about momentum (\\(p\\)). The faster the swing’s going, the quicker it swings back and forth, simple as that!\nNext up:\n\\[\n\\frac{{dp}}{{dt}} = -\\frac{{\\partial H}}{{\\partial q}}\n\\]\nNow, this beauty tells us how momentum (\\(p\\)) changes over time. It’s all about the energy game here, specifically, how the swing’s position (\\(q\\)) affects its momentum. When the swing’s at the highest point, gravity’s pulling hardest, ready to send him back down.\nSo, picture this:\n\nThe kid swings forward, so the angle (\\(q\\)) goes up thanks to the momentum (\\(p\\)) building until bam, top of the swing.\nAt the top, the swing’s momentarily still, but gravity’s pulling to send him flying back down, hence, he is accumulating potential energy.\nZoom! Back down it goes, picking up speed in the opposite direction, and so, the potential energy is then transferred into kinetic energy.\n\nAll the while, the Hamiltonian (\\(H\\)) is keeping tabs on the swing’s total energy, whether it’s zooming at the bottom (high kinetic energy \\(K(p)\\), as a function of momentum \\(p\\)) or pausing at the top (high potential energy \\(U(q)\\), as a function of position \\(q\\)).\nThis dance between kinetic and potential energy is what we care within Hamiltonian mechanics, and also what we mean when we refer to the phase space, which it’s nothing more than the relationship between position and momentum."
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#visualizing-hamiltons-equations",
    "href": "posts/2024-05-15 mcmc part 2/index.html#visualizing-hamiltons-equations",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "Visualizing Hamilton’s Equations",
    "text": "Visualizing Hamilton’s Equations\nOkay, I know we’re diving into some physics territory here in a stats blog, but trust me, understanding these concepts is key to unlocking what HMC’s all about. So, let’s take a little side trip and get a feel for Hamilton’s equations with a different example. Check out the gif below, see that weight on a string? It’s doing this cool back-and-forth dance thanks to the tug-of-war between the string pulling up and gravity pulling down.\n\n\n\nSimple harmonic oscillator. Within this example we could expect that the potential energy \\(U(q)\\) is the greatest at the bottom or top positions \\(q\\), primarely because is in these positions that the force exerted by the string is greater, affecting in consequence the kinetic energy \\(K(p)\\) of the mass attached at the bottom of the string.\n\n\nNow, let’s get a little hands-on with some code. We’re gonna simulate a simple harmonic oscillator (you know, like that weight on a string) and watch how it moves through phase space.\n\n# Define the potential energy function (U) and its derivative (dU/dq)\nU &lt;- function(q) {\n  k &lt;- 1  # Spring constant\n  return(0.5 * k * q^2)\n}\n\n\ndU_dq &lt;- function(q) {\n  k &lt;- 1  # Spring constant\n  return(k * q)\n}\n\n# Kinetic energy (K) used for later\nK &lt;- function(p, m) {\n  return(p^2 / (2 * m))\n}\n\n# Introduce a damping coefficient\nb &lt;- 0.1  # Damping coefficient\n\n# Set up initial conditions\nq &lt;- -3.0  # Initial position\np &lt;- 0.0   # Initial momentum\nm &lt;- 1.0   # Mass\n\n# Time parameters\nt_max &lt;- 20\ndt &lt;- 0.1\nnum_steps &lt;- ceiling(t_max / dt)  # Ensure num_steps is an integer\n\n# Initialize arrays to store position and momentum values over time\nq_values &lt;- numeric(num_steps)\np_values &lt;- numeric(num_steps)\n\n# Perform time integration using the leapfrog method\nfor (i in 1:num_steps) {\n  # Store the current values\n  q_values[i] &lt;- q\n  p_values[i] &lt;- p\n  \n  # Half step update for momentum with damping\n  p_half_step &lt;- p - 0.5 * dt * (dU_dq(q) + b * p / m)\n  \n  # Full step update for position using the momentum from the half step\n  q &lt;- q + dt * (p_half_step / m)\n  \n  # Another half step update for momentum with damping using the new position\n  p &lt;- p_half_step - 0.5 * dt * (dU_dq(q) + b * p_half_step / m)\n}\n\n\nIn this code:\n\nU is the potential energy function, kinda like how much energy’s stored in that spring, if you will.\nK is the kinetic energy function, telling us how much energy’s tied up in the speed of the weight.\ndU_dq is telling us about how the potential energy changes with the weight’s position, aka the force.\nb is just a fancy way of saying how much energy the system loses over time.\nq and p are the weight’s position and momentum, respectively.\nm is the weight’s mass, nothing fancy.\ndt is the time step, and num_steps? Well, that’s just how long we’re gonna keep this simulation running.\nOh, and that leapfrog integration? It’s like stepping through time, updating the momentum, then the position, and back again, but don’t worry about it, we’ll see it shortly.\n\n\n\n\nCode\nharmonic_data &lt;- data.table(`Position (q)` = q_values, `Momentum (p)` = p_values)\n\nggplot(harmonic_data, aes(`Position (q)`, `Momentum (p)`)) +\n  geom_path(linewidth = .7) +\n  geom_point(size = 2) +\n  geom_point(data = harmonic_data[1,], col = \"red\", size = 3) +\n  labs(title = \"Phase Space Trajectory of Hamiltonian Dynamics\",\n       subtitle = \"Accounting for Energy Loss\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\n\n\n\n\n\nCheck out this funky phase space trajectory of a simple harmonic oscillator! At the furthest points, the potential energy is the highest, that’s what’s driving the oscillator back and forth. But in the middle? That’s where the kinetic energy’s taking over, keeping things moving, like our example of a kid on a swing!\n\nNow, take a look at that graphic. See how the position (\\(q\\)) is all about where the oscillator’s hanging out, and momentum (\\(p\\))? Well, that’s just how fast the weight’s swinging. Put them together, and you’ve got what we call the phase space. Basically, it’s like peeking into the dance floor of these mechanical systems through the lenses of Hamiltonian dynamics.\nNow, in a perfect world, there’d be no energy lost over time. But hey, we like to keep it real, so we added a little something called damping effect (think of it like energy leaking out of the system over time). In the real world, that makes sense, but in our statistical playground, we want to keep that energy locked in tight. After all, losing energy means we’re losing precious info about our target distribution, and nobody wants that.\n\n\nCode\nhamiltonian &lt;- harmonic_data[, list(`Total energy` = U(`Position (q)`) + K(`Momentum (p)`, m),\n                                    `Kinetic energy` = K(`Momentum (p)`, m), \n                                    `Potential energy` = U(`Position (q)`))]\nhamiltonian &lt;- melt(hamiltonian, measure.vars = c(\"Total energy\", \"Kinetic energy\", \"Potential energy\")) \n\nggplot(hamiltonian, aes(rep(1:200, times = 3), value, col = variable)) +\n  geom_line(linewidth = 1) +\n  labs(y = \"Energy\", col = \"Variable\", x = \"Time\",\n       title = \"Fluctuation of the Total Energy in the Oscillator\",\n       subtitle = \"As a Function of Kinetic and Potential Energy\") +\n  scale_color_brewer(type = \"qual\", palette = 2) +\n  theme_classic(base_size = 20) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\nCheck out this energy rollercoaster! This graph’s showing us how the total energy (made up of kinetic and potential energy) changes over time. And yep, that damping effect? It’s keeping things realistic, but in stats land, we’re all about conserving that energy for our exploration.\n\nSo, what’s the big takeaway here? Well, whether it’s a ball rolling down a hill or a sampler hunting for model coefficients, this framework’s got us covered. In Bayesian land, think of our model’s parameters as position coordinates \\(q\\) in some space, and \\(p\\) is the momentum helping us navigate the twists and turns of this parameter space. And with Hamiltonian dynamics leading the way, we’re guaranteed to find our path through this statistical dimension, one step at a time."
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#cooking-up-some-data",
    "href": "posts/2024-05-15 mcmc part 2/index.html#cooking-up-some-data",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "Cooking Up Some Data",
    "text": "Cooking Up Some Data\nBut before we dive into the statistical kitchen, let’s whip up some synthetic data. Picture this: we’re mimicking a real-world scenario where relationships between variables are as murky as a foggy morning. So, we’re gonna conjure up a batch of data with a simple linear relationship, jazzed up with a sprinkle of noise. Oh, and let’s keep it small (just 20 subjects), ’cause, hey, science “loves” a manageable sample size.\n\nEmphasis on double quotes on “loves”.\n\n\nset.seed(80) # Set seed for reproducibility\n\n# Define the number of data points and the range of independent variable 'x'\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\n\nOkay, so now let’s get down to business and fit ourselves a nice, cozy linear relationship between an independent variable (\\(x\\)) and a dependent variable (\\(y\\)). We’re talking about laying down a straight line that best describes how \\(y\\) changes with \\(x\\).\nSo, what’s our equation look like? Well, it’s pretty simple:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\cdot x_i \\\\\n\\end{aligned}\n\\]\nHold on, let’s break it down. We’re saying that each \\(y\\) value (\\(y_i\\)) is chillin’ around a mean (\\(\\mu_i\\)), like a bunch of friends at a party. And guess what? They’re all acting like good ol’ normal folks, hanging out with a variance (\\(\\sigma^2\\)) that tells us how spread out they are. Now, the cool part is how we define \\(\\mu_i\\). It’s just a simple sum of an intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)) times \\(x_i\\). Think of it like plotting points on graph paper, each \\(x_i\\) tells us where we are on the \\(x\\)-axis, and multiplying by \\(\\beta_1\\) gives us the corresponding height on the line.\nNow, let’s talk numbers. We’re setting \\(\\beta_0\\) to 2 because, hey, every relationship needs a starting point, right? And for \\(\\beta_1\\), we’re going with 3 (that’s the rate of change we’re expecting for every unit increase in \\(x\\). Oh, and let’s not forget about \\(\\sigma\\)), that’s just a fancy way of saying how much our \\(y\\) values are allowed to wiggle around.\n\n# Define the true parameters of the linear model and the noise level\ntrue_intercept &lt;- 2\ntrue_slope &lt;- 3\nsigma &lt;- 5\n\nWith our model all set up, it’s time to create some data points for our \\(y\\) variable. We’ll do this using the rnorm() function, which is like a magical data generator for normally distributed variables.\n\n# Generate the dependent variable 'y' with noise\nmu_i = true_intercept + true_slope * x\ny &lt;- rnorm(n, mu_i, sigma)"
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#choosing-a-target-distribution",
    "href": "posts/2024-05-15 mcmc part 2/index.html#choosing-a-target-distribution",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "Choosing a Target Distribution",
    "text": "Choosing a Target Distribution\nAlright, now that we’ve got our hands on some data, it’s time to dive into the nitty-gritty of Bayesian stuff. First up, we’re gonna need our trusty log likelihood function for our linear model. This function’s like the Sherlock Holmes of statistics, it figures out the probability of seeing our data given a specific set of parameters (you know, the intercept and slope we’re trying to estimate).\n\n# Define the log likelihood function for linear regression\nlog_likelihood &lt;- function(intercept, slope, x, y, sigma) {\n  \n  # We estimate the predicted response\n  y_pred &lt;- intercept + slope * x \n  \n  # Then we see how far from the observed value we are\n  residuals &lt;- y - y_pred\n  \n  # Then we estimate the likelihood associated with that error from a distribution\n  # with no error (mean = 0)\n  # (this is the function that we are trying to maximize)\n  log_likelihood &lt;- sum( dnorm(residuals, mean = 0, sd = sigma, log = TRUE) )\n  \n  return(log_likelihood)\n}\n\nSo, what’s the deal with priors? Well, think of them as the background music to our data party. They’re like our initial hunches about what the parameters could be before we’ve even glanced at the data. To keep things simple, we’ll go with flat priors (no favoritism towards any particular values). It’s like saying, “Hey, let’s give everyone a fair shot!”\n\n# Define the log prior function for the parameters\nlog_prior &lt;- function(intercept, slope) {\n  # Assuming flat priors for simplicity\n  # (the log of 0 is 1, so it has no effect)\n  return(0) \n}\n\nNow, here’s where the real magic kicks in. We bring our likelihood and priors together in a beautiful dance to reveal the superstar of Bayesian statistics: the posterior distribution! This bad boy tells us everything we wanna know after we’ve taken the data into account. This allow us to take into account previous knowledge (like past research, and the observed data) let’s say, samples from a new experiment.\nThe posterior it’s nothing more than the probability associated with our parameters of interest (aka. the slope and intercept), given the observed data. We represent this posterior distribution as the following:\n\\[\nP(\\text{X}|\\text{Data}) \\propto P(\\text{Data}|\\text{X}) \\times P(\\text{X})\n\\]\nWhich is the same as saying:\n\\[\n\\text{Posterior} \\propto \\text{Likelihood} \\times \\text{Prior}\n\\]\n\n# Combine log likelihood and log prior to get log posterior\nlog_posterior &lt;- function(intercept, slope, x, y, sigma) {\n  return(log_likelihood(intercept, slope, x, y, sigma) + log_prior(intercept, slope))\n}"
  },
  {
    "objectID": "posts/2024-05-15 mcmc part 2/index.html#building-the-hmc-sampler",
    "href": "posts/2024-05-15 mcmc part 2/index.html#building-the-hmc-sampler",
    "title": "The Good, The Bad, and Hamiltonian Monte Carlo",
    "section": "Building the HMC sampler",
    "text": "Building the HMC sampler\nAlright, now that we have everything ready, let’s dig into the guts of HMC and how we put it to work. Remember how HMC takes inspiration from the momentum and potential energy dance in physics? Well, in practice, it’s like having a GPS for our parameter space, guiding us to new spots that are more likely than others.\nBut here’s the thing: our parameter space isn’t some smooth highway we can cruise along endlessly. Nope, it’s more like a rugged terrain full of twists and turns. So, how do we navigate this space? Enter the leapfrog integration method, the backbone of HMC.\n\nLeapfrog Integration\nSo, leapfrog integration is basically this cool math trick we use in HMC to play out how a system moves over time using discrete steps, so we don’t have to compute every single value along the way. This integration method also is advantageous by not allowing energy leaks out of the system (remember the Hamiltonian?) which is super important if you don’t want to get stuck in this statistical dimension mid exploration.\nHere’s how it works in HMC-lingo: we use leapfrog integration to move around the parameter space in discrete steps (rather than sliding through a continuum), and grabbing samples from the posterior distribution. The whole process goes like this:\n\nWe give the momentum \\(p\\) a little nudge by leveraging on the gradient info (\\(\\nabla\\)). The gradient or slope of the position (\\(\\nabla U(q)\\)) in this parameter space will determine by how much our momentum will change. Like when we are in the top position in the swing, the potential energy then transfers to kinetic energy (aka. momentum).\nWe adjust the position (or parameters) based on the momentum boost.\nThen we update the momentum based on the gradient on that new position.\nWe repeat the cycle for as many “jumps” we are doing, for each sample of the posterior we intend to draw.\n\nPicture a frog hopping from one lily pad to another, that’s where the name “leapfrog” comes from. It helps us explore new spots in the parameter space by discretizing the motion of this imaginary particle by using Hamiltonian dynamics, using the slope information (\\(\\nabla U(q)\\)) of the current position \\(q\\) to gain/loss momentum \\(p\\) and move to another position \\(q\\) in the parameter space.\n\n\n\nA frog jumping, with a fixed step size, from one point in the phase space into another.\n\n\nWe prefer leapfrogging over simpler methods like Euler’s method because it keeps errors low, both locally and globally. This stability is key, especially when we’re dealing with big, complicated systems. Plus, it’s a champ at handling high-dimensional spaces, where keeping energy in check is a must for the algorithm to converge.\n\n\nTuning Those Hamiltonian Gears\nNow, to get our HMC sampler purring like a kitten, we’ve got to fine-tune a few gears. Think of these as the knobs and dials on your favorite sound system – adjust them just right, and you’ll be grooving to the perfect beat.\nFirst up, we’ve got the number of samples. This determines how many times our sampler will take a peek at the parameter space before calling it a day.\nNext, we’ve got the step size (\\(\\epsilon\\)). Imagine this as the stride length for our leapfrog integrator. Too short, and we’ll be tiptoeing; too long, and we’ll be taking giant leaps – neither of which gets us where we want to go. It’s all about finding that sweet spot.\nThen, there’s the number of steps for the leapfrog to make. Too few, and we risk missing key spots; too many, and we might tire ourselves out.\nLastly, we need an initial guess for the intercept and slope. This is like dropping a pin on a map – it gives our sampler a starting point to begin its journey through the parameter space.\n\n# Initialization of the sampler\nnum_samples &lt;- 5000  # Number of samples\nepsilon &lt;- 0.05  # Leapfrog step size\nnum_steps &lt;- 50  # Number of leapfrog steps\ninit_intercept &lt;- 0  # Initial guess for intercept\ninit_slope &lt;- 0  # Initial guess for slope\n\n# Placeholder for storing samples\nparams_samples &lt;- matrix(NA, nrow = num_samples, ncol = 2)\n\n\n\nStarting the Sampler\nAlright, let’s fire up this Hamiltonian engine and get this party started. Here’s the game plan:\n\nCreate a Loop: We’ll set up a loop to simulate our little particle moving around the parameter space.\nIntegrate Its Motion: Using our trusty leapfrog integrator, we’ll keep track of how our particle moves.\nGrab a Sample: Once our particle has finished its dance, we’ll grab a sample at its final position.\nAccept or Reject: We’ll play a little game of accept or reject – if the new position looks promising, we’ll keep it; if not, we’ll stick with the old one. It’s like Tinder for parameters.\nRepeat: We’ll rinse and repeat this process until we’ve collected as many samples as we need.\n\nNow, to kick things off, we’ll give our imaginary particle a random speed and direction to start with, and drop it down somewhere in the parameter space. This initial kick sets the stage for our parameter exploration, the rest is up to the physics.\nfor (i in 1:num_samples) {\n  # Start with a random momentum of the particle\n  momentum_current &lt;- rnorm(2)\n  \n  # And set the initial position of the particle\n  params_proposed &lt;-c(init_intercept, init_slope) \n  \n  # Next, we will simulate the particle's motion using\n  # leapfrog integration.\n  ...\n\nHere, I’m using the concept of a particle moving through a probability space. But remember, in reality, we’re talking about the parameter space, where each coefficient in a model parameter or any other parameter that we can represent in this way then becomes a coordinate, therefore, a position \\(q\\) in this statistical space.\n\n\n\nSimulating the Particle’s Motion\nNow that our imaginary particle is all geared up with an initial momentum and position, it’s time to let it loose and see where it goes in this parameter playground. We know we’re using Hamiltonian mechanics, but to make it computationally feasible, we’re bringing in our trusty leapfrog integrator. We’ve already seen that this bad boy discretizes the motion of our particle, making it manageable to track its journey without breaking our computers.\n\n\n\nMarble rolling over a surface. In a perfect world, we’d love to follow our particle smoothly gliding through the parameter space, but hey, computing that would take ages. So instead, we chunk its movement into smaller steps, kind of like a flipbook, to keep tabs on its position.\n\n\nSo, here’s the lowdown on what our leapfrog integrator is up to:\n\nEstimating the Slope: We start off with an initial position and estimate the slope of the terrain at that point.\n\n\\[\n\\nabla U(q) = \\frac{\\partial U}{\\partial q}\n\\]\n\nHere, we calculate the gradient \\(\\nabla\\) with respect of the current position \\(q\\)\n\n\nAdjusting Momentum: This slope is like the potential energy, dictating whether our particle speeds up or slows down. So, we tweak the momentum accordingly based on this slope.\n\n\\[\np \\leftarrow p - \\frac{\\epsilon}{2} \\cdot \\nabla U(q)\n\\]\n\nThen, we update the momentum \\(p\\) with the gradient \\(\\nabla U(q)\\) by half step (\\(\\frac{\\epsilon}{2}\\))\n\n\nTaking a Step: With this momentum tweak, we move the particle for a set distance \\(\\epsilon\\), updating its position from the starting point.\n\n\\[\nq \\leftarrow q - \\epsilon \\cdot p\n\\]\n\nRepeat: Now that our particle has a new spot, we rinse and repeat – estimating the slope and adjusting momentum.\n\n\\[\np \\leftarrow p - \\frac{\\epsilon}{2} \\cdot \\nabla U(q)\n\\]\n\nHere, we update momentum \\(p\\) by the other half step size on the new position \\(q\\)\n\n\nKeep Going: We keep this cycle going for as many steps as we want to simulate, tracking our particle’s journey through the parameter space.\n\nfor (j in 1:num_steps) {\n    # Gradient estimation\n    grad &lt;- c(\n      # Gradient of the intercept of X: -sum(residuals / variance)\n      ...\n      # Gradient of the slope of X: -sum(residuals * X / variance)\n      ...\n    )\n    \n    # Momentum half update\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n    \n    # Full step update for parameters\n    params_proposed &lt;- params_proposed + epsilon * momentum_current\n    \n    # Recalculate gradient for another half step update for momentum\n    grad &lt;- c(\n      # Gradient of the intercept of X: -sum(residuals / variance)\n      ...\n      # Gradient of the slope of X: -sum(residuals * X / variance)\n      ...\n    )\n    \n    # Final half momentum update\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n  }\n\n\nSample Evaluation and Acceptance\nAfter our particle has taken its fair share of steps through the parameter space, it’s decision time – should we accept or reject its proposed new position? We can’t just blindly accept every move it makes; we’ve got to be smart about it.\nThat’s where the Metropolis acceptance criteria come into play. This handy rule determines whether a proposed new position is a good fit or not. The idea is to weigh the probability of the new position against the probability of the current one. If the new spot looks promising, we’ll move there with a certain probability, ensuring that our samples accurately reflect the shape of the distribution we’re exploring. But if it’s not a better fit, we’ll stick with where we are.\nThe formula for this acceptance probability (\\(A(q', q)\\)) when transitioning from the current position (\\(q\\)) to a proposed position (\\(q'\\)) is straightforward:\n\\[\nA(q',q) = min(1,\\frac{p(q')}{p(q)})\n\\]\nHere, \\(p(q')\\) is the probability density of the proposed position \\(q'\\), and \\(p(q)\\) is the probability density of the current position \\(q\\). We’re essentially comparing the fitness of the proposed spot against where we’re currently at. If the proposed position offers a higher probability density, we’re more likely to accept it. This ensures that our samples accurately represent the target distribution.\nHowever, when dealing with very small probability values, we might run into numerical underflow issues. That’s where using the log posterior probabilities comes in handy. By taking the logarithm of the probabilities, we convert the ratio into a difference, making it easier to manage. Here’s how the acceptance criteria look with logarithms:\n\\[\n\\begin{aligned}\n\\alpha &= \\log(p(q')) - \\log(p(q)) \\\\\nA(q',q) &= min(1,exp(\\alpha))\n\\end{aligned}\n\\]\nThis formulation is equivalent to the previous one but helps us avoid numerical headaches, especially when working with complex or high-dimensional data. We’re still comparing the fitness of the proposed position with our current spot, just in a more log-friendly way.\n# Calculate log posteriors and acceptance probability\nlog_posterior_current &lt;- log_posterior( ...current parameters... )\nlog_posterior_proposed &lt;- log_posterior( ...proposed parameters... )\n\nalpha &lt;- min(1, exp(log_posterior_proposed - log_posterior_current))\n\n# Accept or reject the proposal\nif (runif(1) &lt; alpha) {\n  init_intercept &lt;- params_proposed[1]\n  init_slope &lt;- params_proposed[2]\n}\n\n\nMixing All Toghether\nNow that we’ve broken down each piece of our HMC puzzle, it’s time to put them all together and see how the full algorithm works.\n\nfor (i in 1:num_samples) {\n  # Randomly initialize momentum\n  momentum_current &lt;- rnorm(2)\n  \n  # Make a copy of the current parameters\n  params_proposed &lt;- c(init_intercept, init_slope)\n  \n  # Perform leapfrog integration\n  for (j in 1:num_steps) {\n    # Half step update for momentum\n    grad &lt;- c(\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) / sigma^2),\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) * x / sigma^2)\n    )\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n    \n    # Full step update for parameters\n    params_proposed &lt;- params_proposed + epsilon * momentum_current\n    \n    # Recalculate gradient for another half step update for momentum\n    grad &lt;- c(\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) / sigma^2),\n      -sum((y - (params_proposed[1] + params_proposed[2] * x)) * x / sigma^2)\n    )\n    momentum_current &lt;- momentum_current - epsilon * grad * 0.5\n  }\n  \n  # Calculate the log posterior of the current and proposed parameters\n  log_posterior_current &lt;- log_posterior(init_intercept, init_slope, x, y, sigma)\n  log_posterior_proposed &lt;- log_posterior(params_proposed[1], params_proposed[2], x, y, sigma)\n  \n  # Calculate the acceptance probability\n  alpha &lt;- min(1, exp(log_posterior_proposed - log_posterior_current))\n  \n  # Accept or reject the proposal\n  if (runif(1) &lt; alpha) {\n    init_intercept &lt;- params_proposed[1]\n    init_slope &lt;- params_proposed[2]\n  }\n  \n  # Save the sample\n  params_samples[i, ] &lt;- c(init_intercept, init_slope)\n}"
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "",
    "text": "Let’s be honest, being a statistician or a scientist isn’t just about crunching numbers all day. It’s more like being a detective, a problem solver, and yeah, throwing in some math for good measure. When we get into non-linear models, things really start to get interesting. We’re not just drawing straight lines anymore; we’re wrestling with curves, untangling complicated relationships, and trying to figure out what’s really going on behind the scenes.\nTake pharmacokinetics, for example. Sounds fancy, right? But at the core, it’s just about what happens to a drug inside the body, and for us, that’s where the real statistical fun begins. Predicting how a drug’s concentration changes in the bloodstream over time isn’t just about plotting some points and calling it a day. It’s about understanding how the data dances with biology, and then figuring out the best way to describe that dance. And let’s not forget the little thrill of choosing your weapon: Frequentist or Bayesian? It’s like deciding between coffee or mate (and let’s be real, I’m down for both). Each one has its perks, but the choice depends on the situation, and maybe how you’re feeling that day.\nIn this post, we’re going to roll up our sleeves and dig into building a non-linear model to predict how something, let’s say, a drug disappears from the bloodstream. But we’re not stopping there. Nope, we’re also going to throw in a showdown between two big players: the frequentist and Bayesian methods. Think of it like a friendly face-off between two old rivals, each with its own style, strengths, and die-hard fans.\nBut here’s the thing: this isn’t just about which method wins on paper. It’s about the real-life, day-to-day grind of working with data. It’s about those moments when you’re staring at your screen, trying to make sense of a stubborn parameter that just won’t cooperate. It’s about knowing when to trust the numbers and when to rely on your gut, your experience, and maybe even a bit of luck.\nSo whether you’re a seasoned pro who’s been around the block or someone just dipping their toes into the world of applied stats, this post is for you. We’re going to dive in, compare, and yeah, maybe even have a little fun along the way. Because in the world of science and stats, the journey is half the adventure."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#designing-a-solution",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#designing-a-solution",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "Designing a solution",
    "text": "Designing a solution\nIt appears that the concentration of indomethacin decreases according to an exponential function of time. We can model this relationship with:\n\\[\n\\begin{aligned}\ny_i &\\sim \\mathcal{N}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\alpha + \\beta \\cdot \\exp(\\lambda \\cdot time_i)\n\\end{aligned}\n\\]\nIn this model, \\(\\lambda\\) represents the rate of decay in the plasma concentration of indomethacin (\\(y_i\\)), starting from a baseline concentration (\\(\\beta\\)). The term \\(\\alpha\\) estimates the minimum plasma concentration level over the observed time frame. We assume that the variance is constant across expected indomethacin plasma concentrations (\\(\\mu\\)) for each subject (\\(i\\)).\nHowever, given that we have repeated measurements from multiple individuals, we need to account for variability in the initial plasma concentrations across subjects. To address this, we introduce a random effect (\\(\\phi_i\\)) into the model:\n\\[\n\\begin{aligned}\n\\mu_i &= \\alpha + (\\beta + \\phi_i) \\cdot \\exp(\\lambda \\cdot time_i)\n\\end{aligned}\n\\]\nHere, \\(\\phi_i\\) represents the deviation from the baseline plasma level (\\(\\beta\\)) for subject \\(i\\). We keep \\(\\lambda\\) and \\(\\alpha\\) fixed because we’re interested in estimating population parameters to describe the pharmacokinetics of the drug."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#nlme-approach",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#nlme-approach",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "nlme approach",
    "text": "nlme approach\nLet’s dive into fitting a non-linear model using the nlme R package, which allows us to specify custom model forms:\n\nnlm_freq &lt;- nlme::nlme(\n  ## Model previously described\n  model = conc ~ alpha + (beta + phi) * exp(lambda * time),\n  data = Indometh,\n  ## Fixed effects\n  fixed = alpha + beta + lambda ~ 1,\n  ## Random effects\n  random = phi ~ 1 | Subject, \n  ## Starting proposal values\n  start = list(fixed = c(0, 2, -1)))\n\n## Confidence intervals for fixed effects\nnlme::intervals(nlm_freq, which = \"fixed\")\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n             lower       est.      upper\nalpha   0.09050133  0.1311638  0.1718262\nbeta    2.36887258  2.8249635  3.2810543\nlambda -1.77509197 -1.6140483 -1.4530047\n\n\nLet’s kick things off by looking at where we stand with indomethacin’s plasma levels. At time zero, we’re seeing baseline concentrations hovering around 2.74 mcg/ml. Not too shabby, right? But this little molecule doesn’t stick around for long, our decay rate clocks in at about \\(\\exp(-1.61) \\approx 0.20\\), which translates to a swift 80% drop in those levels each hour. Eventually, the levels bottom out at around 0.13 mcg/ml, where the decline takes a bit of a breather.\nNow, if you were paying close attention to the code, you might have noticed something interesting: when we’re playing in the frequentist sandbox, particularly with non-linear models, we’ve got to give the algorithm a little nudge with some starting values. It’s like setting up the board for a game, these initial values are where the algorithm begins its quest through the likelihood landscape, hunting down the most likely parameters that explain our data. Remember that previous post about Hamiltonian Monte Carlo? Well, this is a bit like rolling a ball down a hill of parameter space, but here we’re aiming to land on the single spot that maximizes our chances of observing the data we have.\nBut enough with the theory, let’s dive back into our non-linear model and see how these predicted plasma levels measure up against the real-world data we’ve got in hand:\n\nfreq_pred &lt;- predict(nlm_freq)\n\nggplot(cbind(Indometh, pred = freq_pred), aes(time, conc)) +\n  facet_wrap(~ Subject) +\n  labs(y = \"Plasma levels (mcg/ml)\", x = \"Time (hours)\") +\n  geom_point(aes(col = Subject), cex = 3) +\n  geom_line(aes(y = pred, col = Subject), linewidth = 1)\n\n\n\n\n\n\n\n\nOur model does a decent job fitting the observed data. But what’s the story beyond standard errors? How do we quantify uncertainty in our model parameters? What’s the likelihood of observing a specific decay rate or baseline level? With frequentist methods, our insights are somewhat limited to point estimates and standard errors. We need a broader view."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#brms-approach",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#brms-approach",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "brms approach",
    "text": "brms approach\nTo harness the power of the Bayesian framework, we need to not only define our model but also incorporate prior beliefs about the parameters. Let’s revisit our parameters:\n\n\\(\\alpha\\): Minimum plasma levels of the drug.\n\\(\\beta\\): Baseline plasma levels at time zero.\n\\(\\phi\\): Subject-specific deviation from the population \\(\\beta\\).\n\\(\\lambda\\): The amount of exponential decay.\n\nWe’ll assign prior distributions based on prior knowledge and results from our frequentist model. Here’s the prior setup:\n\nFor \\(\\alpha\\), we’ll use a normal distribution centered around 0.1 mcg/ml with a standard deviation of 0.5 mcg/ml. We’ll truncate this prior at zero, since negative plasma levels aren’t physically meaningful.\nFor \\(\\beta\\), we’ll specify a normal distribution centered around 2.5 mcg/ml with a standard deviation of 3 mcg/ml to avoid overly restricting the parameter space. We’ll also set a lower bound of zero.\nFor \\(\\phi\\), we’ll use a normal prior centered on 0.5 mcg/ml with a moderate standard deviation of 2 mcg/ml to capture variability around baseline levels.\nFor \\(\\lambda\\), we’ll set a weakly informative prior centered around -1 with a standard deviation of 3. This reflects our expectation of a negative decay rate, with the upper bound fixed at zero to prevent increases in plasma levels.\n\nThe priors for our model parameters are:\n\\[\n\\begin{aligned}\n\\alpha &\\sim \\mathcal{N}(0.1, 0.5) \\\\\n\\beta &\\sim \\mathcal{N}(2.5, 3.0) \\\\\n\\phi &\\sim \\mathcal{N}(0.5, 2.0) \\\\\n\\lambda &\\sim \\mathcal{N}(-1.0, 3.0) \\\\\n\\end{aligned}\n\\]\nSo now that we have what we need we can already proceed to fit our bayesian non-linear model:\n\nnlme_brms &lt;- brm(\n  ## Formula\n  formula = bf(conc ~ alpha + (beta + phi) * exp(lambda * time),\n               alpha + beta + lambda ~ 1, phi ~ 1 | Subject,\n               nl = TRUE),\n  data = Indometh,\n  ## Priors\n  prior = prior(normal(0.1, 0.5), nlpar = \"alpha\", lb = 0) +\n    prior(normal(2.5, 3.0), nlpar = \"beta\", lb = 0) +\n    prior(normal(0.5, 2.0), nlpar = \"phi\") +\n    prior(normal(-1.0, 3.0), nlpar = \"lambda\", ub = 0),\n  ## MCMC hyperparameters\n  chains = 5, iter = 4000, \n  warmup = 2000, cores = 5,\n  ## More flexible exploration parameters\n  control = list(adapt_delta = 0.99, \n                 max_treedepth = 50),\n  ## For reproducibility\n  seed = 1234, file = \"nlme_brms.RDS\"\n)\n\nfixef(nlme_brms)\n\n                   Estimate  Est.Error        Q2.5      Q97.5\nalpha_Intercept   0.1346163 0.02173654  0.09162845  0.1768141\nbeta_Intercept    2.6519527 1.44843180  0.24992601  5.7345653\nlambda_Intercept -1.6448947 0.09467393 -1.83556844 -1.4659436\nphi_Intercept     0.2150963 1.44471628 -2.84981889  2.6512578\n\n\nIn this Bayesian model, we get not just point estimates but full distributions for each parameter. This approach allows us to explore the probable range of parameter values and answer probabilistic questions. But first, let’s see how well our Bayesian model fits the observed data:\n\nbmrs_pred &lt;- predict(nlme_brms)\n\nggplot(cbind(Indometh, bmrs_pred), aes(time, conc)) +\n  facet_wrap(~ Subject) +\n  labs(y = \"Plasma levels (mcg/ml)\", x = \"Time (hours)\") +\n  geom_point(aes(col = Subject), cex = 3) +\n  geom_line(aes(y = Estimate, col = Subject), linewidth = 1) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, fill = Subject), alpha = .3)\n\n\n\n\n\n\n\n\nThe Bayesian model’s fitted effects align nicely with the observed data, and the uncertainty around the expected plasma concentrations is well-represented. To explore the range of parameters compatible with our data, we can plot the posterior distributions:\n\nplot(nlme_brms, variable = \"^b_\", regex = TRUE)\n\n\n\n\n\n\n\n\nThese plots reveal a spectrum of parameter values that fit the observed data. For the decay parameter \\(\\lambda\\), we can expect a 78% (\\(1 - \\exp(-1.5)\\)) to 83% (\\(1 - \\exp(-1.8)\\)) decrease in indomethacin plasma concentrations per hour.\nWe can further explore the posterior distributions. For example, transforming the \\(\\lambda\\) parameter into a percentage decay scale:\n\nposterior_dist &lt;- as_draws_df(nlme_brms, variable = \"b_lambda_Intercept\")\nposterior_dist$prob_decay &lt;- (1 - exp(posterior_dist$b_lambda_Intercept))\n\nggplot(posterior_dist, aes(x = prob_decay)) +\n  tidybayes::stat_halfeye(fill = \"lightblue\") +\n  labs(y = \"Density\", x = \"Decay of plasma levels per hour (%)\") +\n  scale_x_continuous(labels = scales::label_percent(), n.breaks = 8)\n\n\n\n\n\n\n\n\nThis flexibility in the Bayesian framework allows us to interpret the decay rate in more intuitive terms, reflecting a range of plausible rates consistent with our data. We can now communicate the percent decay of indomethacin plasma levels in a more accessible manner, considering the variability captured by our model."
  },
  {
    "objectID": "posts/2024-08-05 non-linear-models part 1/index.html#comparing-models",
    "href": "posts/2024-08-05 non-linear-models part 1/index.html#comparing-models",
    "title": "Non-linear models: Pharmacokinetics and Indomethacin",
    "section": "Comparing models",
    "text": "Comparing models\nNow that we’ve implemented linear, non-linear, and Bayesian non-linear models, it’s time to compare their performances. It’s important to remember that each model has its own set of performance metrics, which can make direct comparisons tricky. However, by calculating the root mean square error (RMSE), we can get a sense of the average error each model makes when predicting plasma levels. RMSE gives us a tangible measure of error on the same scale as our predictor, helping us gauge how well each model is performing:\n\ndata.frame(\n  lm = performance::performance_rmse(lm_freq),\n  nlme = performance::performance_rmse(nlm_freq),\n  brms = performance::performance_rmse(nlme_brms)\n)\n\n         lm      nlme      brms\n1 0.4417804 0.1080546 0.1077859\n\n\nHere, we can see that both the frequentist and Bayesian non-linear models outperformed the simple linear model by a significant margin. The lower RMSE values indicate a better overall fit. Interestingly, the Bayesian model edged out the frequentist model by a tiny margin, with a difference of just 0.000402 mcg/ml in RMSE (\\(\\text{RMSE}(M_{nlme}) - \\text{RMSE}(M_{brms})\\)). Given that the standard deviation of the plasma levels is 0.63 mcg/ml, this difference is practically negligible and unlikely to be meaningful in a real-world context."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Services"
  },
  {
    "objectID": "services.html#every-problem-is-an-oportunity",
    "href": "services.html#every-problem-is-an-oportunity",
    "title": "Services",
    "section": "Every problem is an oportunity",
    "text": "Every problem is an oportunity\nIf you are having issues with your statistical analyses, research or thesis project, say no more. Here at Bayesically-Speaking we believe in a collaborative and supporting environment, providing consulting services throughout your stats problem.\nWhether you need assistance solving methodological issues, sample size power, interpreting complex models or just in need to wrap your head around statistical concepts, you are in the right place."
  },
  {
    "objectID": "services.html#why-choose-us",
    "href": "services.html#why-choose-us",
    "title": "Services",
    "section": "Why choose us?",
    "text": "Why choose us?\n\n\nExperience\nWe have worked across diverse industries, from healthcare to political sciences.\n\n\nCollaboration\nWe partner with you to understand your unique needs and tailor solutions.\n\n\nResults-Driven\nWe don’t just crunch numbers; we deliver actionable insights.\n\n\nPeople-Centered\nWe are compromised to deliver solutions for non-statisticians."
  },
  {
    "objectID": "services.html#lets-work-together",
    "href": "services.html#lets-work-together",
    "title": "Services",
    "section": "Let’s work together!",
    "text": "Let’s work together!\n Send us an email!"
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html",
    "href": "posts/2023-05-30 welcome/index.html",
    "title": "Welcome to Bayesically Speaking",
    "section": "",
    "text": "Photo from Jon Tyson at Unsplash."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#the-statistics-toolbox",
    "href": "posts/2023-05-30 welcome/index.html#the-statistics-toolbox",
    "title": "Welcome to Bayesically Speaking",
    "section": "The statistics toolbox",
    "text": "The statistics toolbox\nWithin the statistics toolbox, we have commonly used tests like t-tests, ANOVA, correlations, and regression. These methods have their advantages, as they are relatively easy to use and understand. However, like any other tool, they also have their limitations. For instance, they struggle with scenarios involving variables with asymmetric distributions, non-linear relationships, unbalanced groups, heterogeneous variance, extreme values, or repeated measurements with loss of follow-up.\nTo address these limitations, non-parametric alternatives have been developed. These approaches offer flexibility but make it challenging to extrapolate inferences to new data due to the lack of distributional parameters. Other models, such as neural networks or random forest models, provide assistance when analyzing data with special properties. However, they often sacrifice simplicity and interpretability for increased flexibility and are commonly referred to as “black box” models.\n\n\n\nJust between us, I only put this picture because it looked cool. Photo from Dan Cristian Pădureț at Unsplash.\n\n\nDespite the availability of these alternative methods, there is still a pressing need to incorporate previous knowledge and align with the way human understanding is constructed. As humans, our perception of the world is shaped by experiences and prior beliefs. This is where Bayesian statistics come into play.\nBayesian statistics offer several advantages over classical statistics (also known as “frequentist”). Firstly, they provide a coherent framework for incorporating prior information into our analysis, enabling us to update our beliefs systematically. Additionally, Bayesian statistics allow us to quantify uncertainty through probability distributions, offering a more intuitive and interpretable way to express our findings and the degree of certainty."
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#computing-the-posterior",
    "href": "posts/2023-05-30 welcome/index.html#computing-the-posterior",
    "title": "Welcome to Bayesically Speaking",
    "section": "Computing the posterior",
    "text": "Computing the posterior\nTo estimate the posterior probability of getting heads after tossing a coin, we can use the Bayesian framework. Let’s denote the probability of getting heads in a coin toss as \\(P(H)\\).\nAccording to the information provided, we have the prior probability of \\(P(H)\\) estimated from an independent experiment as 10 heads out of 15 tosses. This can be written as a Beta distribution:\n\n\nThis symbol “\\(\\sim\\)” means distributed as\n\\[\nP(H) \\sim Beta(10, 5)\n\\]\nHere, the Beta distribution parameters are (10, 5) since we had 10 heads and 5 tails in the prior experiment.\nNow, a new experiment with the same 15 tosses gives us 2 heads. To update our prior belief, we can use this information to calculate the posterior probability which can be expressed as follow:\n\n\nThis symbol “\\(\\propto\\)” means proportional to\n\\[\nP(H | Data) \\propto P(Data | H) \\times P(H)\n\\]\nWhich is equivalent as saying:\n\\[\nPosterior \\propto Likelihood \\times Prior\n\\]\nTo calculate the posterior probability, we need to normalize the product of the likelihood and prior, which involves integrating over all possible values of H. However, in this case, we can use a shortcut because the prior distribution is conjugate to the binomial distribution, so the posterior distribution will also follow a Beta distribution:\n\n\n\n\n\n\nAbout normalization\n\n\n\n\n\nThe product of both the prior and the likelihood maintains the same shape as the final posterior probability distribution, indicated by the “proportional to” (\\(\\propto\\)) in the previous equation. However, this raw product does not sum up to 1, making it an improper probability density function. To rectify this, the raw product needs to be normalized using integration or simulation in most cases.\n\n\n\n\\[\nP(H | Data) \\sim Beta(10 + 2, 5 + 13)\n\\]\nAfter incorporating the data from the new experiment, the parameters of the Beta distribution become (12, 18) since we had 2 heads and 13 tails in the new experiment, meaning 12 heads and 18 tails in total.\n\n\n\n\n\n\nAbout conjugacy\n\n\n\n\n\nWhen we choose a Beta distribution as our prior belief and gather new data from a coin toss, an intriguing property emerges: the posterior distribution also follows a Beta distribution. This property, known as conjugacy, offers a valuable advantage by simplifying calculations. It acts as a mathematical shortcut that saves time and effort, making the analysis more efficient and streamlined.\n\n\n\nTo calculate the posterior probability of getting heads, we can consider the mode (maximum) of the Beta distribution, which is \\((a - 1) / (a + b - 2)\\):\n\n\\(\\begin{aligned}\nP(H | Data)    &= (12 - 1) / (12 + 18 - 2) \\\\\n               &= 11 / 28 \\\\\n               &\\approx 0.39\n\\end{aligned}\\)\n\nTherefore, the posterior probability of getting heads is approximately 39% when we consider all the available evidence.\n\n\nCode\n# Prior and Likelihood functions\ndata = function(x, to_log = FALSE) dbeta(x, 2, 13, log = to_log)\nprior = function(x, to_log = FALSE) dbeta(x, 10, 5, log = to_log)\n\n# Posterior\nposterior = function(x) {\n  p_fun = function(i) {\n    # Operation is on log-scale merely for computing performance\n    # and minimize rounding errors giving the small nature of\n    # probability density values at each interval.\n    i_log = data(i, to_log = TRUE) + prior(i, to_log = TRUE)\n    # Then transformed back to get probabilities again\n    exp(i_log)\n  }\n  \n  # Then we integrate using base function `integrate`\n  const = integrate(f = p_fun, \n                    lower = 0L,  upper = 1L, \n                    subdivisions = 1e3L,\n                    rel.tol = .Machine$double.eps)$value\n  p_fun(x) / const\n}\n\n## Plotting phase\n\n### Color palette\ncol_pal &lt;- c(Prior = \"#DEEBF7\", \n             Data = \"#3182BD\", \n             Posterior = \"#9ECAE1\")\n### Main plotting code\nggplot() +\n  #### Main probability density functions\n  stat_function(aes(fill = \"Data\"), fun = data, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Prior\"), fun = prior, geom = \"density\", alpha = 1/2) +\n  stat_function(aes(fill = \"Posterior\"), fun = posterior, geom = \"density\", alpha = 1/2) +\n  #### Minor aesthetics tweaks\n  labs(fill = \"\", y = \"Density\", x = \"Probability of getting heads\") +\n  scale_fill_manual(values = col_pal, aesthetics = \"fill\") +\n  scale_x_continuous(labels = scales::label_percent(), \n                     limits = c(0,1)) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 6.5)) +\n  see::theme_modern() +\n  theme(legend.position = \"top\",\n        legend.spacing.x = unit(3, \"mm\")) +\n  #### Arrows\n  geom_curve(aes(x = .81, y = 4.1, xend = .69232, yend = 3.425), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .9, y = 4.1, label = \"Beta(10,5)\")) +\n  geom_curve(aes(x = .2, y = 5.9, xend = .07693, yend = 5.45), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .29, y = 5.85, label = \"Beta(2,13)\")) +\n  geom_curve(aes(x = .5, y = 5, xend = .3847, yend = 4.4), curvature = .4,\n               arrow = arrow(length = unit(1/3, \"cm\"), angle = 20)) +\n  geom_text(aes(x = .55, y = 5, label = \"≈ 39%\"))\n\n\n\n\n\nGraphical representation of the posterior probability as the combination of both the data and the prior evidence"
  },
  {
    "objectID": "posts/2023-05-30 welcome/index.html#from-past-to-future",
    "href": "posts/2023-05-30 welcome/index.html#from-past-to-future",
    "title": "Welcome to Bayesically Speaking",
    "section": "From past to future",
    "text": "From past to future\nImagine a time not too long ago when Bayesian statistics were not as prevalent as they are today. The computational challenges posed significant hurdles, limiting our ability to fully embrace their potential. But thanks to the rapid advancement of computing power and simulation techniques, the statistical landscape has undergone a revolution. We now find ourselves in an exciting era where complex Bayesian analysis is accessible to all. It’s like having a superpower in the palm of our hands—an empowering time where our statistical prowess can thrive and conquer new frontiers.\nAs passionate self-learners on this thrilling statistical journey, even without a formal statistician’s hat, we can’t help but feel an overwhelming excitement to share the vast potential of these tools for unraveling real-world phenomena. Delving into the world of statistics, especially through the lens of Bayesian inference, opens up a universe of captivating possibilities. By melding prior knowledge with fresh evidence and embracing the enigmatic realm of uncertainty, we can uncover profound insights into health, well-being, and the wondrous phenomena that shape our lives.\nSo, fellow adventurers, let’s ignite our curiosity, embrace our thirst for knowledge, and embark on this exhilarating voyage together. With statistics as our compass, we will navigate the complexities of our reality, expanding our understanding and seizing the extraordinary opportunities that await us.\nGet ready to experience a world that’s more vivid, more nuanced, and more awe-inspiring than ever before. Together, let’s dive into the captivating realm of statistics, fueled by enthusiasm and a passion for discovery."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html",
    "href": "posts/2024-04-14 mcmc part 1/index.html",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "Alright, folks, let’s dive into the wild world of statistics and data science! Picture this: you’re knee-deep in data, trying to make sense of the chaos. But here’s the kicker, sometimes the chaos is just too darn complex. With tons of variables flying around, getting a grip on uncertainty can feel like trying to catch smoke with your bare hands.\nPlease, have in your consideration that the kind of problems that we’re dealing with, it’s not solely related to the number of dimensions, it’s mostly related to trying to estimate something that we can’t see in full beforehand. For instance, consider the following banana distribution (shown below). How could we map this simple two dimensional surface without computing it all at once?\n\n\nCode\ndbanana &lt;- function(x) {\n  a = 2;\n  b = 0.2;\n  \n  y = x / a\n  y = (a * b) * (x^2 + a^2)\n}\n\nx &lt;- seq(-6, 6, length.out = 300)\n\ny = dbanana(x)\n\nz &lt;- MASS::kde2d(x, y, n = 100, lims = c(-10, 10, -2.6, 20))\n\nplot_ly(x = z$x, y = z$y, z = sqrt(z$z)) |&gt; \n  add_surface() |&gt; \n  style(hoverinfo = \"none\")\n\n\n\n\n\n\n\n\nYou know when you hit a roadblock in your calculations, and you’re like, “Can’t we just crunch the numbers for every single value?” Well, let’s break it down. Picture a grid with \\(N\\) points for \\(D\\) dimensions. Now, brace yourself, ’cause the math needed is like \\(N\\) raised to the power of \\(D\\).\nSo, let’s say you wanna estimate 100 points (to get a decent estimation of the shape) for each of 100 dimensions. That’s like slamming your head against ten to the power of 200 computations… that’s a hell of a lot of computations!\nSure, in la-la land, you could approximate every single number with some degree of approximation. But let’s get real here, even if you had all the time in the world, you’d still be chipping away at those calculations until the sun swallowed the Earth, especially with continuous cases and tons of dimensions that are somewhat correlated (which in reality, tends to be the case).\nThis headache we’re dealing with? It’s what we “affectionately” call (emphasis on double quotes) the curse of dimensionality. It’s like trying to squeeze a square peg into a round hole… it ain’t gonna happen without a supersized hammer!\n\n\nCode\ncurse_dimensionality &lt;- data.frame(dimensions = factor((1:10)^2),\n                                   calculations = 100^((1:10)^2))\n\nggplot(curse_dimensionality, aes(dimensions, calculations)) +\n  geom_col(fill = ggsci::pal_jama()(1)) +\n  scale_y_continuous(transform = \"log10\", n.breaks = 9,\n                     labels = scales::label_log(), expand = c(0,0,.1,0)) +\n  labs(y = \"Computations (log-scale)\", x = \"Dimensions (Variables)\",\n       title = \"Computations needed to compute a grid of 100 points\",\n       subtitle = \"As a function of dimensions/variables involved\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nIllustration of computations needed (in log-scale) for 100 points as a function of dimensions considered.\n\n\n\n\n\n\n\n\n\n\nExplaining the curse of dimensionality further\n\n\n\n\n\nImagine you’re trying to create a grid to map out the probability space for a set of variables. As the number of dimensions increases, the number of grid points needed to adequately represent the space explodes exponentially. This means that even with the most powerful computers, it becomes practically impossible to compute all the probabilities accurately.\n\n\n\n\n\n\nNow, if we can’t crack the problem analytically (which, let’s face it, is the case most of the time), we gotta get creative. Lucky for us, there’s a bunch of algorithms that can lend a hand by sampling this high-dimensional parameter space. Enter the Markov Chain Monte Carlo (MCMC) family of algorithms.\nBut hold up… Markov Chain Monte What? Yeah, it’s a mouthful, but bear with me. You’re probably wondering how this fancy-schmancy term is connected to exploring high-dimensional probability spaces. Well, I’ll let you in on the secret sauce behind these concepts and why they’re the go-to tools in top-notch probabilistic software like Stan.\nBut before we get into the nitty-gritty of MCMC, let’s take a detour and talk about Markov Chains, because they’re like the OGs of this whole MCMC gang."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#just-put-a-darn-grid-to-it",
    "href": "posts/2024-04-14 mcmc part 1/index.html#just-put-a-darn-grid-to-it",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "You know when you hit a roadblock in your calculations, and you’re like, “Can’t we just crunch the numbers for every single value?” Well, let’s break it down. Picture a grid with \\(N\\) points for \\(D\\) dimensions. Now, brace yourself, ’cause the math needed is like \\(N\\) raised to the power of \\(D\\).\nSo, let’s say you wanna estimate 100 points (to get a decent estimation of the shape) for each of 100 dimensions. That’s like slamming your head against ten to the power of 200 computations… that’s a hell of a lot of computations!\nSure, in la-la land, you could approximate every single number with some degree of approximation. But let’s get real here, even if you had all the time in the world, you’d still be chipping away at those calculations until the sun swallowed the Earth, especially with continuous cases and tons of dimensions that are somewhat correlated (which in reality, tends to be the case).\nThis headache we’re dealing with? It’s what we “affectionately” call (emphasis on double quotes) the curse of dimensionality. It’s like trying to squeeze a square peg into a round hole… it ain’t gonna happen without a supersized hammer!\n\n\nCode\ncurse_dimensionality &lt;- data.frame(dimensions = factor((1:10)^2),\n                                   calculations = 100^((1:10)^2))\n\nggplot(curse_dimensionality, aes(dimensions, calculations)) +\n  geom_col(fill = ggsci::pal_jama()(1)) +\n  scale_y_continuous(transform = \"log10\", n.breaks = 9,\n                     labels = scales::label_log(), expand = c(0,0,.1,0)) +\n  labs(y = \"Computations (log-scale)\", x = \"Dimensions (Variables)\",\n       title = \"Computations needed to compute a grid of 100 points\",\n       subtitle = \"As a function of dimensions/variables involved\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nIllustration of computations needed (in log-scale) for 100 points as a function of dimensions considered.\n\n\n\n\n\n\n\n\n\n\nExplaining the curse of dimensionality further\n\n\n\n\n\nImagine you’re trying to create a grid to map out the probability space for a set of variables. As the number of dimensions increases, the number of grid points needed to adequately represent the space explodes exponentially. This means that even with the most powerful computers, it becomes practically impossible to compute all the probabilities accurately."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#sampling-the-unknown-markov-chain-monte-carlo",
    "href": "posts/2024-04-14 mcmc part 1/index.html#sampling-the-unknown-markov-chain-monte-carlo",
    "title": "Markov Chain Monte What?",
    "section": "",
    "text": "Now, if we can’t crack the problem analytically (which, let’s face it, is the case most of the time), we gotta get creative. Lucky for us, there’s a bunch of algorithms that can lend a hand by sampling this high-dimensional parameter space. Enter the Markov Chain Monte Carlo (MCMC) family of algorithms.\nBut hold up… Markov Chain Monte What? Yeah, it’s a mouthful, but bear with me. You’re probably wondering how this fancy-schmancy term is connected to exploring high-dimensional probability spaces. Well, I’ll let you in on the secret sauce behind these concepts and why they’re the go-to tools in top-notch probabilistic software like Stan.\nBut before we get into the nitty-gritty of MCMC, let’s take a detour and talk about Markov Chains, because they’re like the OGs of this whole MCMC gang."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#converging-to-an-answer",
    "href": "posts/2024-04-14 mcmc part 1/index.html#converging-to-an-answer",
    "title": "Markov Chain Monte What?",
    "section": "Converging to an answer",
    "text": "Converging to an answer\nNow, let’s imagine letting time run. After a year passes, if we observe how the weather behaves, we’ll notice that the relative frequencies of each state tend to converge to a single number.\nNow, fast forward a year. If we keep an eye on the weather every day, we’ll notice something interesting: the relative frequencies of rainy and sunny days start to settle into a rhythm. This steady state is what we call a stationary distribution. It’s like the true probability of what the weather’s gonna be like in the long run, taking into account all the different scenarios.\n\n\nCode\nsimulate_weather &lt;- function(total_time) {\n  \n  weather &lt;- vector(\"character\", total_time) # Create slots for each day\n  day &lt;- 1 # First day\n  weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1) # Weather for first day\n  \n  while (day &lt; total_time) {\n    day &lt;- day + 1 # Add one more day\n    if (weather[day] == \"Rainy\") {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.6, .4))\n    } else {\n      weather[day] &lt;- sample(c(\"Rainy\", \"Sunny\"), size = 1, prob = c(.3, .7))\n    }\n  }\n  \n  return(weather)\n}\n\nsim_time &lt;- 365*1\nweather &lt;- simulate_weather(total_time = sim_time)\n\nweather_data &lt;- data.frame(\n  prop = c(cumsum(weather == \"Rainy\") / seq_len(sim_time), cumsum(weather == \"Sunny\") / seq_len(sim_time)),\n  time = c(seq_len(sim_time), seq_len(sim_time)),\n  weather = c(rep(\"Rainy\", times = sim_time), rep(\"Sunny\", times = sim_time))\n)\n\nggplot(weather_data, aes(time, prop, fill = weather)) +\n  geom_area() +\n  scale_y_continuous(labels = scales::label_percent(), n.breaks = 6,\n                     name = \"Proportion of each weather\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Days\", n.breaks = 10, expand = c(0,0)) +\n  scale_fill_brewer(type = \"qual\", palette = 3) +\n  labs(fill = \"Weather\", title = \"Convergence to stationary distribution\",\n       subtitle = \"Based on cumulative proportion of each Sunny or Rainy days\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean proportion of sunny/rainy days across 365 days. Right pass the 100 days, the proportion of rainy/sunny days tends to display a stable trend when we averaged the previous days. This is known as stationary distribution.\n\n\n\n\nThis heuristic allows us to naturally converge to an answer without needing to solve it analytically, which tends to be useful for really complex and high-dimensional problems.\nSure, we could’ve crunched the numbers ourselves to figure out these probabilities. But why bother with all that math when we can let time do its thing and naturally converge to the same answer? Especially when we’re dealing with complex problems that could have given even Einstein himself a headache.\n\n\n\n\n\n\nExplaining the convergence process further\n\n\n\n\n\nThe idea of convergence to a stationary distribution can be likened to taking a random walk through the space of possible outcomes. Over time, the relative frequencies of each outcome stabilize, giving us a reliable estimate of the true probabilities.\n\n\n\nAs we’ve seen, sometimes it becomes impractical to solve analytically or even approximate the posterior distribution using a grid, given the number of calculations needed to even get a decent approximation of the posterior.\nHowever, we’ve also seen that Markov Chains might offer us a way to compute complex conditional probabilities and, if we let them run long enough, they will eventually converge to the stationary distribution, which could resemble the posterior distribution itself. So, all things considered, when does the Monte Carlo part come in?"
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#what-is-mcmc-actually-doing",
    "href": "posts/2024-04-14 mcmc part 1/index.html#what-is-mcmc-actually-doing",
    "title": "Markov Chain Monte What?",
    "section": "What is MCMC actually doing?",
    "text": "What is MCMC actually doing?\nIn essence, MCMC is an algorithm that generates random samples from a proposal distribution. These samples are accepted or rejected based on how much more likely the proposed sample is compared to the previous accepted sample.\nIn this way, the proposed samples are accepted in the same proportion as the actual probability in the target distribution, accepting more samples that are more likely and fewer samples that are less likely.\nThe fascinating nature of this heuristic is that it works to approximate complex distributions without needing to know much about the shape of the final distribution.\nSo, think of it as trekking through this complex landscape, taking random steps (the Monte Carlo part) but guided by the likelihood of each move, given where you currently stand (the Markov Chain part). It’s a meticulous journey, but one that ultimately leads us to a better understanding of these elusive distributions.\nFor instance, consider that we have a distribution (shown below) that we can’t to compute, because it would take too long to integrate the whole function. This will be our target distribution, from which we can only compute the density of one value at a time.\n\n\n\n\n\n\nAbout the target distribution\n\n\n\n\n\nIn practice, we would derive the target distribution from the data and prior information, this enable us to estimate the density in a point-wise manner, without the need to estimate the whole PDF all at once. But for the sake of demonstration we will the use the Gamma probability density function.\nHowever, please consider that you can’t use some family distribution to describe perfectly any probability density, sometimes it can be a mixture of distributions, truncation, censoring. All comes down to the underlying process that generates the data that we are trying to mimic.\n\n\n\n\n\nCode\n# Target distribution that we in practice would derive from\n# the data.\ntarget_dist &lt;- function(i) dgamma(i, shape = 2, scale = 1)\n\nggplot() +\n  stat_function(fun = target_dist,\n                xlim = c(0, 11), geom = \"area\", \n                fill = \"#374E55FF\") +\n  scale_y_continuous(breaks = NULL, name = \"Density\", expand = c(0,0)) +\n  scale_x_continuous(name = \"Some scale\", expand = c(0,0)) +\n  theme_classic(base_size = 20)\n\n\n\n\n\nThis is a Gamma distribution with shape of 2 and scale of 1. We will try to estimate it.\n\n\n\n\nNext thing to do is to specify a proposal distribution, from which we’ll generate proposals for the next step. To this end we’ll be using a Normal density function with \\(\\mu\\) = 0 and \\(\\sigma\\) = 1.\n\n# This is a function that will generate proposals for the next step.\nproprosal &lt;- function() rnorm(1, mean = 0, sd = 1)\n\nAnd set some algorithm parameters that are necessary for our MCMC to run:\n\n## Algorithm parameters ----\n\ntotal_steps &lt;- 1000 # Total number of steps\nstep &lt;- 1 # We start at step 1\nvalue &lt;- 10 # set a initial starting value\n\nFinally, we run our algorithm as explained in previous sections. Try to follow the code to get an intuition of what is doing.\n\n## Algorithm ----\n\nset.seed(1234) # Seed for reproducibility\nwhile(step &lt; total_steps) {\n  # Increase for next step\n  step &lt;- step + 1\n  \n  ## 1. Propose a new value ----\n  \n  # Proposal of the next step is ...\n  value[step] &lt;- \n    # the previous step plus...\n    value[step - 1L] + \n    # a change in a random direction (based on the \n    # proposal distribution)\n    proprosal() \n  \n  ## 2. We see if the new value is more or less likely ----\n  \n  # How likely (in the target distribution)\n  likelihood &lt;- \n    # is the proposed value compared to the previous step\n    target_dist(value[step]) / target_dist(value[step - 1L]) \n  \n  ## 3. Based on its likelihood, we accept or reject it ----\n  \n  # If the proposal value is less likely, we accept it only \n  # to the likelihood of the proposed value\n  if (likelihood &lt; runif(1)) \n    value[step] &lt;- value[step - 1L]\n  \n  # Then we repeat for the next step\n}\n\nFinally, let’s explore how well our algorithm converge to the target distribution.\n\n\nCode\nmcmc &lt;- data.frame(\n  step = seq_len(step),\n  value = value\n)\n\nggplot(mcmc, aes(x = step, y = value)) +\n  geom_line(col = \"#374E55FF\") +\n  ggside::geom_ysidehistogram(aes(x = -after_stat(count)), fill = \"#374E55FF\", binwidth = .3) +\n  ggside::geom_ysidedensity(aes(x = -after_stat(count)*.35), col = \"#374E55FF\") +\n  ggside::scale_ysidex_continuous(expand = c(0,0,0,.1), breaks = NULL) +\n  scale_x_continuous(expand = c(0,0), name = \"Step\") +\n  scale_y_continuous(name = NULL, position = \"right\") +\n  labs(title = \"Trace of MCMC values to target distribution\",\n       subtitle = \"Evolution of values at each step\") +\n  theme_classic(base_size = 20) +\n  ggside::ggside(y.pos = \"left\") +\n  theme(ggside.panel.scale = .4)\n\n\n\n\n\nTraceplot of convergence of MCMC for 1000 steps. With increasing steps we see an increasing resemblance to the target distribution.\n\n\n\n\nAnother thing that we care is to see how well our MCMC is performing. After all, if not, then what would be the point of using it in first place? To check this, we’ll compare the expectation (\\(E(X)\\)) of the target distribution against the posterior derived from our MCMC.\nFor this, we have to consider that the expectation, \\(E(X)\\), of any Gamma distribution is equal to the shape parameter (\\(\\alpha\\)) times by the scale parameter (\\(\\sigma\\)). We could express the aforementioned the following.\n\n\\(\\begin{aligned}\n  E(X) &= \\alpha \\sigma \\\\\n  \\text{with}~X &\\sim \\text{Gamma}(\\alpha, \\sigma)\n\\end{aligned}\\)\n\n\n\nCode\nggplot(mcmc, aes(x = step, y = cumsum(value)/step)) +\n  geom_line(col = \"#374E55FF\") +\n  scale_x_continuous(expand = c(0,.1), name = \"Steps (log-scale)\", \n                     transform = \"log10\", labels = scales::label_log()) +\n  scale_y_continuous(name = NULL, expand = c(0, 1)) +\n  labs(title = \"Convergence to location parameter\",\n       subtitle = \"Cumulative mean across steps\") +\n  geom_hline(aes(yintercept = 2), col = \"darkred\") +\n  geom_hline(aes(yintercept = mean(value)), lty = 2) +\n  annotate(x = 1.5, xend = 1.1, y = 7.5, yend = 9.5, geom = \"curve\", curvature = -.2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = 2, y = 6.8,  label = \"Initial value\", size = 5, geom = \"text\") +\n  annotate(x = (10^2.5), xend = (10^2.6), y = 5, yend = 2.5, geom = \"curve\", curvature = .2,\n           arrow = arrow(length = unit(.1, \"in\"), type = \"closed\")) +\n  annotate(x = (10^2.5), y = 5.8,  label = \"Convergence\", size = 5, geom = \"text\") +\n  theme_classic(base_size = 20)\n\n\n\n\n\nCumulative mean of the posterior distributions across steps, compared to the empirical mean of the target distribution. Here the dark red line represents the empirical location parameter and the dashed line the one estimated using MCMC."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#gibbs-sampling-a-buffet-adventure",
    "href": "posts/2024-04-14 mcmc part 1/index.html#gibbs-sampling-a-buffet-adventure",
    "title": "Markov Chain Monte What?",
    "section": "Gibbs Sampling: A Buffet Adventure",
    "text": "Gibbs Sampling: A Buffet Adventure\nImagine you’re at a buffet with stations offering various cuisines (Italian, Chinese, Mexican, you name it). You’re on a mission to create a plate with a bit of everything, but here’s the catch: you can only visit one station at a time. Here’s how you tackle it:\n\nHit up a station and randomly pick a dish.\nMove on to the next station and repeat the process.\nKeep going until you’ve got a plateful of diverse flavors.\n\nGibbs sampling works kind of like this buffet adventure. You take turns sampling from conditional distributions, just like you visit each station for a dish. Each time, you focus on one variable, updating its value while keeping the others constant. It’s like building your plate by sampling from each cuisine until you’ve got the perfect mix."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#hamiltonian-monte-carlo-charting-your-hiking-path",
    "href": "posts/2024-04-14 mcmc part 1/index.html#hamiltonian-monte-carlo-charting-your-hiking-path",
    "title": "Markov Chain Monte What?",
    "section": "Hamiltonian Monte Carlo: Charting Your Hiking Path",
    "text": "Hamiltonian Monte Carlo: Charting Your Hiking Path\nPicture yourself hiking up a rugged mountain with rocky trails and valleys. Your goal? Reach the summit without breaking a sweat (or falling off a cliff). So, you whip out your map and binoculars to plan your route:\n\nStudy the map to plot a path with minimal uphill battles and maximum flat stretches.\nUse the binoculars to scout ahead and avoid obstacles along the way.\nAdjust your route as you go, smoothly navigating the terrain like a seasoned pro.\n\nHamiltonian Monte Carlo (HMC) is a bit like this hiking adventure. It simulates a particle moving through a high-dimensional space, using gradient info to find the smoothest path. Instead of blindly wandering, HMC leverages the curvature of the target distribution to explore efficiently. It’s like hiking with a GPS that guides you around the rough spots and straight to the summit."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#strengths-weaknesses-and-real-world-applications",
    "href": "posts/2024-04-14 mcmc part 1/index.html#strengths-weaknesses-and-real-world-applications",
    "title": "Markov Chain Monte What?",
    "section": "Strengths, Weaknesses, and Real-World Applications",
    "text": "Strengths, Weaknesses, and Real-World Applications\nNow that you’ve dipped your toes into the MCMC pool, it’s time to talk turkey (well, sampling). Each MCMC method has its perks and quirks, and knowing them is half the battle.\nGibbs sampling is the laid-back surfer dude of the group, simple, chill, and great for models with structured dependencies. But throw in some highly correlated variables, and it starts to wobble like a rookie on a surfboard.\nMeanwhile, HMC is the sleek Ferrari, efficient, powerful, and perfect for tackling complex models head-on. Just don’t forget to fine-tune those parameters, or you might end up spinning out on a sharp curve."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#key-differences",
    "href": "posts/2024-04-14 mcmc part 1/index.html#key-differences",
    "title": "Markov Chain Monte What?",
    "section": "Key Differences",
    "text": "Key Differences\n\nSampling Approach\n\nMetropolis-Hastings: Takes random walks to generate samples, with acceptance based on a ratio of target distribution probabilities.\nGibbs Sampling: Updates variables one by one based on conditional distributions, like a tag team wrestling match.\nHamiltonian Monte Carlo: Glides through high-dimensional space using deterministic trajectories guided by Hamiltonian dynamics, like a graceful dancer in a crowded room.\n\n\n\nEfficiency and Exploration\n\nMetropolis-Hastings: Easy to implement but might struggle to explore efficiently, especially in high-dimensional spaces.\nGibbs Sampling: Perfect for structured models but may stumble with highly correlated variables.\nHamiltonian Monte Carlo: Efficiently navigates high-dimensional spaces, leading to faster convergence and smoother mixing.\n\n\n\nAcceptance Criterion\n\nMetropolis-Hastings: Decides whether to accept or reject proposals based on a ratio of target distribution probabilities.\nGibbs Sampling: Skips the acceptance drama and generates samples directly from conditional distributions.\nHamiltonian Monte Carlo: Judges proposals based on the joint energy of position and momentum variables, like a strict dance instructor.\n\n\n\nParameter Tuning and Complexity\n\nMetropolis-Hastings: Requires tweaking the proposal distribution but keeps it simple.\nGibbs Sampling: A breeze to implement, but watch out for those conditional distributions (they can be sneaky).\nHamiltonian Monte Carlo: Needs tuning of parameters like step size and trajectory length, and the implementation might get a bit hairy with momentum variables and gradient computation."
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#mcmc-in-action",
    "href": "posts/2024-04-14 mcmc part 1/index.html#mcmc-in-action",
    "title": "Markov Chain Monte What?",
    "section": "MCMC in action",
    "text": "MCMC in action\nIn the following, you can see an interactive animation of different MCMC algorithms (MH, Gibbs and HMC) and how they work to uncover distributions in two dimensions. The code for this animation is borrowed from Chi Feng’s github. You can find the original repository with corresponding code here: https://github.com/chi-feng/mcmc-demo"
  },
  {
    "objectID": "posts/2024-04-14 mcmc part 1/index.html#implementing-mcmc-algorithms-in-practice",
    "href": "posts/2024-04-14 mcmc part 1/index.html#implementing-mcmc-algorithms-in-practice",
    "title": "Markov Chain Monte What?",
    "section": "Implementing MCMC Algorithms in Practice",
    "text": "Implementing MCMC Algorithms in Practice\nAlright, theory’s cool and all, but let’s get down to brass tacks. When you’re rolling up your sleeves to implement MCMC algorithms, it’s like picking the right tool for the job. Simple models? Metropolis-Hastings or Gibbs sampling has your back. But when you’re wrangling with the big boys (those complex models) that’s when you call in Hamiltonian Monte Carlo. It’s like upgrading from a rusty old wrench to a shiny new power tool. And don’t forget about tuning those parameters, it’s like fine-tuning your car for a smooth ride.\nBeyond all the technical jargon, successful Bayesian inference is part gut feeling, part detective work. Picking the right priors is like seasoning a dish, you want just the right flavor without overpowering everything else. And tuning those parameters? It’s like fine-tuning your favorite instrument to make sure the music hits all the right notes."
  },
  {
    "objectID": "posts/2025-01-05 modeling-lymphocyte-dynamics/index.html",
    "href": "posts/2025-01-05 modeling-lymphocyte-dynamics/index.html",
    "title": "Modeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations",
    "section": "",
    "text": "This is a Draft"
  },
  {
    "objectID": "posts/2025-01-05 modeling-lymphocyte-dynamics/index.html#how-does-this-equation-capture-the-switch-like-behavior",
    "href": "posts/2025-01-05 modeling-lymphocyte-dynamics/index.html#how-does-this-equation-capture-the-switch-like-behavior",
    "title": "Modeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations",
    "section": "How Does this Equation Capture the Switch-Like Behavior?",
    "text": "How Does this Equation Capture the Switch-Like Behavior?\nOkay but, how does this equation capture the switch-like behavior? Let’s see how:\n\nAt very low antigen concentrations (\\(\\text{[Antigen]} &lt;&lt; K\\)), the term \\(\\text{[Antigen]}^n\\) is much smaller than \\(K^n\\), so the probability of activation is close to zero.\nAt very high antigen concentrations (\\(\\text{[Antigen]} &gt;&gt; K\\)), the term \\(\\text{[Antigen]}^n\\) dominates, and the probability of activation approaches 1.\n\nThe transition between these two extremes is sharper when \\(n\\) is larger. This steep transition is what we refer to as the “switch-like” behavior.\nIt’s crucial to understand that \\(n\\) is an effective or apparent cooperativity coefficient. It describes the overall cooperativity of the binding process, which can be influenced by multiple factors beyond just the number of binding sites. It can reflect complex downstream signaling events or allosteric effects.\nHere’s the R function:\n\nactivation_probability &lt;- function(antigen, K, n) {\n  (antigen^n) / (K^n + antigen^n)\n}\n\nThis R function is a direct translation of the Hill equation. It takes the antigen concentration (antigen), the dissociation constant (K), and the Hill coefficient (n) as inputs and returns the calculated activation probability. It’s our little digital “activation calculator.”\nAnd a quick visualization to show the effect of the Hill coefficient:\n\n\nCode\nantigen_conc &lt;- seq(0, 2, by = 0.001)\n\ndf &lt;- lapply(c(\"0.2\" = 0.2, \"0.5\" = 0.5, \n         \"1.0\" = 1.0, \"2.0\" = 2.0, \n         \"4.0\" = 4.0), function(x) {\n           data.table(\n             Pr = activation_probability(antigen_conc, K = 0.7, n = x),\n             Antigen = antigen_conc\n           )\n         }) |&gt; rbindlist(idcol = \"Hill coefficient\")\n\nggplot(df, aes(Antigen, Pr)) +\n  geom_line(aes(color = `Hill coefficient`), linewidth = 1) +\n  labs(x = \"Antigen Concentration\", y = \"Probability of Activation\", color = \"Hill Coefficient\", title = \"Effect of Hill Coefficient on Activation Probability\") +\n  scale_color_ordinal()\n\n\n\n\n\nHill coefficient (n) on T cell activation. When n = 1, the response is nearly linear. However, as n increases (n = 2, n = 4), the activation curve becomes increasingly steep, demonstrating enhanced cooperativity and a more pronounced threshold effect. This highlights how small changes in antigen concentration can trigger large changes in activation probability when cooperativity is high. It’s like comparing a gentle slope to a cliff, a small step can make a big difference when you’re near the edge of the cliff (high n).\n\n\n\n\n\nProliferation: The Population Explosion\nOnce a T cell is activated, it’s time for action (and lots of it). This action comes in the form of proliferation, where a single activated T cell divides and creates many identical copies of itself, building an army to fight the infection (Kumar, Abbas, and Aster 2012). If there were no limits, this growth would be exponential (like compound interest in a bank account), but with cells instead of money. Imagine starting with one cell, and it doubles every hour. After just a day, you’d have millions!\nBut, of course, in the real world (and even in our digital one), such unrestrained growth is impossible. There are limited resources: space, nutrients, signaling molecules, all the things a T cell needs to survive and multiply. Eventually, the growth must slow down and plateau. This is where the logistic growth equation comes in, providing a much more realistic model of population dynamics.\nLet’s start with the continuous form of the logistic growth equation:\n\\[\n\\frac{\\partial N}{\\partial t} = rN \\left(1- \\frac{N}{K}\\right)\n\\] This equation has previously been used to model T cell dynamics in both mouse and human models (Morris, Farber, and Yates 2019). Additionally, it’s commonly used to model cell growth in general in discrete-time approximations (Jin, McCue, and Simpson 2018).\nLet’s break down this equation and connect it to our intuitive understanding of population growth:\n\n\\(\\frac{\\partial N}{\\partial t}\\): This term represents the rate of change in the number of T cells (\\(N\\)) over time (\\(t\\)). It’s how quickly the population is growing or shrinking. Think of it as the “speedometer” of our population.\n\\(r\\): This is the intrinsic growth rate, sometimes called the per capita growth rate. It represents how quickly the population would grow if there were no limitations. It’s the “ideal” growth rate, like the speed limit on a highway.\n\\(N\\): This represents the current number of T cells. It’s the current “position” of our population.\n\\(K\\): This is the carrying capacity. It represents the maximum number of T cells that the environment can sustain. It’s the “maximum occupancy” of our battlefield, there’s only so much room for T cells to operate.\n\nNow, let’s see how this equation captures the idea of limited growth.\n\nAt low population densities (N &lt;&lt; K): When the number of T cells (\\(N\\)) is much smaller than the carrying capacity (\\(K\\)), the term \\((1 - \\frac{N}{K})\\) is close to 1. The equation then simplifies to \\(\\frac{\\partial N}{\\partial t} = rN\\), which is the equation for exponential growth. This makes sense: when there are plenty of resources and space, the population grows rapidly, like those initial rabbits in a wide open field.\nAt high population densities (N ≈ K): As the number of T cells (\\(N\\)) approaches the carrying capacity (\\(K\\)), the term \\((1 - \\frac{N}{K})\\) approaches 0. This causes the rate of change \\(\\frac{\\partial N}{\\partial t}\\) to also approach 0, meaning the population growth slows down and eventually stops. This also makes sense, given that as the battlefield becomes crowded, resources become scarce, and the T cell population can’t grow much more.\n\nSince our simulation is discrete-time (we update the population at specific intervals, not continuously), we use a discrete-time approximation of the logistic growth equation:\n\\[\nN(t+1) = N(t) + r * N(t) * \\left(1 - \\frac{N(t)}{K} \\right)\n\\]\nThis equation tells us how to calculate the number of T cells at the next time step (\\(N(t+1)\\)) based on the current number of T cells (\\(N(t)\\)), the growth rate (\\(r\\)), and the carrying capacity (\\(K\\)). It’s like taking snapshots of the population at regular intervals and updating the numbers based on the growth equation.\nHere’s the R function that implements this discrete-time approximation:\n\nproliferation &lt;- function(N, r, K) {\n  N + r * N * (1 - N / K)\n}\n\nThis R function takes N, r, and K as inputs and returns the updated number of T cells. It’s our digital “population updater,” simulating the growth and eventual stabilization of the T cell army.\nAnd here’s a visualization of the proliferation behavior of our function implementation:\n\n\nCode\n## Function to simulate cell proliferation\nproliferating_cells &lt;- function(n_steps, n_cells, r, k) {\n  cells &lt;- numeric(n_steps)\n  cells[1L] &lt;- n_cells\n  for (t in 2:n_steps) {\n    cells[t] &lt;- proliferation(N = cells[t-1], r = r, K = k)\n  }\n  data.table(Time = seq_len(n_steps), Cells = cells)\n}\n\n## Testing different Rate values\ndf &lt;- lapply(X = seq(0.1, 0.6, by = 0.1), function(x) {\n  proliferating_cells(100, 1, x, 200)\n}) |&gt; rbindlist(idcol = \"Rate\")\n\n## Pretty labels for plotting\ndf[, Rate := ordered(Rate, levels = 1:6, labels = seq(0.1, 0.6, by = 0.1))]\n\n## ggplot2 plot\nggplot(df, aes(Time, Cells, color = Rate)) +\n  geom_hline(yintercept = c(0, 200), color = \"gray50\", \n             linewidth = 1/2, linetype = 2) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Time Steps\",\n       y = \"Number of Cells\", \n       title = \"Simulation of Cell Proliferation over Time\")\n\n\n\n\n\nDynamics of T cell proliferation using the logistic growth model. Simulations with varying growth rates (r) show that faster proliferation leads to a more rapid initial increase in cell numbers. However, the carrying capacity (K = 200) acts as a limiting factor, causing all populations to eventually reach a plateau, highlighting the constraint of resources on cell growth. It’s like a race between different rabbit populations, the faster breeders get to the maximum population size quicker, but they all eventually hit the limit of their field.\n\n\n\n\n\n\nApoptosis (Constant Probability): The Programmed Exit\nEven the most valiant soldiers can’t fight forever. T cells, like all living cells, have a finite lifespan. They eventually undergo apoptosis, a process of programmed cell death. It’s like a built-in self-destruct mechanism that ensures cells don’t become a burden or pose a risk to the organism. In our model, we’ll represent apoptosis with a constant probability. This means that at each time step, every activated T cell has the same chance of undergoing apoptosis, regardless of how long it’s been activated or what it’s been doing.\nLet’s represent this with a simple equation:\n\\[\n\\Pr(\\text{apoptosis}) = p_{\\text{apoptosis}}\n\\]\nThis equation is pretty straightforward:\n\n\\(\\Pr(\\text{apoptosis})\\): This represents the probability that a single activated T cell will undergo apoptosis during a given time step. It’s a number between 0 (no chance of apoptosis) and 1 (certain apoptosis).\n\\(p_{\\text{apoptosis}}\\): This is the constant probability of apoptosis. It’s a parameter of our model that we can set to represent different rates of cell death.\n\nThis constant probability model assumes that apoptosis is a random event that occurs independently for each cell (which is a simplication of real-life biological systems). It’s like each cell flipping a weighted coin at every time step. If the coin lands on “heads” (with a probability of \\(p_{\\text{apoptosis}}\\)), the cell undergoes apoptosis.\nNow, let’s translate this into R code:\n\napoptosis &lt;- function(N, p_apoptosis) {\n  n_apoptosis &lt;- rbinom(1, N, p_apoptosis) # Number of cells undergoing apoptosis\n  N - n_apoptosis\n}\n\nThis function simulates apoptosis. It takes two arguments: N (the current number of T cells) and p_apoptosis (the probability of apoptosis). It uses rbinom(1, N, p_apoptosis) to generate a random number of cells that undergo apoptosis. rbinom() simulates a binomial distribution, which is perfect for modeling the number of “successes” (apoptosis events) in a given number of “trials” (cells). It’s like rolling a bunch of dice, where each die has a probability p_apoptosis of landing on “apoptosis”. The function then returns the remaining number of cells after apoptosis.\nAnd here’s a visualization simulating the effect of different p_apoptosis values on the total number of cells over time:\n\n\nCode\n## Function to simulate cell dead\napoptotic_cells &lt;- function(n_steps, n_cells, p) {\n  cells &lt;- numeric(n_steps)\n  cells[1L] &lt;- n_cells\n  for (t in 2:n_steps) {\n    cells[t] &lt;- apoptosis(N = cells[t-1], p_apoptosis = p)\n  }\n  data.table(Time = seq_len(n_steps), Cells = cells)\n}\n\n## Testing different Rate values\ndf &lt;- lapply(X = seq(0.1, 0.5, by = 0.1), function(x) {\n  apoptotic_cells(50, 1000, x)\n}) |&gt; rbindlist(idcol = \"Probability\")\n\n## Pretty labels for plotting\ndf[, Probability := ordered(Probability, levels = 1:5, labels = seq(0.1, 0.5, by = 0.1))]\n\n## ggplot2 plot\nggplot(df, aes(Time, Cells, color = Probability)) +\n  geom_hline(yintercept = c(0), color = \"gray50\", \n             linewidth = 1/2, linetype = 2) +\n  geom_line(linewidth = 1) +\n  labs(x = \"Time Steps\",\n       y = \"Number of Cells\", \n       title = \"Simulation of Cell Apoptosis over Time\",\n       color = expression(P[Apoptosis]))\n\n\n\n\n\nEffect of apoptosis probability on cell population size over time. Simulations show that increasing the probability of apoptosis (\\(p_{    ext{apoptosis}}\\)) results in a more rapid decrease in the number of cells, illustrating the direct relationship between apoptosis rate and population decline. It’s like watching a sandcastle being washed away by the tide, the stronger the tide (higher p_apoptosis), the faster the sandcastle disappears.\n\n\n\n\n\n\nSetting Up the Simulation\nBefore we launch our digital immune response, we need to set the stage. This means defining the initial conditions and parameters for our simulation. Think of it as setting up a virtual battlefield where our T cells will fight their digital battles.\n\n## T Cell population\nn_steps &lt;- 100         # Number of time steps\nn_naive &lt;- 100         # Initial number of naive T cells\n\n## T Cell activation function\nantigen_level &lt;- 0.2   # Antigen concentration\nK_activation &lt;- 0.5    # Dissociation constant for activation\nn_hill &lt;- 3            # Hill coefficient for activation\n\nHere’s what we’re setting up:\n\nn_steps: This is the number of time steps our simulation will run for. Think of it as the duration of the battle. We’ve chosen 100 time steps, but you can adjust this to simulate longer or shorter periods.\nn_naive: This is the initial number of naive T cells we start with. These are our fresh recruits, eager to join the fight. We’re starting with 100 naive T cells.\nantigen_level: This is the concentration of the antigen, the enemy our T cells are fighting against. It’s like the strength of the enemy army. We’ve set it to 0.2, but you can change this to simulate different infection scenarios.\nK_activation: This is the dissociation constant for activation, a parameter of the Hill function. Remember, this determines how strongly the antigen binds to the T cell receptor.\nn_hill: This is the Hill coefficient, which controls the steepness of the activation curve. A higher n_hill means a more switch-like response.\n\nThese parameters are like the initial settings of our simulation, determining the starting conditions of our virtual immune response.\n\n\nSimulating Activation and Early Response: Lighting the Fire\nNow that we have our equations and R functions ready, let’s simulate the initial activation of naive T cells in response to antigen. This section will focus on the early stages of the immune response, where naive T cells are first encountering the antigen and transitioning into their activated state. It’s like the first spark that ignites the immune fire, turning our rookie T cells into seasoned warriors.\nTo simulate the activation and early response of T cells, we will implement the following function:\n\n## Simulation function\nsim_t_activation &lt;- function(n_steps, n_naive, antigen_level, K_activation, n_hill) {\n  # Initialize data structures\n  naive_t_cells &lt;- numeric(n_steps)\n  activated_t_cells &lt;- numeric(n_steps)\n  \n  # Initial number of naive T cells\n  naive_t_cells[1L] &lt;- n_naive\n  \n  ## Probability of activation\n  prob &lt;- activation_probability(antigen_level, K_activation, n_hill)\n  \n  # Loop of activation cycle\n  for (t in 2:n_steps) {\n    # Probabilistic Activation of T cells\n    n_activated &lt;- 0\n    n_activated &lt;- sum(runif(n = naive_t_cells[t-1]) &lt; prob)\n    \n    ## Update population of T cells\n    naive_t_cells[t] &lt;- naive_t_cells[t-1] - n_activated\n    activated_t_cells[t] &lt;- activated_t_cells[t-1] + n_activated\n  }\n  \n  # Preparing the data from the simulation\n  df &lt;- data.table(Time = seq_len(n_steps), \n                   Naive = naive_t_cells, \n                   Activated = activated_t_cells)\n  \n  # In long format\n  melt(df, id.vars = \"Time\")\n}\n\nLet’s break down what’s happening inside this function:\n\nInitializing Data Structures: naive_t_cells &lt;- numeric(n_steps) and activated_t_cells &lt;- numeric(n_steps) create empty vectors to store the number of naive and activated T cells at each time step. It’s like setting up empty scoreboards to track the progress of the battle.\nSetting Initial Conditions: naive_t_cells[1L] &lt;- n_naive sets the initial number of naive T cells at the beginning of the simulation (time step 1). This is where we deploy our initial batch of rookie T cells onto the battlefield.\nCalculating Activation Probability: prob &lt;- activation_probability(antigen_level, K_activation, n_hill) calculates the probability of activation using our previously defined Hill function. This is like assessing the likelihood of our T cells encountering and recognizing the enemy.\nThe Activation Loop (The Battle Begins!): The for (t in 2:n_steps) loop simulates the passage of time, with each iteration representing a time step. Inside this loop, we have:\n\nProbabilistic Activation: n_activated &lt;- sum(runif(n = naive_t_cells[t-1]) &lt; prob) is where the magic of stochastic activation happens. For each naive T cell at the previous time step, we generate a random number between 0 and 1 using runif(). If this random number is less than the calculated activation probability (prob), the T cell is considered activated. It’s like each T cell flipping a weighted coin – the higher the probability, the more likely they are to “win” and become activated. The sum() function counts how many T cells were activated in this time step.\nUpdating Cell Populations: naive_t_cells[t] &lt;- naive_t_cells[t-1] - n_activated and activated_t_cells[t] &lt;- activated_t_cells[t-1] + n_activated update the number of naive and activated T cells. The number of naive T cells decreases as they become activated, and the number of activated T cells increases accordingly. It’s like tracking the casualties and new recruits on both sides of the battle.\n\nPreparing Data for Plotting: The code then prepares the simulation results into a data frame suitable for plotting using data.table and melt.\n\nNow, let’s visualize the results:\n\n\nCode\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill)\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Early T Cell Activation Response\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nEarly stages of T cell activation in response to antigen. The simulation demonstrates a decline in the naive T cell population coupled with a corresponding rise in the activated T cell population, reflecting the transition from naive to activated state. It’s like turning up the heat on the battlefield, the stronger the enemy (higher antigen level), the quicker our T cells get mobilized.\n\n\n\n\n\n\nSimulating Proliferation and Population Dynamics: The Army Grows\nNow that we have some activated T cells itching for action, let’s simulate their proliferation, the process where they multiply like, well, like very enthusiastic cells. We’ll extend our simulation and incorporate the logistic growth equation to keep things realistic (no infinite armies here!).\nFirst, let’s define some parameters that control the proliferation dynamics:\n\n## T Cell Proliferation and Apoptosis\nr_proliferation &lt;- 0.1 # Proliferation rate\nK_carrying &lt;- 150      # Carrying capacity\n\n\nr_proliferation: This is the proliferation rate, which determines how quickly the activated T cell population grows. It’s like the “birth rate” of our T cell army. A higher r_proliferation means faster growth.\nK_carrying: This is the carrying capacity, which represents the maximum number of activated T cells that the environment can support. It’s like the size of the battlefield, there’s only so much space and resources available.\n\nLet’s visualize the results:\n\n\nCode\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill, r_proliferation, K_carrying)\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"T Cell Activation and Proliferation\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nCombined processes of T cell activation and proliferation. Following initial activation driven by antigen (as seen by the decrease in naive T cells and increase in activated T cells), the activated population undergoes logistic growth. This is characterized by an initial phase of rapid, near-exponential expansion, which then slows down as the population approaches the carrying capacity (K = 150), demonstrating resource limitation. It’s like a population of rabbits in a field, they multiply quickly at first, but eventually, they run out of space and resources, and the population stabilizes.\n\n\n\n\n\n\nCombining Processes: A Full Simulation\nNow, the moment we’ve all been waiting for! Let’s combine all the processes we’ve discussed (activation, proliferation, and apoptosis) into a single, more complete simulation. This will give us a more realistic picture of how T cell populations behave during an immune response.\nWe’ll also make a couple of tweaks: we’ll set the apoptosis probability to 10% at each time step and increase our initial number of naive T cells to 300 and a carrying capacity of 500, because a bigger army is always more fun to simulate.\nAnd the final visualization:\n\n\nCode\n## Parameters\np_apoptosis = 0.1 # Probability of apoptosis\nn_naive = 300 # Number of initial naive T cells\nK_carrying = 500 # Carrying capacity\nr_proliferation = 0.5 # Proliferation rate\n\n# Simulation loop with activation, proliferation, and apoptosis\ndf &lt;- sim_t_activation(n_steps, n_naive, antigen_level, K_activation, n_hill, r_proliferation, K_carrying, p_apoptosis)\n\n# Plot the full simulation\nggplot(df, aes(x = Time, y = value, color = variable)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Full T Cell Dynamics: Activation, Proliferation, and Apoptosis\", x = \"Time Steps\", y = \"Number of Cells\", color = \"Cell Type\")\n\n\n\n\n\nIntegration of all key T cell processes: activation, proliferation, and apoptosis. Following antigen-driven activation, activated T cells proliferate, reaching a peak population size. Subsequently, apoptosis leads to a decline in the activated T cell population, demonstrating the dynamic balance between expansion and contraction of the immune response. It’s like watching a battle unfold, there’s an initial surge of troops, but eventually, attrition takes its toll, and the battle either reaches a stalemate or one side prevails.\n\n\n\n\n\n\nWhat Happens if We Tweak the Knobs?\nA crucial step in any modeling study is sensitivity analysis. It’s like playing with the knobs and dials of your model to see how it responds. We want to understand how changes in our input parameters (like the Hill coefficient or the carrying capacity) affect the outputs of our simulation (like the number of activated T cells). This helps us identify which parameters have the biggest influence on the model’s behavior, which knobs are the most important to turn if we want to change the outcome.\nFor instance, we can show a quick example of sensitivity analysis by varying the Hill coefficient (\\(n\\)), which, as we know, controls how switch-like the activation is:\n\n\nCode\n# Sensitivity analysis for Hill coefficient\nn_hill &lt;- c(\"n = 1\" = 1, \"n = 2\" = 2, \"n = 3\" = 3, \"n = 4\" = 4)\ndf &lt;- lapply(n_hill, function(x) {\n  sim_t_activation(n_steps = 100, n_naive = 300, \n                   antigen_level = 10, K_activation = 25, n_hill = x, \n                   r_proliferation = 0.5, K_carrying = 500, p_apoptosis = 0.10)\n}) |&gt; rbindlist(idcol = \"n_hill\")\n\nggplot(df, aes(x = Time, y = value, color = variable)) +\n    geom_line(aes(alpha = ordered(n_hill)), linewidth = 1) +\n    labs(title = \"Sensitivity Analysis for Hill Coefficient\", \n         x = \"Time Steps\", \n         y = \"Number of Activated Cells\", \n         color = \"Carrying Capacity\", \n         alpha = \"Hill Coefficient\",\n         caption = \"With [Antigen] = 10, and K = 25\") +\n  theme(plot.caption = element_text(size = 12))\n\n\n\n\n\nImpact of the Hill coefficient on the dynamics of T cell activation and recruitment. By varying n from 1 to 4, we observe a transition from a gradual, almost linear response (n=1) to a highly switch-like response (n=4).\n\n\n\n\nHere’s a closer look at what’s going on behind the scenes of this code.\nThe resulting plot is where we see the fruits of our labor. It shows how the dynamics of the activated T cell population and the recruitment of naive T cells change depending on the Hill coefficient. Tipically, higher values of \\(n\\) lead to a more rapid and pronounced increase in activated T cells. However, this is only true when \\(\\text{[Antigen]}\\) are above the \\(K\\) threshold.\nIn this case this relationship is inverted (recall the plot of the hill function we saw earlier), which might seem counterintuitive at first, but remember that the Hill coefficient affects the steepness of the activation curve. A higher Hill coefficient is like a very sharp switch, a small increase in antigen concentration leads to a big jump in activation after the \\(K\\) threshold is surpassed. Conversely, a lower Hill coefficient is like a more gradual dimmer switch.\nThis sensitivity analysis is incredibly valuable because it helps us understand just how important are the parameters in the equation controlling how T cells get activated. It shows that this parameter can significantly impact the overall immune response. By playing with these parameters, we can get a much better feel for how the immune system works and how it might react to different challenges, it’s like fine-tuning an instrument to get the perfect sound."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Bayesically Speaking\n\n\nPriors & Coffee\n\n\n\n\n\n\n\nWhat is this?\nIf you are curious about how to use data and probability to understand and visualize real world problems, you have come to the right place.\n About  Related work\n\n\n\n\n\n\nLatest posts\n See all posts\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nModeling Lymphocyte Dynamics: Agent-Based Simulations and Non-Linear Equations\n\n\n40 min\n\n\nIn this post, we’ll take you step by step through building and playing with a model of lymphocyte dynamics. So, grab your keyboard, and let’s whip up a recipe for…\n\n\n\nDec 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: A Case for Synaptic Plasticity\n\n\n35 min\n\n\nIn this second edition we’ll be discussing the role of long term potentiation and depression, a form of neuroplasticity, and their dynamics using non-linear models. Here…\n\n\n\nDec 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-linear models: Pharmacokinetics and Indomethacin\n\n\n15 min\n\n\nHere we dive in the process of making a non-linear model to predict the decay of plasma levels of an anti-inflammatory drug, and compare frequentist and bayesian methods.\n\n\n\nAug 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Good, The Bad, and Hamiltonian Monte Carlo\n\n\n30 min\n\n\nFollowing the footsteps of the previous post, here we delve ourselves into the mud of hamiltonian mechanics and how its dynamics can help us to explore parameter space more…\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chain Monte What?\n\n\n23 min\n\n\nIn this post we will delve into the main idea behind Markov Chain Monte Carlo (MCMC for short) and why it is useful within the bayesian inference framework.\n\n\n\nApr 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Bayesically Speaking\n\n\n12 min\n\n\nHi everyone! This is the first post of Bayesically Speaking, so get your seatbelt on and get ready to join me on this ride!\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\nNeed asistance with your analyses?\nWhether you need assistance performing complex statistical analysis for your project or simply need help wrapping your head around some statistics concepts, our team is ready to assist you.\n Check our services  Contact us"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Portfolio",
    "section": "",
    "text": "Portfolio"
  },
  {
    "objectID": "cv.html#footnotes",
    "href": "cv.html#footnotes",
    "title": "Portfolio",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nChilean Austral Molecular Integrative Neurophysiology by its acronym in Spanish.↩︎"
  }
]