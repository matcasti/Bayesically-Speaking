---
title: "HMC by Hand: Gradients, Heartbeats, PDEs and Other Tales"
description: |
  We already saw in previous posts how to set up your own Hamiltonian Monte Carlo (HMC) sampler to explore the parameter space. However, there was something we didn't cover in that or other posts: how to set up the gradient function that will guide the sampler through the parameter space. So buckle up! We'll dive right into the nitty gritty of math, gradients and partial derivatives by hand!
date: "2025-04-19"
doi: "10.59350/450sp-tdq85"
execute: 
  warning: false
  error: false
categories: [by-hand, mcmc, algorithms, non-linear, educational]
bibliography: ref.bib
title-block-banner: title-block.png
image: body_1.png
editor_options: 
  chunk_output_type: console
code-fold: show
#code-links:
#  - text: test
#    href: url
---

```{r}
#| include: false
library(ggplot2)
library(data.table)

theme_set(new = theme_classic(base_size = 20))
```

# Introduction

When we exercise, our hearts accelerate in response to increased metabolic demand. This acceleration is reflected in the shortening of the interval between successive heartbeats, what we call the **R-R interval (RRi)**. If you've ever looked at an electrocardiogram (ECG or EKG) trace, those prominent vertical spikes are the **R-waves** of successive cardiac cycles. The time between two R-waves gives us the RRi, typically measured in milliseconds. It's a simple yet powerful indicator of instantaneous heart rate and its underlying autonomic control.

So, let's connect the dots: during exercise, sympathetic activation dominates, and the RRi shortens (faster heart rate); during rest or recovery, parasympathetic activity increases, and the RRi lengthens (slower heart rate). This interplay between autonomic branches gives rise to dynamic RRi fluctuations that reflect both physiological status and adaptability.

### The RRi Dance: From Rest to Recovery

:::{style="text-align: center;"}
```{mermaid}
flowchart TD
  a((Rest)) --->| High RRi | b((Exercise))
  b --->| RRi decreases | c((Recovery))
  c -->| RRi increases | a
```
:::

These temporal changes in RRi follow a pattern, initial rapid decay during exercise, followed by a gradual recovery phase after cessation of activity. But describing this transition as merely "fast-to-slow" is a massive oversimplification. The RRi curve encapsulates layers of physiological regulation: baroreflex sensitivity, vagal rebound, sympathetic withdrawal, and more. To mathematically characterize this curve and extract its governing features, we need a model that reflects these complexities.

Enter the [Castillo-Aguilar et al. (2025) nonlinear RRi-vs-Time model](https://doi.org/10.1038/s41598-025-93654-6). This model captures the composite shape of RRi behavior across exercise and recovery, incorporating asymmetrical decay and recovery kinetics, plateau durations, and inter-individual variability in curve smoothness.

![](illustrative_curve.svg){width=100%}
[Example exercise-induced RRi-vs-time dynamics and key parameters governing the overall curve behavior.]{.aside}

### Why Modeling RRi Dynamics Matters

Cardiovascular regulation is nonlinear, adaptive, and context-sensitive. A descriptive model of RRi behavior allows researchers and clinicians to extract meaningful parameters from empirical data, like the time constant of vagal reactivation, the latency of sympathetic offset, or the asymptotic RRi level post-exercise. These parameters can serve as biomarkers of autonomic flexibility, cardiorespiratory fitness, or frailty.

However, **modeling is only half the battle**. Fitting a nonlinear model to real-world RRi data requires solving an inverse problem: given the observed curve, what parameter values best explain it? This isn't trivial. The RRi response to exercise is idiosyncratic, highly subject-specific, noisy, and often non-stationary. The same model can fit different people with vastly different parameter combinations, and slight data perturbations can lead to widely different fits.

That's where uncertainty quantification becomes essential.

### A Matter of Guessing (But Doing It Right)

You can think of modeling RRi dynamics as tuning knobs on a synthesizer: decay rates, asymptotes, inflection points, all influence the output sound (or in our case, the curve). You might eventually find a knob setting that matches the observed data, but how confident are you in those settings? Could the same sound be achieved with slightly different settings? How sensitive is the system to each knob?

Enter **Bayesian parameter estimation**, which doesn't just find one "best" configuration but explores the entire landscape of plausible settings given the data. In Bayesian terms, each configuration corresponds to a point in a high-dimensional parameter space, and our goal is to characterize the **posterior distribution** over this space.

In this landscape, some regions are more probable than others. Estimating these probabilities gives us both central estimates and associated uncertainties, credibility intervals, parameter correlations, multimodal structures, etc. But exploring this high-dimensional landscape isn't easy. Classical Markov Chain Monte Carlo (MCMC) methods like Metropolis-Hastings can take forever to converge in nonlinear models, especially when dimensions rise or parameter correlations are strong.

This is where **Hamiltonian Monte Carlo (HMC)** shines.

### Hamiltonian Monte Carlo: Beyond Random Walks

HMC is a gradient-informed sampling technique that dramatically improves efficiency in exploring posterior distributions. Instead of taking blind steps through the parameter space (as in random-walk MCMC), HMC leverages **gradient information**, i.e., the slope of the log-posterior function, to guide its trajectory.

Let's unpack the intuition. Imagine you're navigating a hilly landscape in the dark. Metropolis-Hastings takes random steps in various directions, accepting or rejecting them based on elevation (posterior value). This strategy works, but it's inefficient. Now imagine you can feel the slope of the ground under your feet, where it's rising, where it's falling. That's HMC. It uses the local gradient to simulate the motion of a particle gliding across the surface, accelerating downhill and coasting through flat regions.

In this analogy, the **position** of the particle corresponds to the current model parameters ($q$), while the **momentum** ($p$) is an auxiliary variable introduced to simulate inertia. This combination leads us to a physics-inspired representation of probability distributions through **Hamiltonian mechanics**.

HMC samples are generated by simulating the dynamics of this particle across the landscape defined by the **Hamiltonian**, the sum of potential and kinetic energies.

### From Posterior to Physics: Hamilton's Equations

The total energy of the system is described by the **Hamiltonian**:

$$
H(q, p) = U(q) + K(p)
$$

- $U(q)$ is the potential energy, which in HMC is the **negative log posterior** (i.e., higher probability = lower potential).
- $K(p)$ is the kinetic energy, typically modeled as a quadratic function of momentum (e.g., $K(p) = \frac{1}{2}p^T M^{-1} p$ where $M$ is a mass matrix).

The time evolution of the system is then governed by **Hamilton's equations**:

$$
\frac{dq}{dt} = \frac{\partial H}{\partial p}, \quad \frac{dp}{dt} = -\frac{\partial H}{\partial q}
$$

These differential equations simulate a reversible, volume-preserving dynamic system where the particle moves through parameter space while conserving energy. Unlike Metropolis-Hastings, which tends to get stuck in narrow valleys or slowly crawl across ridges, HMC can make long, informed jumps without reducing acceptance rates, thanks to its physical metaphor.

This elegant trick depends crucially on having access to the **gradient of the potential energy**, in Bayesian terms, the gradient of the negative log-likelihood (plus log-prior). That is:

$$
\nabla U(q) = - \nabla \log P(q \mid \text{data}) = - \left( \nabla \log L(q) + \nabla \log \text{prior}(q) \right)
$$

And here's the catch: **you need the gradient**. You need to know how each model parameter affects the log-likelihood in infinitesimal ways. This is where things get technical, messy, and, frankly, fun.

However, here we're going to assume flat priors, so the contribution of the gradient of the prior ($\nabla \log \text{prior}(q)$) to the overall gradient of the potential energy $\nabla U(q)$ is non-existing. This happens because the gradient of a flat function is always zero (i.e., there is no slope), so get:

$$
\nabla U(q) = - \left( \nabla \log L(q) + 0 \right)
$$

So we end up with: 

$$
\nabla U(q) = - \nabla \log L(q)
$$

So for pedagogical reasons, we'll assume $\nabla U(q) = -\nabla \log L (q)$. 

### The Gradient Grind: What Powers HMC

Most modern probabilistic programming languages, **Stan**, **PyMC**, **Turing.jl**, handle gradient computation automatically using automatic differentiation. This is amazing for practical work. However, manually deriving gradients gives you unmatched insight into how your model works under the hood. You'll see which parameters dominate the behavior of the likelihood, where the curvature of the surface is steep (informative regions), and where it's flat (uninformative dimensions prone to identifiability issues).

In our case, the RRi-vs-Time model is composed of sigmoidal and exponential components. Each parametercontributes nonlinearly to the shape of the curve. The likelihood is built from comparing observed RRi values to predicted ones via a probabilistic error model (usually Gaussian). Deriving the gradient means computing partial derivatives of the likelihood function with respect to each of these nonlinear parameters.

That means: chain rules, product rules, careful bookkeeping, and a healthy dose of mathematical patience.

But it's worth it.

### Educational Payoff

Understanding how the gradient of the likelihood behaves has major benefits:

- **Sensitivity analysis**: You'll see which parameters are well-constrained by the data.
- **Diagnosing identifiability**: Flat gradients often signal redundancies or parameter entanglement.
- **Hyperparameter tuning**: You'll learn how leapfrog steps, mass matrices, and warmup affect sampling.
- **Better priors**: You'll grasp which priors meaningfully affect the posterior and which are washed out by the likelihood.
- **Model improvement**: You'll see where your model structure might be too rigid or too flexible.

And finally, it demystifies what HMC samplers are doing. Instead of treating your inference engine as a black box, you now get to pop the hood and admire the machinery.

### A Journey Ahead

In a [previous post](https://doi.org/10.59350/qfn6w-qyt38), we showed how to implement the RRi-vs-Time model in R using HMC. We defined priors, specified the likelihood, and let the sampler do its job. But we didn't dwell on how the gradient was computed, nor on the intricacies of the target function with respect to each parameter.

That's what this post is about.

We will **derive the full gradient** of the RRi-vs-Time likelihood function with respect to its parameters, manually. We will examine each component, apply calculus step-by-step, and connect the math back to the model's physiological interpretation. It will be dense. But it will also be enlightening. This isn't just a math exercise, it's a way to sharpen your modeling intuition, improve your Bayesian inference skills, and gain deeper insight into cardiovascular dynamics.

So buckle up. The next sections will be filled with partial derivatives, log-likelihood terms, chain rules, and Hessian hints. But we'll walk through it all together, one gradient at a time.

Because understanding how a particle moves through probability space begins with understanding how each tiny slope pulls it forward.

# Data Generation Process

Let's assume that the observed RRi signal ($y_i$) comes from a Gaussian distribution (as the data generation process), centered around the true signal $M$ with variance $\sigma^2$. This can be denoted as:

$$
y_i \sim \mathcal{N}(M, \sigma^2)
$$

However, we assume that this true signal $M$ comes from our model $f(t \mid \theta_k)$.

So let $M: f(t \mid \theta_k)$, with $k \in \{\alpha, \beta, c, \lambda, \phi, \tau, \delta\}$, the parameters that control the output of our true signal $M$. Where:

$$
f(t \mid \theta_k) = \alpha + \frac{\beta}{1 + e^{\lambda (t - \tau)}}  - \frac{c\beta}{1 + e^{\phi (t - \tau - \delta)}}
$$

However, we need to estimate the parameters of the data generation process ($\theta_k$), in order to be able to reproduce the observed data. 

# Arriving to the Log Likelihood 

Remember that we assumed that the data generation process came from a Gaussian distribution. Thus, lets consider that the Gaussian probability function has this form:

$$
\mathcal{N}_\text{pdf}(y \mid M, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(y - M)^2}{2\sigma^2}}
$$

So, the likelihood for a set of data points would have the following form:

$$
L(y_i \mid M, \sigma^2) = \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(y_i - M)^2}{2\sigma^2}}}
$$

But for computational efficiency and stability, we work with logarithms (given the very small probabilities we are computing). Hence, the logarithm of our likelihood would look like the following:

$$
\log L(y_i \mid M, \sigma^2) = \sum_{i = 1}^{N}{\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \log\left(e^{-\frac{(y_i - M)^2}{2\sigma^2}}\right)}
$$

where we can simplify the first term by the quotient rule and the second one by canceling the euler (given that $\log(e^x) = x$) term:

$$
\log L(y_i \mid M, \sigma^2) = \sum_{i = 1}^{N}{\log(1) - \log(\sqrt{2\pi\sigma^2}) - \frac{(y_i - M)^2}{2\sigma^2}}
$$

which can be further simplified by solving $\log(1) = 0$ and applying the power rule to the square root inside the log using $\log((2\pi\sigma^2)^\frac{1}{2})$ which comes out of the log term:

$$
\log L(y_i \mid M, \sigma^2) = \sum_{i = 1}^{N}{-\frac{1}{2} \log(2\pi\sigma^2) - \frac{(y_i - M)^2}{2\sigma^2}}
$$

Now we can solve the summation symbolically for the first term and leave it only on the variable term, leaving with our final log likelihood gaussian function:

$$
\log L(y_i \mid M, \sigma^2) = -\frac{N}{2} \log(2\pi\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}}
$$

And like this, we reach the final form for our likelihood function

# Estimating the Gradient of the Function

### With Respect to $M$

To estimate the gradient of our log likelihood function, we need to derive it with respect to each parameter term. Let's start by solving for $M$:

$$
\frac{\partial \log L}{\partial M} \left( -\frac{N}{2} \log(2\pi\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

Here, we see that the first term does not contain $M$, so we can drop it from our calculations, leaving this expression:

$$
\frac{\partial \log L}{\partial M} \left( - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

Here, we can remove out of the denominator the term $2\sigma^2$ like this:

$$
\frac{\partial \log L}{\partial M} \left( - \frac{1}{2\sigma^2} \sum_{i = 1}^{N}{ (y_i - M)^2} \right)
$$

And now we apply the chain rule, where we derive the exponent and then the $M$ term. So it would look like this:

$$
\frac{\partial \log L}{\partial M} = - \frac{1}{2\sigma^2} \sum_{i = 1}^{N}{ (2)(y_i - M)(-\frac{\partial M}{\partial \theta_k})}
$$

Remember that $M$ its a function with multiple parameters ($k$ parameters), so "deriving $M$" actually means taking the partial derivative of the function with respect to each parameter $\theta_k$, also preserving the sign inside the parenthesis.

So by passing the 2 out the summation, we can cancel the 2 in the outside denominator by $\frac{2}{2\sigma^2} = \frac{1}{\sigma^2}$ and passing the negative sign in the partial derivative we can cancel the negative sign at the beginning of the function. So the whole expression would end up like this:

$$
\frac{\partial \log L}{\partial M} = \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \theta_k}}
$$

### With Respect to $\sigma$

Now to solve for $\frac{\partial \log L}{\partial \sigma^2}$ let's remember where were we with the derived log likelihood:

$$
\frac{\partial \log L}{\partial \sigma} \left( -\frac{N}{2} \log(2\pi\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

Here, let's start with the first term, and remember that the $-\frac{N}{2}$ is multiplying the log term. We need to have this present given that we want to pull the $\sigma^2$ out of the log term, but in order to do so, we need to also distribute the $-\frac{N}{2}$ term with it, leaving it this way:

$$
\frac{\partial \log L}{\partial \sigma} \left( -\frac{N}{2} \log(2\pi) -\frac{N}{2} \log(\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

And now, we can discard the first term given it doesn't contain our target parameter $\sigma^2$, leaving us with:

$$
\frac{\partial \log L}{\partial \sigma} \left( -\frac{N}{2} \log(\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

And by applying the power rule, we can pull out of the log term the square term out of the log, leaving the term $-\frac{2N}{2}$ canceling out the 2 like this:

$$
\frac{\partial \log L}{\partial \sigma} \left( -N \log(\sigma) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}} \right)
$$

Additionally and for convenience, let's pull the $2\sigma^2$ out of the denominator and let's call $\sum_{i = 1}^{N} (y_i - M)^2 = S$ for ease of notation. Additionally, let's express the $sigma^2$ with negative exponent for clarity when we derive:

$$
\frac{\partial \log L}{\partial \sigma} \left( -N \log(\sigma) - \frac{\sigma^{-2}}{2} S \right)
$$

And finally, let's derive this bad boy. For the first term, the derivative of any log is the inverse of the term, so our $\log(\sigma) \rightarrow \frac{1}{\sigma}$. And in our second term, we cancel out the 2 by pulling out the -2 from the exponent, leaving $\sigma^{-2}$ as $\sigma^{-3}$. This whole show will look like this:

$$
\frac{\partial \log L}{\partial \sigma} = -\frac{N}{\sigma} + \frac{1}{\sigma^3} S
$$

Which can be expressed as:

$$
\frac{\partial \log L}{\partial \sigma} = -\frac{N}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{N}{(y_i - M)^2}
$$

And this conclude our gradient estimation section for our log likelihood function. These functions can be further used to estimate the gradient and update the parameters when using gradient-based methods to explore the parameter space, like Hamiltonian Monte Carlo methods.

Up next, we'll calculate analytically the partial derivatives of the Castillo-Aguilar RRi-vs-time model. 

# Estimation of RRi-vs-Time Gradient

Let's recall our original target function:

$$
f(t \mid \theta_k) = \alpha + \frac{\beta}{1 + e^{\lambda (t - \tau)}}  - \frac{c\beta}{1 + e^{\phi (t - \tau - \delta)}}
$$

So the gradient of this function $\nabla f(t \mid \theta_k)$ would be a column vector of partial derivatives with the following form:

$$
\nabla f(t \mid \theta_k) = 
\begin{bmatrix}
    \frac{\partial f(t \mid \theta_k)}{\partial \alpha} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \beta} \\
    \frac{\partial f(t \mid \theta_k)}{\partial c} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \lambda} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \phi} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \tau} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \delta}
  \end{bmatrix}
$$

But in order to produce this beauty, we need to derive each term separately. And you know the saying: every journey begins with a single step, let's go!

### Deriving the RRi-vs-Time Equations

**Deriving the alpha ($\alpha$) parameter**

Lucky for us, this is the easiest of the seven model parameters, given that it is a constant,  it is not being transformed in any way. For this reason its derivative is just 1 (because the derivative of any constant is 1).

$$ 
\frac{\partial f(t \mid \theta_k)}{\partial \alpha} = 1
$$

**Deriving the beta ($\beta$) parameter**

The beta parameter is present on both logistic functions. However it is on the top position, so we can derive it while mantaining the rest of the parameters constant and dropping the ones where the term $\beta$ is not used like $\alpha$, like this:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \beta} = \frac{1}{1 + e^{\lambda (t - \tau)}}  - \frac{c}{1 + e^{\phi (t - \tau - \delta)}}
$$

**Deriving the $c$ parameter**

The derivation of $c$ is relatively trivial given that it only appears on the numerator of the recovery logistic function. So the final derivative form would look like this:

$$
\frac{\partial f(t \mid \theta_k)}{\partial c} =  - \frac{\beta}{1 + e^{\phi (t - \tau - \delta)}}
$$

**Deriving the lambda ($\lambda$) parameter**

Here is where it gets more difficult. Given that $\lambda$ appears on the exponent we need to apply the chain rule plus some additional calculus. However, we can use a shortcut here. For any function in the form of $f(x) = \frac{A}{g(x)}$ it's derivative ends up being $f'(x) = -A \frac{g'(x)}{g(x)^2}$. 

Let's recall the part of the function that contains $\lambda$:

$$
\frac{\beta}{1 + e^{\lambda (t - \tau)}}
$$

Here, we can assume that $1 + e^{\lambda (t - \tau)}$ correspond to the $g(x)$ we previusly mentioned. With this being said, the derivative of $g(x) = 1 + e^{x (t - \tau)}$ with respect to $x$ would be $(t - \tau) e^{x (t - \tau)}$. Consequently, applying the aforementioned transformations to our original equation, the function would have the following form:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \lambda} = -\beta \frac{(t - \tau) e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2}
$$

which can be further rearranged as:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \lambda} = -\beta (t - \tau) \frac{e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2}
$$

**Deriving the phi ($\phi$) parameter**

Here, we apply the same logic to the one we used earlier for $\lambda$. So the functional form would be:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \phi} = c\beta \frac{(t - \tau - \delta) e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}
$$

which can be further rearranged into:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \phi} = c\beta(t - \tau - \delta) \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}
$$

**Deriving the tau ($\tau$) parameter**

For $\tau$ things gets complicated. This is because this parameter is present in both logistic components (exercise-induced decay and recovery components). So we need to apply our same trick, but instead of pulling the time-specific segment of the exponent, we need to pull the rate component in the exponent. Let's recall the components of the function that contains the $\tau$ parameters:

$$
\underbrace{\frac{\beta}{1 + e^{\lambda (t - \tau)}}}_{\text{Exercise}} - \underbrace{\frac{c\beta}{1 + e^{\phi (t - \tau - \delta)}}}_{\text{Recovery}}
$$

So the derivative form (applying our previous derivation rule) would be the following:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \tau} = \underbrace{-\beta \frac{-\lambda e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2}}_{\text{Exercise}} - \underbrace{(-c\beta) \frac{-\phi e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}}_{\text{Recovery}}
$$

Note that the negative sign in the pulled rate parameter is propagated from the negative sign in the $\tau$ parameter. Rearrenging the signs and the numerators, the final form would be:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \tau} =
\beta \lambda \frac{e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2} - c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2} 
$$

**Deriving the delta ($\delta$) parameter**

For the $\delta$ parameter, the final product is similar for $\tau$. This, in the sense that the final form is similar in the recovery function, given that its relation with the rate parameter $\phi$ is conserved with respect to $\tau$. That is why, we obtain the same form for that side of the equation:

$$
\frac{\partial f(t \mid \theta_k)}{\partial \delta} = -c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2} 
$$

There we have it! Now the final gradient of our function (by taking the whole equations) would be something like this:

$$
\nabla f(t \mid \theta_k) = 
  \begin{bmatrix}
    \frac{\partial f(t \mid \theta_k)}{\partial \alpha} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \beta} \\
    \frac{\partial f(t \mid \theta_k)}{\partial c} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \lambda} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \phi} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \tau} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \delta}
  \end{bmatrix} = 
  \begin{bmatrix}
    1 \\
    \frac{1}{1 + e^{\lambda (t - \tau)}}  - \frac{c}{1 + e^{\phi (t - \tau - \delta)}} \\
    - \frac{\beta}{1 + e^{\phi (t - \tau - \delta)}} \\
    -\beta \frac{(t - \tau) e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2} \\
    c\beta(t - \tau - \delta) \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2} \\
    \beta \lambda \frac{e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2} - c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}  \\
    -c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}
  \end{bmatrix}
$$

And this monstruosity is the whole gradient of our RRi-vs-time model, which as you can see, is a vector of partial derivatives that describe the change in the function value with respect a change in the parameter being derived (i.e., a measure of how sensitive is). 

# Expansion of the Gradient of $\log L$

Now we have all the necessary tools to compute the gradient of the log likelihood function, which remember, involves not only the parameters of $M$, but also the $\sigma$ parameter. The gradient of this function will update the momentum $p$ of the imaginary particle, that then will update the position coordinates of this particle, which itself correspond to the values we're trying to estimate. 

To honor Hamilton's memory, we'll refer to the gradient of the log likelihood as $\nabla U(q)$, given that the "position" parameter $q$ under the Hamiltonian framework are the parameter values (which work as coordinates in the parameter space), and the gradient of the log likelihood ($\nabla U(q)$) will give us the updated momentum $p$ to then, update our parameter coordinates $q$. 

::: {.callout-note}
## Terminology Clarification

However, its important to considerate that in Hamiltonian Monte Carlo (HMC), the **potential energy** is defined as $U(q) = -\log L(q) - \log \text{prior}(q)$. Here, we focus on the gradient of the log-likelihood $\nabla \log L(q)$ for simplicity (assuming uniform priors). For non-uniform priors, $\nabla U(q) = -\nabla \log L(q) - \nabla \log \text{prior}(q)$.
:::

Let's recall our derived (log) likelihood function with respect to $M$ ($\frac{\partial \log L}{\partial M}$):

$$
\frac{\partial \log L}{\partial M} = \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \theta_k}}
$$

And with respect to $\sigma$ ($\frac{\partial \log L}{\partial \sigma}$):

$$
\frac{\partial \log L}{\partial \sigma} = -\frac{N}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{N}{(y_i - M)^2}
$$

So our gradient function would look like something like this:

$$
\nabla U(q) = 
-\nabla \log L(q) = 
  -\begin{bmatrix}
    \frac{\partial \log L}{\partial M} \\
    \frac{\partial \log L}{\partial \sigma}
  \end{bmatrix} = 
  -\begin{bmatrix}
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \theta_k}} \\
    -\frac{N}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{N}{(y_i - M)^2}
  \end{bmatrix}
$$

But recall that $\frac{\partial M}{\partial \theta_k}$ is the partial derivative of the model $M$ with respecto to each parameter value $\theta_k$, so $\frac{\partial \log L}{\partial M}$ can be expanded with respect to each parameter value using the elements of the gradient of our RRi-vs-time model $\nabla f(t \mid \theta_k)$. By incorporating these changes into $\nabla U(q)$ the result is the following:
[Please note that here I use $M$ as a shorthand for $f(t \mid \theta_k)$ (as indicated at the beginning of the math section). So plase consider that when I refer to $\frac{\partial M}{\partial \theta_k}$ is equivalent to $\frac{\partial f(t \mid \theta_k)}{\partial \theta_k}$]{.aside}

$$
\nabla U(q) = 
-\nabla \log L(q) =
  -\begin{bmatrix}
    \frac{\partial \log L}{\partial \alpha} \\
    \frac{\partial \log L}{\partial \beta} \\
    \frac{\partial \log L}{\partial c} \\
    \frac{\partial \log L}{\partial \lambda} \\
    \frac{\partial \log L}{\partial \phi} \\
    \frac{\partial \log L}{\partial \tau} \\
    \frac{\partial \log L}{\partial \delta} \\
    \frac{\partial \log L}{\partial \sigma}
  \end{bmatrix} = 
  -\begin{bmatrix}
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \alpha}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \beta}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial c}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \lambda}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \phi}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \tau}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \delta}} \\
    -\frac{N}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{N}{(y_i - M)^2}
  \end{bmatrix}
$$

This is the cherry and cream on the top of gradient-based estimation methods, because with this gradient function we can now compute the slope of this probability landscape and ride it without problems. Now the next challenge is transforming the hand-derived equations into a valid R code that will actually be useful to estimate anything.

# Translating Math Into Code

Okay, after almost igniting the latex out of my keyboard, now we'll have to pull these fancy equations into an efficient set of functions in R that will be able to actually do something. This "something" will be estimating the gradient of the surface where this particle is in space (using $-\nabla U(q)$) and updating its position $q$ by leveraging momentum $p$ to carry us away, sliding over higher probability landscapes.

We'll start simple. First we will implement the Castillo-Aguilar Equation (the RRi-vs-Time model) in order to accept an input vector as time and a named list of parameter values. This will serve our purpose when we need to compute the predicted RRi values, to then compute the residuals for the log likelihood and bla bla bla (i.e., what we already have been discussing so far). 

Let's recall what is the function we'll implement in R code:

$$
f(t \mid \theta_k) = \alpha + \frac{\beta}{1 + e^{\lambda (t - \tau)}}  - \frac{c\beta}{1 + e^{\phi (t - \tau - \delta)}} \\
\text{where:}~k\in\{\alpha,\beta,c,\lambda,\phi,\tau,\delta\}
$$

Where $t$ will be the time vector and $\theta_k$ the list of parameters. Let's see how this turns out!

```{r}
# ==============================================
# 1. RRi Model (Castillo-Aguilar Equation)
# ==============================================
rr_model <- function(t, params) {
  alpha <- params$alpha
  beta <- params$beta
  c <- params$c
  lambda <- params$lambda
  phi <- params$phi
  tau <- params$tau
  delta <- params$delta
  
  term2 <- beta / (1 + exp(lambda * (t - tau)))
  term3 <- (-c * beta) / (1 + exp(phi * (t - tau - delta)))
  alpha + term2 + term3
}
```

Now let's ensure ourselves that this code is actually working. We'll simulate some rest-exercise-rest dynamics using our function and then visualize it using `ggplot2` package:

```{r}
#| code-fold: true

# Time vector
t <- seq(0.01, 12, 0.01) 

# list with RRi parameters
params <- list(alpha = 800, beta = -300, c = 0.9,
               lambda = -3, phi = -2, 
               tau = 5, delta = 2)

# list with parameters for SD dynacmis of RRi over time
params_sd <- list(alpha = 30, beta = -30, c = 0.7,
               lambda = -1, phi = -0.8, 
               tau = 5, delta = 3)

# Simulate the "true" (smooth) RRi curve
y_true <- rr_model(t, params)
# And the fluctuations for RRi SD over time
y_sd <- rr_model(t, params_sd)

# And simulate the observed RRi signal
y <- y_true + rnorm(n = length(t), sd = y_sd)

ggplot() +
  geom_line(aes(t, y), linewidth = 1/4, color = "purple") +
  geom_line(aes(t, y_true), linewidth = 1, color = "purple") +
  geom_line(aes(t, y_true + y_sd), linewidth = 1/2, linetype = 2, color = "purple") +
  geom_line(aes(t, y_true - y_sd), linewidth = 1/2, linetype = 2, color = "purple") +
  geom_vline(xintercept = c(5, 7), linetype = 2, color = "gray50") +
  annotate("text", label = "Rest", x = 3.5, y = 650, color = "gray50") +
  annotate("text", label = "Exercise", x = 6, y = 750, color = "gray50") +
  annotate("text", label = "Recovery", x = 8.5, y = 650, color = "gray50") +
  labs(y = "RRi (ms)", x = "Time (min)", 
       title = "Exercise-Induced RRi Dynamics",
       subtitle = "Cardiac Autonomic Modulation")
```

So as you can see, we can model not only a time-varying RRi, but also, we can simulate the variations in the standard deviation of RRi over time. See how the dispersion changes from rest, exercise and recovery around the true curve and the observed RRi curve.

Cool! now we have to implement the log likelihood function, which is the function that will give us the probability (in log scale) of the current parameter values on this parameter space, which will inform us if we are going into a place of greater probability (or not). Let's recall the equation we arrived earlier:

$$
\log L(y_i \mid M, \sigma^2) = -\frac{N}{2} \log(2\pi\sigma^2) - \sum_{i = 1}^{N}{\frac{(y_i - M)^2}{2\sigma^2}}
$$

Let's go implementing this bad boy as a function in R:

```{r}
# ==============================================
# 2. Gaussian Log-Likelihood
# ==============================================
log_likelihood <- function(y, t, params, sigma) {
  M <- rr_model(t, params)
  residuals <- y - M
  N <- length(residuals)
  sigma_squared <- sigma^2
  
  -N/2 * log(2 * pi * sigma_squared) - sum(residuals^2) / (2 * sigma_squared)
}
```

Now let's see if we can visualize something using this function:

```{r}
#| code-fold: true

# Same parameters as before

# Observed signal
y <- y_true + rnorm(n = length(t), sd = 30)

# Input parameters
param_data <- expand.grid(
  alpha = seq(710, 890, length.out = 100),
  c = seq(0.3, 1.5, length.out = 100)
)

# Compute log likelihood
param_data$log_likelihood <- 
  apply(param_data, 1, function(x) {
    new_params <- within(params, {
      alpha <- x[["alpha"]];
      c <- x[["c"]]
    }) 
    log_likelihood(y, t, new_params, sigma = 20)
  })

param_data <- as.data.table(param_data)


# Get unique alpha and c values for axes
z_vals <- as.matrix(dcast(param_data, alpha ~ c, value.var = "log_likelihood")[,-1])
alpha_vals <- unique(param_data$alpha)
c_vals <- unique(param_data$c)

# Create the 3D surface plot
library(plotly)

plot_ly() |> 
  add_trace(
    x = c_vals,
    y = alpha_vals,
    z = z_vals,
    type = "surface",
    colorscale = "Viridis",
    showscale = TRUE
  ) |> 
  layout(
    scene = list(
      xaxis = list(title = "C (Recovery proportion)"),
      yaxis = list(title = "Alpha parameter"),
      zaxis = list(title = "Log-Likelihood"),
      camera = list(eye = list(x = -1, y = -2.5, z = 0.5))
    ),
    title = "Log-Likelihood Landscape for alpha and c Parameters"
  )
```

So as you can see, we can now explore the probability of our parameter space (at least, in two dimensions). However, in reality, our parameter space has 8 parameters (7 from the RRi-vs-time model and the $\sigma$ parameter), which increases exponentially the number of computations needed to use grid approximation.
[I don't have access to a quantum computer to be able to compute the whole probability landscape without my computer bursting into flames]{.aside}

Some algorithms that are designed to find optimal parameter values, work by maximizing the likelihood of seeing the data for a given set of the parameters (e.g., maximum likelihood estimation, MLE for short). In those cases, some methods might even take advantage of the gradient information similar to HMC methods, like gradient-descent for instance. However, almost all methods require you to able to compute the likelihood of observing the data given a set of parameter values (which is what we just plotted).

Up next, we need to specify (in R code) the vector of partial derivatives we estimated earlier. Let's remember that this vector of derivatives (of the RRi-vs-time model) will be important to compute the gradient of the log likelihood, which is necessary to update the momentum vector to then update our parameter vector, by simulating our imaginary particle moving throught this parameter space.

Let's recall our vector of partial derivatives for the RRi-vs-time model:

$$
\nabla f(t \mid \theta_k) = 
  \begin{bmatrix}
    \frac{\partial f(t \mid \theta_k)}{\partial \alpha} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \beta} \\
    \frac{\partial f(t \mid \theta_k)}{\partial c} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \lambda} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \phi} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \tau} \\
    \frac{\partial f(t \mid \theta_k)}{\partial \delta}
  \end{bmatrix} = 
  \begin{bmatrix}
    1 \\
    \frac{1}{1 + e^{\lambda (t - \tau)}}  - \frac{c}{1 + e^{\phi (t - \tau - \delta)}} \\
    - \frac{\beta}{1 + e^{\phi (t - \tau - \delta)}} \\
    -\beta \frac{(t - \tau) e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2} \\
    c\beta(t - \tau - \delta) \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2} \\
    \beta \lambda \frac{e^{\lambda (t - \tau)}}{(1 + e^{\lambda (t - \tau)})^2} - c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}  \\
    -c \beta \phi \frac{e^{\phi (t - \tau - \delta)}}{(1 + e^{\phi (t - \tau - \delta)})^2}
  \end{bmatrix}
$$

Yeah... Its a picture hard to erase from memory. Even though this look like ITS A LOT, it truly isn't. Let's focus on the implementation of these set of equations in R:

```{r}
# ==============================================
# 3. Partial Derivatives of the RRi Model
# ==============================================
compute_partials <- function(t, params) {
  alpha <- params$alpha
  beta <- params$beta
  c <- params$c
  lambda <- params$lambda
  phi <- params$phi
  tau <- params$tau
  delta <- params$delta
  
  # Precompute common terms
  t_tau <- t - tau
  t_tau_delta <- t - tau - delta
  
  exp_lambda <- exp(lambda * t_tau)
  exp_phi <- exp(phi * t_tau_delta)
  denom1 <- (1 + exp_lambda)^2
  denom2 <- (1 + exp_phi)^2
  
  list(
    d_alpha = rep(1, length(t)),             # Is constant, so we repeat 1's
    d_beta = 1/(1 + exp_lambda) - c/(1 + exp_phi), # ∂M/∂β
    d_c = -beta / (1 + exp_phi),             # ∂M/∂c
    d_lambda = -beta * t_tau * exp_lambda / denom1, # ∂M/∂λ
    d_phi = c * beta * t_tau_delta * exp_phi / denom2, # ∂M/∂φ
    d_tau = beta * lambda * exp_lambda / denom1 - c * beta * phi * exp_phi / denom2, # ∂M/∂τ
    d_delta = -c * beta * phi * exp_phi / denom2 # ∂M/∂δ
  )
}
```

These set of equation only tell us how sensitive is each parameter with regard to the observed error in the predicted RRi. Let's plot this for further understanding. 

```{r}
#| fig-asp: 1.5
#| code-fold: true

partials <- 
  compute_partials(t, params) |> 
  as.data.table()

names(partials) <- 
  c("alpha", "beta", "c", 
    "lambda", "phi", "tau", "delta")

data <- 
  cbind(time = t, partials) |> 
  melt.data.table(id.vars = "time")

ggplot(data, aes(time, value, col = variable)) +
  facet_grid(rows = vars(variable), scales = "free_y", 
             labeller = label_parsed) +
  geom_line(show.legend = FALSE, linewidth = 1) +
  geom_vline(xintercept = c(5, 7), linetype = 2, color = "gray50") +
  labs(x = "Time (min)", y = "Effect on RRi (ms)",
       title = "Effect of parameter change on RRi",
       subtitle = "Change in RRi for a unit change for each parameter")
```

In this plot, we can observed the effect of modifying in one unit each parameter on the observed RRi dynamics. This crucial piece of information is obtained thanks to the magic of partial derivatives. It allow us to describe how a system, composed of multiple parameters controlling the behavior of the observed dynamics, change relative to each parameter at a time. In reality, this set of equations describe a richer picture that we are intending to show here (but that's an adventure for another blog post).

Finally, we now need to implement the gradient of the log likelihood function $\nabla U(q)$, which let's recall, it looked like this:

$$
\nabla U(q) = 
-\nabla \log L(q) =
  -\begin{bmatrix}
    \frac{\partial \log L}{\partial \alpha} \\
    \frac{\partial \log L}{\partial \beta} \\
    \frac{\partial \log L}{\partial c} \\
    \frac{\partial \log L}{\partial \lambda} \\
    \frac{\partial \log L}{\partial \phi} \\
    \frac{\partial \log L}{\partial \tau} \\
    \frac{\partial \log L}{\partial \phi} \\
    \frac{\partial \log L}{\partial \sigma}
  \end{bmatrix} = 
  -\begin{bmatrix}
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \alpha}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \beta}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial c}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \lambda}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \phi}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \tau}} \\
    \frac{1}{\sigma^2} \sum_{i = 1}^{N}{ (y_i - M) \cdot \frac{\partial M}{\partial \phi}} \\
    -\frac{N}{\sigma} + \frac{1}{\sigma^3} \sum_{i=1}^{N}{(y_i - M)^2}
  \end{bmatrix}
$$

However, most of the work is already done, so we basically need to assemble the functions that take as input our observed data (RRi and time vectors), the named list of parameter values and $\sigma$.

Let's see how this implementation looks like:

```{r}
# ==============================================
# 4. Gradient of Log-Likelihood (∇U(q))
# ==============================================
grad_U <- function(y, t, params, sigma) {
  M <- rr_model(t, params)
  residuals <- y - M
  partials <- compute_partials(t, params)
  N <- length(residuals)
  
  # Precompute inverses
  sigma_sq_inv <- 1 / sigma^2
  sigma_cu_inv <- 1 / sigma^3
  
  # Gradients for model parameters
  grad_params <- list(
    alpha = sum(residuals * partials$d_alpha) * sigma_sq_inv,
    beta = sum(residuals * partials$d_beta) * sigma_sq_inv,
    c = sum(residuals * partials$d_c) * sigma_sq_inv,
    lambda = sum(residuals * partials$d_lambda) * sigma_sq_inv,
    phi = sum(residuals * partials$d_phi) * sigma_sq_inv,
    tau = sum(residuals * partials$d_tau) * sigma_sq_inv,
    delta = sum(residuals * partials$d_delta) * sigma_sq_inv
  )
  
  # Gradient for sigma
  grad_sigma <- (-N / sigma) + sum(residuals^2) * sigma_cu_inv
  
  # Return -∇logL(q)
  gradLL <- c(grad_params, list(sigma = grad_sigma))
  
  lapply(gradLL, function(x) -x)
}
```

It's important to not forget that the gradient of the potential energy $\nabla U(q)$, used to update the momentum parameter $p$ is itself a vector of gradients for all parameters, including $\sigma$:

$$
\nabla U(q) = -\nabla \log L(q) = -\begin{bmatrix}
\frac{\partial \log L}{\partial \alpha} \\
\vdots \\
\frac{\partial \log L}{\partial \sigma}
\end{bmatrix}
$$

Now that we have our gradient function $\nabla U(q)$, we'll need to use it to simulate how our particle moves through the parameter space. The common way to do this is to use what we know as symplectic integrators, which are a special kind of algorithms specifically design to leverage Hamiltonian parameters ($p$ and $q$) to simulate the evolution of a Hamiltonian system over time. The most famous symplectic integrator used in common Bayesian analysis software is the leapfrog integrator. 

I have already described the leapfrog integrator in previous [blog posts](https://doi.org/10.59350/fa26y-xa178), however, for the sake of not interrumpting the flow of the narrative, let me remind you what was about.

In the leapfrog integrator, we start with a random momentum, commonly drawn from a standard normal distribution (with $\mu = 0$ and $\sigma^2 = 1$):

$$
p \leftarrow \mathcal{N}(n \mid 0, 1), \text{ where: } n = \text{length}(p)
$$

However, its important to note that we need to draw a random standard normal vector of the same length of the position parameter vector $q$, so that we'll have a momentum $p$ for each value in $q$.

Once we start with a random momentum, we need to updated it right away based on the gradient of our parameter space. To do this, let's recall the form of the rate of change of the momentum paramter $p$:

$$
\frac{d p}{d t} = -\frac{\partial H}{\partial q} = -\nabla U(q)
$$

This means that in order to update $p$ we need add the value of the $\nabla U(q)$ in the opposite direction, analogous to a ball rolling downhill. We manage to do this by "half-updates":

$$
p_{t+0.5} \leftarrow p_{t} - \nabla U(q_{t}) \cdot \frac{\text{stepsize}}{2}
$$
[First half-step momentum update]{.aside}

Then, with the updated momentum, we update the position vector $q$ like the following:

$$
q_{t+1} \leftarrow q_{t} + p_{t+0.5} \cdot \text{stepsize}
$$
[Full-step position update]{.aside}

And then, we end up the algorithm by half-updating the momentum again, but this time with the new position:

$$
p_{t+1} \leftarrow p_{t+0.5} - \nabla U(q) \cdot \frac{\text{stepsize}}{2}
$$
[Second half-step momentum update]{.aside}

And this is the whole algorithm to simulate the movement of the particle through this parameter space. As you see, the complicated stuff is not the algorithm itself, its the process to obtaining the equations to compute such gradients in the first place.

We can compact this algorithm into this tiny R function:

```{r}
leapfrog <- function(q, p, dt, y, t) {
  # Extract sigma and model parameters
  sigma <- q$sigma
  params <- q[names(q) != "sigma"]
  
  # Compute gradient
  grad <- grad_U(y, t, params, sigma)
  
  # Update momentum
  p <- p - 0.5 * dt * unlist(grad)
  
  # Update position
  q_new <- as.list(x = unlist(q) + dt * p)
  
  # Recompute gradient at new position
  new_sigma <- q_new$sigma
  new_params <- q_new[names(q_new) != "sigma"]
  grad_new <- grad_U(y, t, new_params, new_sigma)
  
  # Final momentum update
  p <- p - 0.5 * dt * unlist(grad_new)
  
  list(q = q_new, p = p)
}
```

Now let's try simulating this bad boy!

```{r}
# Initial parameters ----
## Castillo-Aguilar's model
params <- list(
  alpha = 800, beta = -300, c = 0.8,
  lambda = -2, phi = -1, tau = 6, delta = 3
)
## Parameter for the standard deviation
sigma <- 10

# Data ----
## Time
t <- seq(0, 15, length.out = 1000)
## Observed RRi
y <- rr_model(t, params) + rnorm(1000, sd = sigma)

# Parameters for simulation ----
## Number of steps
steps <- 10000
## Stepsize at each step
stepsize <- 0.00025
## Random seed
set.seed(1234)

## Initiate random momentum vector
p <- rnorm(n = 8, mean = 0, sd = 1)

## Parameter
q <- c(params, list(sigma = sigma))
out <- vector("list", length = steps)

## Leapfrog integrator
for (step in 1:steps) {
  out[[step]] <- leapfrog(q = q, p = p, dt = stepsize, y = y, t = t)
  q <- out[[step]]$q
  p <- out[[step]]$p
}
```

And that's it! Now we have to check how well our leapfrog algorithm performed. Here we can leverage the famous Phase-Space plots, which illustrates the Position-vs-Momentum evolution of the system on the parameter space:

```{r}
#| code-fold: true
#| fig-asp: 2

position <- lapply(out, function(x) {
  as.data.table(x$q)
}) |> rbindlist()

momentum <- lapply(out, function(x) {
  as.list(x$p)
}) |> rbindlist()

position$step <- 1:steps
momentum$step <- 1:steps

position <- melt.data.table(position, id.vars = "step", value.name = "p")
momentum <- melt.data.table(momentum, id.vars = "step", value.name = "q")

sim_data <- position[momentum, on = c("step", "variable")]
sim_data[, `:=`(q = scale(q), p = scale(p)), list(variable)]

ggplot(sim_data, aes(p, q)) +
  facet_grid(rows = vars(variable), labeller = label_parsed) +
  geom_path(aes(group = 1, alpha = step, col = variable), show.legend = FALSE) +
  labs(x = "Momentum (p)", y = "Position (q)",
       title = "Phase Space of RRi Model",
       subtitle = "Particle Moving on the Parameter Space",
       caption = "Values are Standardized For Each Parameter")
```

With this plot, we can already see which parameters behave accordingly to what we should expect (i.e., cyclic behavior like a pendulum) and which doesn't. At first glance we can see that the $\beta$ parameter didn't reach a stationary behavior. This could be for a number of different reasons like hyperparameter tuning, and in the same way, we could try to fix it using several methods (e.g., step size, trajectory length). However, that's another beast for another article. 

# Synthesizing Theory, Computation, and Cardiovascular Insights

The journey through implementing Hamiltonian Monte Carlo (HMC) for the Castillo-Aguilar RRi-vs-Time model has been both intellectually rigorous and deeply instructive. At its core, this exercise underscores the power of Bayesian inference in tackling complex physiological models, where parameter estimation is not just a mathematical challenge but a bridge to understanding biological systems. The Castillo-Aguilar model, with its logistic components capturing the dynamics of heart rate variability during exercise and recovery, serves as a microcosm of the broader challenges in computational physiology. By translating its equations into code and deriving the necessary gradients by hand, we've peeled back the layers of abstraction that often shroud probabilistic machine learning frameworks. This process reveals the intricate dance between theory and practice, where every line of code is a step toward reconciling mathematical ideals with the messiness of real-world data.

One of the most striking realizations from this endeavor is the elegance of Hamiltonian mechanics as a framework for navigating high-dimensional parameter spaces. The analogy of a particle gliding through a landscape of probabilities is not merely poetic, it's a precise metaphor for how HMC balances exploration and exploitation. By leveraging momentum, the algorithm avoids the inefficient random walk of traditional Markov Chain Monte Carlo (MCMC) methods, instead following the contours of the posterior distribution with a physicist's intuition. This approach is particularly valuable in physiological models like the RRi-vs-Time framework, where parameters such as decay rates ($\lambda$, $\phi$) and time constants ($\tau$, $\delta$) are not just abstract numbers but reflections of autonomic nervous system activity. The ability to quantify uncertainty in these parameters opens doors to personalized medicine, where interventions could be tailored based on an individual's unique cardiovascular response profile.

However, the path to this point was paved with challenges that mirror the broader struggles of computational biology. Deriving the partial derivatives of the RRi model, a task that required meticulous application of the chain rule to nested logistic functions, highlighted the fragility of manual gradient computation. A single sign error or misplaced exponent could derail the entire sampling process, a reminder of why automatic differentiation has become the backbone of modern probabilistic programming. Yet, there's irreplaceable value in this manual process. By walking through each derivative, we gain an intimate understanding of how changes in parameters like β (governing the exercise response magnitude) propagate through the model to affect predictions. This granular awareness becomes crucial when interpreting HMC diagnostics, for instance, understanding why certain parameters exhibit divergent behavior in phase-space plots while others converge smoothly.

The implementation of the leapfrog integrator brought its own lessons. The delicate balance between step size and trajectory length emerged as a critical factor in sampling efficiency. Too large a step size, and the particle overshoots regions of high probability; too small, and computational resources are wasted on microscopic explorations. This tuning process mirrors the broader tension in computational science between precision and practicality. In our simulation, the non-stationary behavior of the β parameter, visible in its swirling phase-space trajectory, serves as a case study in sampler diagnostics. Such patterns could indicate inadequate tuning, model misspecification, or inherent identifiability issues. For instance, if the exercise phase in the observed data is too brief, the parameters governing the recovery phase ($c$, $\phi$, $\delta$) might become entangled, creating ridges in the posterior that challenge even HMC's sophisticated exploration.

A crucial insight from this work is the symbiotic relationship between model complexity and computational methodology. The Castillo-Aguilar model, while sophisticated enough to capture exercise-recovery dynamics, exists in a sweet spot between simplicity and realism. Adding more physiological detail, say, accounting for respiratory sinus arrhythmia or blood pressure coupling, would quickly escalate the computational demands. This tension raises fundamental questions about model selection in biomedical research: How much complexity is necessary to answer the scientific question at hand? When does additional mechanistic detail become statistical noise? The HMC framework provides a natural platform for addressing these questions through Bayesian model comparison, where evidence ratios can objectively weigh competing formulations of cardiac dynamics.

The role of prior distributions, though minimized in our flat-prior implementation, looms large in real-world applications. In clinical settings, where population-level data might inform plausible ranges for parameters like resting heart rate (α) or recovery speed (φ), informative priors could stabilize estimation. For example, a clinician might know that patients with certain cardiovascular conditions exhibit slower recovery times, which could be encoded as a prior on δ. This Bayesian synthesis of domain knowledge and data is particularly powerful in medicine, where small sample sizes are common but physiological first principles are strong. Future extensions of this work could explore hierarchical modeling approaches, where population-level distributions inform individual parameter estimates, a framework naturally accommodated by HMC's ability to handle high-dimensional correlations.

Looking beyond the specifics of cardiac modeling, this project illuminates broader trends in computational science. The painstaking process of hand-coding gradients, while educational, underscores why tools like Stan, PyMC, and TensorFlow Probability have revolutionized Bayesian workflows. These platforms abstract away the mathematical heavy lifting through automatic differentiation, allowing researchers to focus on model design and interpretation. Yet, as our implementation demonstrates, there remains value in understanding the machinery beneath the hood. When a sampler fails to converge or mixes poorly, the ability to inspect gradient calculations or tweak integrator settings can mean the difference between insight and obscurity. This duality, between user-friendly abstractions and technical mastery, defines modern computational research.

The phase-space plots of parameter trajectories offer a visual poetry to this statistical narrative. The swirling patterns of momentum and position for parameters like λ and φ resemble orbital mechanics, a reminder that Hamiltonian dynamics borrow from the same principles that govern celestial bodies. In these plots, convergence issues manifest as irregular orbits, some parameters looping tightly around equilibrium points, others drifting into the statistical equivalent of deep space. Interpreting these patterns requires a blend of statistical intuition and physical analogy. For instance, the persistent momentum in β's trajectory might suggest that the model's exercise response component is under-constrained by the data, requiring either longer sampling runs or stronger regularization through priors.

Practical considerations for deploying such models in research settings abound. The computational cost of HMC, while lower than traditional MCMC for complex models, remains non-trivial. Our simulation of 10,000 leapfrog steps, a modest number by contemporary standards, already demands careful optimization. In clinical applications where real-time analysis might be desired, this raises questions about hardware acceleration (e.g., GPU implementation) or algorithmic approximations. Techniques like the No-U-Turn Sampler (NUTS), which automatically tunes trajectory lengths, could enhance efficiency but add another layer of complexity to the implementation. These trade-offs between automation and control are characteristic of applied Bayesian work, where every project balances mathematical purity with pragmatic constraints.

The educational dimension of this exercise cannot be overstated. By rebuilding HMC from first principles, we confront the assumptions and approximations that often hide behind library function calls. Why does the leapfrog integrator use half-steps for momentum updates? The answer lies in preserving symplectic structure, a geometric property that maintains volume in phase space, crucial for energy conservation. Such insights transform HMC from a black-box tool into a transparent process, demystifying its superiority over alternatives like Metropolis-Hastings. For students and researchers alike, this deep understanding fosters more than just technical competence; it cultivates a mindset of critical engagement with statistical tools.

Looking forward, several frontiers beckon. Integrating this RRi model into hierarchical frameworks could enable population-level analyses of cardiovascular responses across demographic groups. Combining it with pharmacokinetic models might reveal how medications affect autonomic regulation. On the computational side, implementing gradient-based samplers in differentiable programming languages like JAX could marry the flexibility of manual implementation with the speed of automatic differentiation. There's also the tantalizing possibility of connecting these models to neural architectures, creating hybrid systems that leverage deep learning's pattern recognition strengths while maintaining physiological interpretability.

Ethical considerations emerge alongside technical ones. As models grow more sophisticated, potentially predicting individual cardiovascular risks or optimizing exercise regimens, questions of validation and accountability become paramount. A misestimated τ parameter might seem abstract in a simulation, but if it informs clinical decisions about a patient's recovery capacity, the stakes are real. This underscores the importance of robust uncertainty quantification, a inherent strength of Bayesian methods, and the need for transparent reporting of posterior distributions in applied research.

In closing, this exploration of HMC for the Castillo-Aguilar model serves as both a case study and a metaphor. It illustrates how Bayesian methods can breathe life into physiological models, transforming differential equations into probabilistic narratives of heart rate dynamics. The challenges encountered, from gradient derivation to sampler tuning, mirror the broader journey of computational science: a relentless pursuit of patterns in noise, guided by mathematical theory and tempered by empirical reality. As we stand at the intersection of cardiac physiology and statistical computing, projects like this remind us that every parameter estimate tells a story, a story of ion channels and neural signals, of mathematical optimization and epistemic humility. In the rhythmic rise and fall of RR intervals, we find not just data, but a language of life waiting to be deciphered, one Hamiltonian leap at a time.
